"""
æ¸¬è©¦ OpenAI æ¨¡å‹çš„å–®å…ƒæ¸¬è©¦
"""
import pytest
import json
import time
from unittest.mock import Mock, patch, MagicMock, mock_open
from requests.exceptions import RequestException
from src.models.openai_model import OpenAIModel
from src.models.base import (
    ModelProvider, ChatMessage, ChatResponse, ThreadInfo, 
    FileInfo, RAGResponse
)


class TestOpenAIModelInitialization:
    """æ¸¬è©¦ OpenAIModel åˆå§‹åŒ–"""
    
    def test_openai_model_initialization_basic(self):
        """æ¸¬è©¦åŸºæœ¬åˆå§‹åŒ–"""
        model = OpenAIModel(
            api_key="test_api_key",
            assistant_id="test_assistant_id"
        )
        
        assert model.api_key == "test_api_key"
        assert model.assistant_id == "test_assistant_id"
        assert model.base_url == "https://api.openai.com/v1"
    
    def test_openai_model_initialization_with_custom_base_url(self):
        """æ¸¬è©¦ä½¿ç”¨è‡ªå®šç¾© base URL åˆå§‹åŒ–"""
        custom_url = "https://custom-api.example.com/v1"
        model = OpenAIModel(
            api_key="test_api_key",
            assistant_id="test_assistant_id",
            base_url=custom_url
        )
        
        assert model.base_url == custom_url
    
    def test_get_provider(self):
        """æ¸¬è©¦ç²å–æ¨¡å‹æä¾›å•†"""
        model = OpenAIModel("test_key", "test_assistant")
        assert model.get_provider() == ModelProvider.OPENAI


class TestConnectionCheck:
    """æ¸¬è©¦é€£ç·šæª¢æŸ¥"""
    
    @pytest.fixture
    def model(self):
        return OpenAIModel("test_key", "test_assistant")
    
    def test_check_connection_success(self, model):
        """æ¸¬è©¦æˆåŠŸçš„é€£ç·šæª¢æŸ¥"""
        mock_response = {"data": [{"id": "model-1", "object": "model"}]}
        
        with patch.object(model, '_request', return_value=(True, mock_response, None)):
            success, error = model.check_connection()
            
            assert success is True
            assert error is None
    
    def test_check_connection_failure(self, model):
        """æ¸¬è©¦å¤±æ•—çš„é€£ç·šæª¢æŸ¥"""
        error_message = "API key invalid"
        
        with patch.object(model, '_request', return_value=(False, None, error_message)):
            success, error = model.check_connection()
            
            assert success is False
            assert error == error_message
    
    def test_check_connection_exception(self, model):
        """æ¸¬è©¦é€£ç·šæª¢æŸ¥ç•°å¸¸"""
        with patch.object(model, '_request', side_effect=Exception("Network error")):
            success, error = model.check_connection()
            
            assert success is False
            assert "Network error" in error


class TestChatCompletion:
    """æ¸¬è©¦èŠå¤©å®ŒæˆåŠŸèƒ½"""
    
    @pytest.fixture
    def model(self):
        return OpenAIModel("test_key", "test_assistant")
    
    def test_chat_completion_success(self, model):
        """æ¸¬è©¦æˆåŠŸçš„èŠå¤©å®Œæˆ"""
        messages = [
            ChatMessage(role="user", content="Hello"),
            ChatMessage(role="assistant", content="Hi there!")
        ]
        
        mock_response = {
            "choices": [{
                "message": {"content": "Hello! How can I help you?"},
                "finish_reason": "stop"
            }],
            "usage": {"total_tokens": 50}
        }
        
        with patch.object(model, '_request', return_value=(True, mock_response, None)):
            success, chat_response, error = model.chat_completion(messages)
            
            assert success is True
            assert isinstance(chat_response, ChatResponse)
            assert chat_response.content == "Hello! How can I help you?"
            assert chat_response.finish_reason == "stop"
            assert chat_response.metadata['usage'] == {"total_tokens": 50}
            assert error is None
    
    def test_chat_completion_with_parameters(self, model):
        """æ¸¬è©¦å¸¶åƒæ•¸çš„èŠå¤©å®Œæˆ"""
        messages = [ChatMessage(role="user", content="Test")]
        
        mock_response = {
            "choices": [{"message": {"content": "Response"}, "finish_reason": "stop"}]
        }
        
        with patch.object(model, '_request', return_value=(True, mock_response, None)) as mock_request:
            model.chat_completion(messages, model="gpt-4", temperature=0.7)
            
            call_args = mock_request.call_args
            json_body = call_args[1]['body']
            assert json_body['model'] == 'gpt-4'
            assert json_body['temperature'] == 0.7
    
    def test_chat_completion_failure(self, model):
        """æ¸¬è©¦èŠå¤©å®Œæˆå¤±æ•—"""
        messages = [ChatMessage(role="user", content="Hello")]
        error_message = "Rate limit exceeded"
        
        with patch.object(model, '_request', return_value=(False, None, error_message)):
            success, chat_response, error = model.chat_completion(messages)
            
            assert success is False
            assert chat_response is None
            assert error == error_message
    
    def test_chat_completion_exception(self, model):
        """æ¸¬è©¦èŠå¤©å®Œæˆç•°å¸¸"""
        messages = [ChatMessage(role="user", content="Hello")]
        
        with patch.object(model, '_request', side_effect=Exception("API error")):
            success, chat_response, error = model.chat_completion(messages)
            
            assert success is False
            assert chat_response is None
            assert "API error" in error


class TestThreadManagement:
    """æ¸¬è©¦å°è©±ä¸²ç®¡ç†"""
    
    @pytest.fixture
    def model(self):
        return OpenAIModel("test_key", "test_assistant")
    
    def test_create_thread_success(self, model):
        """æ¸¬è©¦æˆåŠŸå‰µå»ºå°è©±ä¸²"""
        mock_response = {
            "id": "thread_12345",
            "object": "thread",
            "created_at": 1699000000
        }
        
        with patch.object(model, '_request', return_value=(True, mock_response, None)):
            success, thread_info, error = model.create_thread()
            
            assert success is True
            assert isinstance(thread_info, ThreadInfo)
            assert thread_info.thread_id == "thread_12345"
            assert thread_info.created_at == 1699000000
            assert error is None
    
    def test_create_thread_failure(self, model):
        """æ¸¬è©¦å‰µå»ºå°è©±ä¸²å¤±æ•—"""
        error_message = "Failed to create thread"
        
        with patch.object(model, '_request', return_value=(False, None, error_message)):
            success, thread_info, error = model.create_thread()
            
            assert success is False
            assert thread_info is None
            assert error == error_message
    
    def test_delete_thread_success(self, model):
        """æ¸¬è©¦æˆåŠŸåˆªé™¤å°è©±ä¸²"""
        thread_id = "thread_12345"
        
        with patch.object(model, '_request', return_value=(True, {"deleted": True}, None)):
            success, error = model.delete_thread(thread_id)
            
            assert success is True
            assert error is None
    
    def test_delete_thread_failure(self, model):
        """æ¸¬è©¦åˆªé™¤å°è©±ä¸²å¤±æ•—"""
        thread_id = "thread_12345"
        error_message = "Thread not found"
        
        with patch.object(model, '_request', return_value=(False, None, error_message)):
            success, error = model.delete_thread(thread_id)
            
            assert success is False
            assert error == error_message
    
    def test_add_message_to_thread_success(self, model):
        """æ¸¬è©¦æˆåŠŸæ·»åŠ è¨Šæ¯åˆ°å°è©±ä¸²"""
        thread_id = "thread_12345"
        message = ChatMessage(role="user", content="Hello")
        
        with patch.object(model, '_request', return_value=(True, {"id": "msg_123"}, None)):
            success, error = model.add_message_to_thread(thread_id, message)
            
            assert success is True
            assert error is None
    
    def test_add_message_to_thread_failure(self, model):
        """æ¸¬è©¦æ·»åŠ è¨Šæ¯åˆ°å°è©±ä¸²å¤±æ•—"""
        thread_id = "thread_12345"
        message = ChatMessage(role="user", content="Hello")
        error_message = "Thread not found"
        
        with patch.object(model, '_request', return_value=(False, None, error_message)):
            success, error = model.add_message_to_thread(thread_id, message)
            
            assert success is False
            assert error == error_message


class TestAssistantExecution:
    """æ¸¬è©¦åŠ©ç†åŸ·è¡Œ"""
    
    @pytest.fixture
    def model(self):
        return OpenAIModel("test_key", "test_assistant_123")
    
    def test_run_assistant_success(self, model):
        """æ¸¬è©¦æˆåŠŸåŸ·è¡ŒåŠ©ç†"""
        thread_id = "thread_12345"
        
        mock_run_response = {"id": "run_123", "status": "queued"}
        mock_final_response = {"status": "completed"}
        mock_messages_response = {
            "data": [{
                "role": "assistant",
                "content": [{"text": {"value": "Assistant response"}}]
            }]
        }
        
        with patch.object(model, '_request', return_value=(True, mock_run_response, None)), \
             patch.object(model, '_wait_for_run_completion', return_value=mock_final_response), \
             patch.object(model, '_get_thread_messages', return_value=(True, ChatResponse(content="Assistant response"), None)):
            
            success, chat_response, error = model.run_assistant(thread_id)
            
            assert success is True
            assert isinstance(chat_response, ChatResponse)
            assert chat_response.content == "Assistant response"
            assert error is None
    
    def test_run_assistant_run_creation_failure(self, model):
        """æ¸¬è©¦åŠ©ç†åŸ·è¡Œå‰µå»ºå¤±æ•—"""
        thread_id = "thread_12345"
        error_message = "Failed to create run"
        
        with patch.object(model, '_request', return_value=(False, None, error_message)):
            success, chat_response, error = model.run_assistant(thread_id)
            
            assert success is False
            assert chat_response is None
            assert error == error_message
    
    def test_run_assistant_run_failed_status(self, model):
        """æ¸¬è©¦åŠ©ç†åŸ·è¡Œå¤±æ•—ç‹€æ…‹"""
        thread_id = "thread_12345"
        
        mock_run_response = {"id": "run_123", "status": "queued"}
        mock_final_response = {"status": "failed"}
        
        with patch.object(model, '_request', return_value=(True, mock_run_response, None)), \
             patch.object(model, '_wait_for_run_completion', return_value=mock_final_response):
            
            success, chat_response, error = model.run_assistant(thread_id)
            
            assert success is False
            assert chat_response is None
            assert "Assistant run failed with status: failed" in error


class TestFileOperations:
    """æ¸¬è©¦æª”æ¡ˆæ“ä½œ"""
    
    @pytest.fixture
    def model(self):
        return OpenAIModel("test_key", "test_assistant")
    
    def test_upload_knowledge_file_success(self, model):
        """æ¸¬è©¦æˆåŠŸä¸Šå‚³çŸ¥è­˜æª”æ¡ˆ"""
        file_path = "/path/to/test.txt"
        mock_response = {
            "id": "file_123",
            "filename": "test.txt",
            "bytes": 1024,
            "status": "processed",
            "purpose": "assistants"
        }
        
        with patch('builtins.open', mock_open(read_data="test content")), \
             patch.object(model, '_request', return_value=(True, mock_response, None)):
            
            success, file_info, error = model.upload_knowledge_file(file_path)
            
            assert success is True
            assert isinstance(file_info, FileInfo)
            assert file_info.file_id == "file_123"
            assert file_info.filename == "test.txt"
            assert file_info.size == 1024
            assert error is None
    
    def test_upload_knowledge_file_failure(self, model):
        """æ¸¬è©¦ä¸Šå‚³çŸ¥è­˜æª”æ¡ˆå¤±æ•—"""
        file_path = "/path/to/test.txt"
        error_message = "File upload failed"
        
        with patch('builtins.open', mock_open(read_data="test content")), \
             patch.object(model, '_request', return_value=(False, None, error_message)):
            
            success, file_info, error = model.upload_knowledge_file(file_path)
            
            assert success is False
            assert file_info is None
            assert error == error_message
    
    def test_list_files_success(self, model):
        """æ¸¬è©¦æˆåŠŸåˆ—å‡ºæª”æ¡ˆ"""
        mock_response = {
            "data": [
                {
                    "id": "file_1",
                    "filename": "doc1.txt",
                    "bytes": 1024,
                    "status": "processed",
                    "purpose": "assistants"
                },
                {
                    "id": "file_2", 
                    "filename": "doc2.txt",
                    "bytes": 2048,
                    "status": "processed",
                    "purpose": "assistants"
                }
            ]
        }
        
        with patch.object(model, '_request', return_value=(True, mock_response, None)):
            success, files, error = model.list_files()
            
            assert success is True
            assert len(files) == 2
            assert all(isinstance(f, FileInfo) for f in files)
            assert files[0].file_id == "file_1"
            assert files[1].filename == "doc2.txt"
            assert error is None
    
    def test_get_knowledge_files(self, model):
        """æ¸¬è©¦ç²å–çŸ¥è­˜æª”æ¡ˆï¼ˆåˆ¥åæ–¹æ³•ï¼‰"""
        mock_files = [FileInfo(file_id="file_1", filename="test.txt")]
        
        with patch.object(model, 'list_files', return_value=(True, mock_files, None)):
            success, files, error = model.get_knowledge_files()
            
            assert success is True
            assert len(files) == 1
            assert files[0].file_id == "file_1"
    
    def test_get_file_references_success(self, model):
        """æ¸¬è©¦æˆåŠŸç²å–æª”æ¡ˆå¼•ç”¨å°æ‡‰è¡¨"""
        mock_files = [
            FileInfo(file_id="file_1", filename="document1.txt"),
            FileInfo(file_id="file_2", filename="data.json")
        ]
        
        with patch.object(model, 'list_files', return_value=(True, mock_files, None)):
            references = model.get_file_references()
            
            assert references == {
                "file_1": "document1",
                "file_2": "data"
            }
    
    def test_get_file_references_failure(self, model):
        """æ¸¬è©¦ç²å–æª”æ¡ˆå¼•ç”¨å¤±æ•—"""
        with patch.object(model, 'list_files', return_value=(False, None, "API error")), \
             patch('src.models.openai_model.logger') as mock_logger:
            
            references = model.get_file_references()
            
            assert references == {}
            mock_logger.warning.assert_called_once()
    
    def test_get_file_references_exception(self, model):
        """æ¸¬è©¦ç²å–æª”æ¡ˆå¼•ç”¨ç•°å¸¸"""
        with patch.object(model, 'list_files', side_effect=Exception("Connection error")), \
             patch('src.models.openai_model.logger') as mock_logger:
            
            references = model.get_file_references()
            
            assert references == {}
            mock_logger.error.assert_called_once()


class TestRAGFunctionality:
    """æ¸¬è©¦ RAG åŠŸèƒ½"""
    
    @pytest.fixture
    def model(self):
        return OpenAIModel("test_key", "test_assistant")
    
    def test_query_with_rag_with_existing_thread(self, model):
        """æ¸¬è©¦ä½¿ç”¨ç¾æœ‰ thread çš„ RAG æŸ¥è©¢"""
        query = "What is machine learning?"
        thread_id = "existing_thread_123"
        
        mock_chat_response = ChatResponse(
            content="Machine learning is...",
            metadata={'thread_messages': {'data': []}}
        )
        
        with patch.object(model, 'add_message_to_thread', return_value=(True, None)), \
             patch.object(model, 'run_assistant', return_value=(True, mock_chat_response, None)), \
             patch.object(model, '_process_openai_response', return_value=("Processed content", [])):
            
            success, rag_response, error = model.query_with_rag(query, thread_id)
            
            assert success is True
            assert isinstance(rag_response, RAGResponse)
            assert rag_response.answer == "Processed content"
            assert error is None
    
    def test_query_with_rag_create_new_thread(self, model):
        """æ¸¬è©¦å‰µå»ºæ–° thread çš„ RAG æŸ¥è©¢"""
        query = "Explain AI"
        
        mock_thread_info = ThreadInfo(thread_id="new_thread_456")
        mock_chat_response = ChatResponse(
            content="AI explanation...",
            metadata={'thread_messages': {'data': []}}
        )
        
        with patch.object(model, 'create_thread', return_value=(True, mock_thread_info, None)), \
             patch.object(model, 'add_message_to_thread', return_value=(True, None)), \
             patch.object(model, 'run_assistant', return_value=(True, mock_chat_response, None)), \
             patch.object(model, '_process_openai_response', return_value=("AI explanation", [])):
            
            success, rag_response, error = model.query_with_rag(query)
            
            assert success is True
            assert rag_response.metadata['thread_id'] == "new_thread_456"
            assert error is None
    
    def test_query_with_rag_thread_creation_failure(self, model):
        """æ¸¬è©¦ RAG æŸ¥è©¢ thread å‰µå»ºå¤±æ•—"""
        query = "Test query"
        error_message = "Failed to create thread"
        
        with patch.object(model, 'create_thread', return_value=(False, None, error_message)):
            success, rag_response, error = model.query_with_rag(query)
            
            assert success is False
            assert rag_response is None
            assert "Failed to create thread" in error
    
    def test_query_with_rag_message_addition_failure(self, model):
        """æ¸¬è©¦ RAG æŸ¥è©¢è¨Šæ¯æ·»åŠ å¤±æ•—"""
        query = "Test query"
        
        with patch.object(model, 'create_thread', return_value=(True, ThreadInfo(thread_id="thread_123"), None)), \
             patch.object(model, 'add_message_to_thread', return_value=(False, "Failed to add message")):
            
            success, rag_response, error = model.query_with_rag(query)
            
            assert success is False
            assert rag_response is None
            assert "Failed to add message to thread" in error


class TestAudioTranscription:
    """æ¸¬è©¦éŸ³è¨Šè½‰éŒ„"""
    
    @pytest.fixture
    def model(self):
        return OpenAIModel("test_key", "test_assistant")
    
    def test_transcribe_audio_success(self, model):
        """æ¸¬è©¦æˆåŠŸçš„éŸ³è¨Šè½‰éŒ„"""
        audio_path = "/path/to/audio.mp3"
        mock_response = {"text": "Hello, this is a test audio."}
        
        with patch('builtins.open', mock_open()), \
             patch.object(model, '_request', return_value=(True, mock_response, None)):
            
            success, text, error = model.transcribe_audio(audio_path)
            
            assert success is True
            assert text == "Hello, this is a test audio."
            assert error is None
    
    def test_transcribe_audio_with_model_parameter(self, model):
        """æ¸¬è©¦å¸¶æ¨¡å‹åƒæ•¸çš„éŸ³è¨Šè½‰éŒ„"""
        audio_path = "/path/to/audio.mp3"
        mock_response = {"text": "Transcribed text"}
        
        with patch('builtins.open', mock_open()), \
             patch.object(model, '_request', return_value=(True, mock_response, None)) as mock_request:
            
            model.transcribe_audio(audio_path, model="whisper-1")
            
            # æª¢æŸ¥æ˜¯å¦æ­£ç¢ºå‚³éäº†æ¨¡å‹åƒæ•¸
            call_kwargs = mock_request.call_args[1]
            files = call_kwargs['files']
            assert files['model'][1] == "whisper-1"
    
    def test_transcribe_audio_failure(self, model):
        """æ¸¬è©¦éŸ³è¨Šè½‰éŒ„å¤±æ•—"""
        audio_path = "/path/to/audio.mp3"
        error_message = "Audio format not supported"
        
        with patch('builtins.open', mock_open()), \
             patch.object(model, '_request', return_value=(False, None, error_message)):
            
            success, text, error = model.transcribe_audio(audio_path)
            
            assert success is False
            assert text is None
            assert error == error_message
    
    def test_transcribe_audio_exception(self, model):
        """æ¸¬è©¦éŸ³è¨Šè½‰éŒ„ç•°å¸¸"""
        audio_path = "/path/to/audio.mp3"
        
        with patch('builtins.open', side_effect=Exception("File not found")):
            success, text, error = model.transcribe_audio(audio_path)
            
            assert success is False
            assert text is None
            assert "File not found" in error


class TestImageGeneration:
    """æ¸¬è©¦åœ–ç‰‡ç”Ÿæˆ"""
    
    @pytest.fixture
    def model(self):
        return OpenAIModel("test_key", "test_assistant")
    
    def test_generate_image_success(self, model):
        """æ¸¬è©¦æˆåŠŸç”Ÿæˆåœ–ç‰‡"""
        prompt = "A beautiful sunset over mountains"
        mock_response = {
            "data": [{"url": "https://example.com/generated_image.png"}]
        }
        
        with patch.object(model, '_request', return_value=(True, mock_response, None)):
            success, image_url, error = model.generate_image(prompt)
            
            assert success is True
            assert image_url == "https://example.com/generated_image.png"
            assert error is None
    
    def test_generate_image_with_parameters(self, model):
        """æ¸¬è©¦å¸¶åƒæ•¸çš„åœ–ç‰‡ç”Ÿæˆ"""
        prompt = "Test image"
        
        mock_response = {
            "data": [{"url": "https://example.com/test.png"}]
        }
        
        with patch.object(model, '_request', return_value=(True, mock_response, None)) as mock_request:
            model.generate_image(prompt, n=2, size="1024x1024")
            
            call_args = mock_request.call_args
            json_body = call_args[1]['body']
            assert json_body['n'] == 2
            assert json_body['size'] == "1024x1024"
    
    def test_generate_image_failure(self, model):
        """æ¸¬è©¦åœ–ç‰‡ç”Ÿæˆå¤±æ•—"""
        prompt = "Test image"
        error_message = "Image generation failed"
        
        with patch.object(model, '_request', return_value=(False, None, error_message)):
            success, image_url, error = model.generate_image(prompt)
            
            assert success is False
            assert image_url is None
            assert error == error_message


class TestResponseProcessing:
    """æ¸¬è©¦å›æ‡‰è™•ç†"""
    
    @pytest.fixture
    def model(self):
        return OpenAIModel("test_key", "test_assistant")
    
    def test_process_openai_response_with_citations(self, model):
        """æ¸¬è©¦è™•ç†å¸¶å¼•ç”¨çš„ OpenAI å›æ‡‰"""
        thread_messages = {
            "data": [{
                "role": "assistant",
                "content": [{
                    "type": "text",
                    "text": {
                        "value": "æ ¹æ“šæ–‡ä»¶ã€0â€ sourceã€‘ï¼Œé€™æ˜¯ç­”æ¡ˆã€‚",
                        "annotations": [{
                            "text": "ã€0â€ sourceã€‘",
                            "file_citation": {
                                "file_id": "file_123",
                                "quote": "relevant quote"
                            }
                        }]
                    }
                }]
            }]
        }
        
        with patch.object(model, 'get_file_references', return_value={"file_123": "document1"}), \
             patch('src.models.openai_model.s2t_converter') as mock_converter:
            
            mock_converter.convert.side_effect = lambda x: x  # æ¨¡æ“¬è½‰æ›
            
            content, sources = model._process_openai_response(thread_messages)
            
            assert "[1]" in content
            assert len(sources) == 1
            assert sources[0]['file_id'] == "file_123"
            assert sources[0]['filename'] == "document1"
    
    def test_process_openai_response_no_data(self, model):
        """æ¸¬è©¦è™•ç†ç„¡æ•¸æ“šçš„å›æ‡‰"""
        empty_messages = {"data": []}
        
        with patch.object(model, '_get_response_data', return_value=None):
            content, sources = model._process_openai_response(empty_messages)
            
            assert content == ""
            assert sources == []
    
    def test_process_openai_response_multiple_citations(self, model):
        """æ¸¬è©¦è™•ç†å¤šå€‹å¼•ç”¨çš„å›æ‡‰"""
        thread_messages = {
            "data": [{
                "role": "assistant",
                "content": [{
                    "type": "text",
                    "text": {
                        "value": "æ ¹æ“šã€0â€ sourceã€‘å’Œã€1â€ sourceã€‘çš„è³‡æ–™",
                        "annotations": [
                            {
                                "text": "ã€0â€ sourceã€‘",
                                "file_citation": {"file_id": "file_1", "quote": "quote1"}
                            },
                            {
                                "text": "ã€1â€ sourceã€‘", 
                                "file_citation": {"file_id": "file_2", "quote": "quote2"}
                            }
                        ]
                    }
                }]
            }]
        }
        
        with patch.object(model, 'get_file_references', return_value={"file_1": "doc1", "file_2": "doc2"}), \
             patch('src.models.openai_model.s2t_converter') as mock_converter:
            
            mock_converter.convert.side_effect = lambda x: x
            content, sources = model._process_openai_response(thread_messages)
            
            assert "[1]" in content and "[2]" in content
            assert len(sources) == 2
    
    def test_process_openai_response_error_handling(self, model):
        """æ¸¬è©¦å›æ‡‰è™•ç†éŒ¯èª¤è™•ç†"""
        malformed_data = {"invalid": "format"}
        
        with patch('src.models.openai_model.logger') as mock_logger:
            content, sources = model._process_openai_response(malformed_data)
            
            assert content == ""
            assert sources == []
            mock_logger.error.assert_called_once()
    
    def test_get_response_data_success(self, model):
        """æ¸¬è©¦æå–å›æ‡‰æ•¸æ“šæˆåŠŸ"""
        response = {
            "data": [
                {"role": "user", "content": [{"type": "text", "text": {"value": "Question"}}]},
                {"role": "assistant", "content": [{"type": "text", "text": {"value": "Answer"}}]}
            ]
        }
        
        data = model._get_response_data(response)
        
        assert data is not None
        assert data["role"] == "assistant"
        assert data["content"][0]["text"]["value"] == "Answer"
    
    def test_get_response_data_no_assistant(self, model):
        """æ¸¬è©¦ç„¡åŠ©ç†å›æ‡‰çš„æ•¸æ“šæå–"""
        response = {
            "data": [{
                "role": "user",
                "content": [{"type": "text", "text": {"value": "Question"}}]
            }]
        }
        
        data = model._get_response_data(response)
        assert data is None
    
    def test_get_response_data_exception(self, model):
        """æ¸¬è©¦æ•¸æ“šæå–ç•°å¸¸"""
        malformed_response = {"invalid": "format"}
        
        with patch('src.models.openai_model.logger') as mock_logger:
            data = model._get_response_data(malformed_response)
            
            assert data is None
            mock_logger.error.assert_called_once()
    
    def test_extract_sources_duplicate_files(self, model):
        """æ¸¬è©¦æå–ä¾†æºæ™‚é¿å…é‡è¤‡æª”æ¡ˆ"""
        thread_messages = {
            "data": [{
                "role": "assistant",
                "content": [{
                    "type": "text",
                    "text": {
                        "annotations": [
                            {
                                "type": "file_citation",
                                "file_citation": {"file_id": "file_123", "quote": "quote1"}
                            },
                            {
                                "type": "file_citation", 
                                "file_citation": {"file_id": "file_123", "quote": "quote2"}
                            }
                        ]
                    }
                }]
            }]
        }
        
        sources = model._extract_sources_from_response(thread_messages)
        
        # æ‡‰è©²åªæœ‰ä¸€å€‹ä¾†æºï¼ˆå»é‡ï¼‰
        assert len(sources) == 1
        assert sources[0]['file_id'] == "file_123"


class TestBackwardCompatibility:
    """æ¸¬è©¦å‘å¾Œç›¸å®¹æ–¹æ³•"""
    
    @pytest.fixture
    def model(self):
        return OpenAIModel("test_key", "test_assistant")
    
    def test_check_token_valid(self, model):
        """æ¸¬è©¦ check_token_valid å‘å¾Œç›¸å®¹æ–¹æ³•"""
        with patch.object(model, 'check_connection', return_value=(True, None)):
            success, _, error = model.check_token_valid()
            
            assert success is True
            assert error is None
    
    def test_retrieve_thread(self, model):
        """æ¸¬è©¦ retrieve_thread å‘å¾Œç›¸å®¹æ–¹æ³•"""
        thread_id = "thread_123"
        
        with patch.object(model, '_request', return_value=(True, {"id": thread_id}, None)):
            success, response, error = model.retrieve_thread(thread_id)
            
            assert success is True
            assert response["id"] == thread_id
    
    def test_create_thread_message(self, model):
        """æ¸¬è©¦ create_thread_message å‘å¾Œç›¸å®¹æ–¹æ³•"""
        thread_id = "thread_123"
        content = "Test message"
        
        with patch.object(model, 'add_message_to_thread', return_value=(True, None)):
            success, _, error = model.create_thread_message(thread_id, content)
            
            assert success is True
            assert error is None
    
    def test_create_thread_run(self, model):
        """æ¸¬è©¦ create_thread_run å‘å¾Œç›¸å®¹æ–¹æ³•"""
        thread_id = "thread_123"
        
        with patch.object(model, '_request', return_value=(True, {"id": "run_123"}, None)):
            success, response, error = model.create_thread_run(thread_id)
            
            assert success is True
            assert response["id"] == "run_123"
    
    def test_retrieve_thread_run(self, model):
        """æ¸¬è©¦ retrieve_thread_run å‘å¾Œç›¸å®¹æ–¹æ³•"""
        thread_id = "thread_123"
        run_id = "run_456"
        
        with patch.object(model, '_request', return_value=(True, {"status": "completed"}, None)):
            success, response, error = model.retrieve_thread_run(thread_id, run_id)
            
            assert success is True
            assert response["status"] == "completed"
    
    def test_list_thread_messages(self, model):
        """æ¸¬è©¦ list_thread_messages å‘å¾Œç›¸å®¹æ–¹æ³•"""
        thread_id = "thread_123"
        
        with patch.object(model, '_request', return_value=(True, {"data": []}, None)):
            success, response, error = model.list_thread_messages(thread_id)
            
            assert success is True
            assert "data" in response
    
    def test_audio_transcriptions(self, model):
        """æ¸¬è©¦ audio_transcriptions å‘å¾Œç›¸å®¹æ–¹æ³•"""
        file_path = "/path/to/audio.mp3"
        model_name = "whisper-1"
        
        with patch.object(model, 'transcribe_audio', return_value=(True, "transcribed text", None)):
            result = model.audio_transcriptions(file_path, model_name)
            
            assert result == (True, "transcribed text", None)


class TestInternalMethods:
    """æ¸¬è©¦å…§éƒ¨æ–¹æ³•"""
    
    @pytest.fixture
    def model(self):
        return OpenAIModel("test_key", "test_assistant")
    
    def test_request_get_method(self, model):
        """æ¸¬è©¦ GET è«‹æ±‚"""
        with patch('requests.get') as mock_get:
            mock_response = Mock()
            mock_response.status_code = 200
            mock_response.json.return_value = {"data": "test"}
            mock_get.return_value = mock_response
            
            success, data, error = model._request('GET', '/test', assistant=True)
            
            assert success is True
            assert data == {"data": "test"}
            assert error is None
            
            # æª¢æŸ¥è«‹æ±‚é ­
            call_args = mock_get.call_args
            headers = call_args[1]['headers']
            assert 'Authorization' in headers
            assert 'OpenAI-Beta' in headers
    
    def test_request_post_method(self, model):
        """æ¸¬è©¦ POST è«‹æ±‚"""
        with patch('requests.post') as mock_post:
            mock_response = Mock()
            mock_response.status_code = 200
            mock_response.json.return_value = {"success": True}
            mock_post.return_value = mock_response
            
            body = {"message": "test"}
            success, data, error = model._request('POST', '/test', body=body)
            
            assert success is True
            assert data == {"success": True}
    
    def test_request_delete_method(self, model):
        """æ¸¬è©¦ DELETE è«‹æ±‚"""
        with patch('requests.delete') as mock_delete:
            mock_response = Mock()
            mock_response.status_code = 200
            mock_response.json.return_value = {"deleted": True}
            mock_delete.return_value = mock_response
            
            success, data, error = model._request('DELETE', '/test', assistant=True)
            
            assert success is True
            assert data == {"deleted": True}
    
    def test_request_rate_limit_error(self, model):
        """æ¸¬è©¦é€Ÿç‡é™åˆ¶éŒ¯èª¤"""
        with patch('requests.post') as mock_post:
            mock_response = Mock()
            mock_response.status_code = 429
            mock_post.return_value = mock_response
            
            with pytest.raises(RequestException):
                model._request('POST', '/test')
    
    def test_request_server_error(self, model):
        """æ¸¬è©¦æœå‹™å™¨éŒ¯èª¤"""
        with patch('requests.post') as mock_post:
            mock_response = Mock()
            mock_response.status_code = 500
            mock_post.return_value = mock_response
            
            with pytest.raises(RequestException):
                model._request('POST', '/test')
    
    def test_request_client_error(self, model):
        """æ¸¬è©¦å®¢æˆ¶ç«¯éŒ¯èª¤"""
        with patch('requests.post') as mock_post:
            mock_response = Mock()
            mock_response.status_code = 400
            mock_response.json.return_value = {
                "error": {"message": "Bad request"}
            }
            mock_post.return_value = mock_response
            
            success, data, error = model._request('POST', '/test')
            
            assert success is False
            assert data is None
            assert error == "Bad request"
    
    def test_request_client_error_no_json(self, model):
        """æ¸¬è©¦å®¢æˆ¶ç«¯éŒ¯èª¤ç„¡ JSON æ ¼å¼"""
        with patch('requests.post') as mock_post:
            mock_response = Mock()
            mock_response.status_code = 400
            mock_response.json.side_effect = ValueError("Invalid JSON")
            mock_response.text = "Bad Request Error"
            mock_post.return_value = mock_response
            
            success, data, error = model._request('POST', '/test')
            
            assert success is False
            assert "HTTP 400" in error
    
    def test_request_network_exception(self, model):
        """æ¸¬è©¦ç¶²è·¯ç•°å¸¸"""
        with patch('requests.post', side_effect=RequestException("Network error")):
            with pytest.raises(RequestException):
                model._request('POST', '/test')
    
    def test_request_general_exception(self, model):
        """æ¸¬è©¦ä¸€èˆ¬ç•°å¸¸"""
        with patch('requests.post', side_effect=Exception("General error")):
            success, data, error = model._request('POST', '/test')
            
            assert success is False
            assert data is None
            assert "OpenAI API ç³»çµ±ä¸ç©©å®š" in error
    
    def test_wait_for_run_completion_success(self, model):
        """æ¸¬è©¦ç­‰å¾…åŸ·è¡Œå®Œæˆ - æˆåŠŸ"""
        thread_id = "thread_123"
        run_id = "run_456"
        
        with patch.object(model, 'retrieve_thread_run') as mock_retrieve:
            # æ¨¡æ“¬ç‹€æ…‹è®ŠåŒ–ï¼šqueued -> in_progress -> completed
            mock_retrieve.side_effect = [
                (True, {"status": "queued"}, None),
                (True, {"status": "in_progress"}, None),
                (True, {"status": "completed"}, None)
            ]
            
            with patch('time.sleep'):  # é¿å…å¯¦éš›ç­‰å¾…
                result = model._wait_for_run_completion(thread_id, run_id, max_wait_time=120)
                
                assert result["status"] == "completed"
    
    def test_wait_for_run_completion_timeout(self, model):
        """æ¸¬è©¦ç­‰å¾…åŸ·è¡Œå®Œæˆ - è¶…æ™‚"""
        thread_id = "thread_123"
        run_id = "run_456"
        
        with patch.object(model, 'retrieve_thread_run', return_value=(True, {"status": "in_progress"}, None)), \
             patch('time.time', side_effect=[0, 130]):  # æ¨¡æ“¬è¶…æ™‚
            
            with pytest.raises(Exception, match="Request timeout"):
                model._wait_for_run_completion(thread_id, run_id, max_wait_time=120)
    
    def test_wait_for_run_completion_api_failure(self, model):
        """æ¸¬è©¦ç­‰å¾…åŸ·è¡Œå®Œæˆ - API å¤±æ•—"""
        thread_id = "thread_123"
        run_id = "run_456"
        
        with patch.object(model, 'retrieve_thread_run', return_value=(False, None, "API error")):
            with pytest.raises(Exception, match="API error"):
                model._wait_for_run_completion(thread_id, run_id)
    
    def test_wait_for_run_completion_queued_status(self, model):
        """æ¸¬è©¦ç­‰å¾…åŸ·è¡Œå®Œæˆ - æ’éšŠç‹€æ…‹æ›´é•·ç­‰å¾…"""
        thread_id = "thread_123"
        run_id = "run_456"
        
        with patch.object(model, 'retrieve_thread_run') as mock_retrieve, \
             patch('time.sleep') as mock_sleep:
            
            mock_retrieve.side_effect = [
                (True, {"status": "queued"}, None),
                (True, {"status": "completed"}, None)
            ]
            
            result = model._wait_for_run_completion(thread_id, run_id)
            
            assert result["status"] == "completed"
            # æª¢æŸ¥æ’éšŠç‹€æ…‹æ™‚ä½¿ç”¨æ›´é•·çš„ç­‰å¾…æ™‚é–“
            assert mock_sleep.call_args_list[0][0][0] == 10
    
    def test_get_thread_messages_success(self, model):
        """æ¸¬è©¦ç²å–å°è©±ä¸²è¨Šæ¯ - æˆåŠŸ"""
        thread_id = "thread_123"
        
        mock_response = {
            "data": [{
                "role": "assistant",
                "content": [{"text": {"value": "Assistant response"}}]
            }]
        }
        
        with patch.object(model, 'list_thread_messages', return_value=(True, mock_response, None)):
            success, chat_response, error = model._get_thread_messages(thread_id)
            
            assert success is True
            assert isinstance(chat_response, ChatResponse)
            assert chat_response.content == "Assistant response"
    
    def test_get_thread_messages_no_assistant_response(self, model):
        """æ¸¬è©¦ç²å–å°è©±ä¸²è¨Šæ¯ - ç„¡åŠ©ç†å›æ‡‰"""
        thread_id = "thread_123"
        
        mock_response = {
            "data": [{
                "role": "user",
                "content": [{"text": {"value": "User message"}}]
            }]
        }
        
        with patch.object(model, 'list_thread_messages', return_value=(True, mock_response, None)):
            success, chat_response, error = model._get_thread_messages(thread_id)
            
            assert success is False
            assert chat_response is None
            assert error == "No assistant response found"
    
    def test_get_thread_messages_api_failure(self, model):
        """æ¸¬è©¦ç²å–å°è©±ä¸²è¨Šæ¯ - API å¤±æ•—"""
        thread_id = "thread_123"
        
        with patch.object(model, 'list_thread_messages', return_value=(False, None, "API error")):
            success, chat_response, error = model._get_thread_messages(thread_id)
            
            assert success is False
            assert chat_response is None
            assert error == "API error"


class TestUserLevelConversationManagement:
    """æ¸¬è©¦ç”¨æˆ¶ç´šå°è©±ç®¡ç†"""
    
    @pytest.fixture
    def model(self):
        return OpenAIModel("test_key", "test_assistant")
    
    def test_chat_with_user_existing_thread(self, model):
        """æ¸¬è©¦ä½¿ç”¨ç¾æœ‰ thread çš„ç”¨æˆ¶å°è©±"""
        user_id = "user_123"
        message = "Hello, how are you?"
        platform = "line"
        existing_thread_id = "thread_456"
        
        mock_chat_response = ChatResponse(
            content="I'm doing well, thank you!",
            metadata={'thread_messages': {'data': []}}
        )
        
        with patch('src.models.openai_model.get_thread_id_by_user_id', return_value=existing_thread_id), \
             patch.object(model, 'add_message_to_thread', return_value=(True, None)), \
             patch.object(model, 'run_assistant', return_value=(True, mock_chat_response, None)), \
             patch.object(model, '_process_openai_response', return_value=("I'm doing well!", [])):
            
            success, rag_response, error = model.chat_with_user(user_id, message, platform)
            
            assert success is True
            assert isinstance(rag_response, RAGResponse)
            assert rag_response.answer == "I'm doing well!"
            assert rag_response.metadata['user_id'] == user_id
            assert rag_response.metadata['thread_id'] == existing_thread_id
            assert error is None
    
    def test_chat_with_user_create_new_thread(self, model):
        """æ¸¬è©¦å‰µå»ºæ–° thread çš„ç”¨æˆ¶å°è©±"""
        user_id = "new_user_789"
        message = "First message"
        platform = "discord"
        
        mock_thread_info = ThreadInfo(thread_id="new_thread_abc")
        mock_chat_response = ChatResponse(
            content="Welcome!",
            metadata={'thread_messages': {'data': []}}
        )
        
        with patch('src.models.openai_model.get_thread_id_by_user_id', return_value=None), \
             patch.object(model, 'create_thread', return_value=(True, mock_thread_info, None)), \
             patch('src.models.openai_model.save_thread_id') as mock_save, \
             patch.object(model, 'add_message_to_thread', return_value=(True, None)), \
             patch.object(model, 'run_assistant', return_value=(True, mock_chat_response, None)), \
             patch.object(model, '_process_openai_response', return_value=("Welcome!", [])):
            
            success, rag_response, error = model.chat_with_user(user_id, message, platform)
            
            assert success is True
            assert rag_response.metadata['thread_id'] == "new_thread_abc"
            mock_save.assert_called_once_with(user_id, "new_thread_abc", platform)
    
    def test_chat_with_user_thread_creation_failure(self, model):
        """æ¸¬è©¦ç”¨æˆ¶å°è©± thread å‰µå»ºå¤±æ•—"""
        user_id = "user_123"
        message = "Test message"
        
        with patch('src.models.openai_model.get_thread_id_by_user_id', return_value=None), \
             patch.object(model, 'create_thread', return_value=(False, None, "Thread creation failed")):
            
            success, rag_response, error = model.chat_with_user(user_id, message)
            
            assert success is False
            assert rag_response is None
            assert "Failed to create thread" in error
    
    def test_chat_with_user_exception(self, model):
        """æ¸¬è©¦ç”¨æˆ¶å°è©±ç•°å¸¸"""
        user_id = "user_123"
        message = "Test message"
        
        with patch('src.models.openai_model.get_thread_id_by_user_id', side_effect=Exception("Database error")), \
             patch('src.models.openai_model.logger') as mock_logger:
            
            success, rag_response, error = model.chat_with_user(user_id, message)
            
            assert success is False
            assert rag_response is None
            assert "Database error" in error
            mock_logger.error.assert_called_once()
    
    def test_clear_user_history_success(self, model):
        """æ¸¬è©¦æˆåŠŸæ¸…é™¤ç”¨æˆ¶æ­·å²"""
        user_id = "user_123"
        platform = "line"
        thread_id = "thread_456"
        
        with patch('src.models.openai_model.get_thread_id_by_user_id', return_value=thread_id), \
             patch.object(model, 'delete_thread', return_value=(True, None)), \
             patch('src.models.openai_model.delete_thread_id') as mock_delete:
            
            success, error = model.clear_user_history(user_id, platform)
            
            assert success is True
            assert error is None
            mock_delete.assert_called_once_with(user_id, platform)
    
    def test_clear_user_history_no_thread(self, model):
        """æ¸¬è©¦æ¸…é™¤ä¸å­˜åœ¨çš„ç”¨æˆ¶æ­·å²"""
        user_id = "user_123"
        platform = "line"
        
        with patch('src.models.openai_model.get_thread_id_by_user_id', return_value=None), \
             patch('src.models.openai_model.logger') as mock_logger:
            
            success, error = model.clear_user_history(user_id, platform)
            
            assert success is True
            assert error is None
            mock_logger.info.assert_called_once()
    
    def test_clear_user_history_delete_thread_failure(self, model):
        """æ¸¬è©¦åˆªé™¤ thread å¤±æ•—ä½†ä»æ¸…é™¤æœ¬åœ°è¨˜éŒ„"""
        user_id = "user_123"
        platform = "line"
        thread_id = "thread_456"
        
        with patch('src.models.openai_model.get_thread_id_by_user_id', return_value=thread_id), \
             patch.object(model, 'delete_thread', return_value=(False, "API error")), \
             patch('src.models.openai_model.delete_thread_id') as mock_delete, \
             patch('src.models.openai_model.logger') as mock_logger:
            
            success, error = model.clear_user_history(user_id, platform)
            
            assert success is True
            assert error is None
            mock_delete.assert_called_once_with(user_id, platform)
            mock_logger.error.assert_called_once()
    
    def test_clear_user_history_exception(self, model):
        """æ¸¬è©¦æ¸…é™¤ç”¨æˆ¶æ­·å²ç•°å¸¸"""
        user_id = "user_123"
        platform = "line"
        
        with patch('src.models.openai_model.get_thread_id_by_user_id', side_effect=Exception("Database error")), \
             patch('src.models.openai_model.logger') as mock_logger:
            
            success, error = model.clear_user_history(user_id, platform)
            
            assert success is False
            assert "Database error" in error
            mock_logger.error.assert_called_once()


class TestEdgeCases:
    """æ¸¬è©¦é‚Šç•Œæƒ…æ³å’ŒéŒ¯èª¤è™•ç†"""
    
    @pytest.fixture
    def model(self):
        return OpenAIModel("test_key", "test_assistant")
    
    def test_empty_api_key(self):
        """æ¸¬è©¦ç©º API key"""
        model = OpenAIModel("", "test_assistant")
        assert model.api_key == ""
    
    def test_empty_assistant_id(self):
        """æ¸¬è©¦ç©º assistant ID"""
        model = OpenAIModel("test_key", "")
        assert model.assistant_id == ""
    
    def test_none_parameters(self):
        """æ¸¬è©¦ None åƒæ•¸"""
        model = OpenAIModel("test_key", None)
        assert model.assistant_id is None
    
    def test_empty_message_content(self, model):
        """æ¸¬è©¦ç©ºè¨Šæ¯å…§å®¹"""
        messages = [ChatMessage(role="user", content="")]
        
        mock_response = {
            "choices": [{"message": {"content": "Please provide a message"}, "finish_reason": "stop"}]
        }
        
        with patch.object(model, '_request', return_value=(True, mock_response, None)):
            success, chat_response, error = model.chat_completion(messages)
            
            assert success is True
            assert chat_response.content == "Please provide a message"
    
    def test_very_long_message(self, model):
        """æ¸¬è©¦éå¸¸é•·çš„è¨Šæ¯"""
        long_message = "A" * 10000
        messages = [ChatMessage(role="user", content=long_message)]
        
        mock_response = {
            "choices": [{"message": {"content": "Response to long message"}, "finish_reason": "stop"}]
        }
        
        with patch.object(model, '_request', return_value=(True, mock_response, None)):
            success, chat_response, error = model.chat_completion(messages)
            
            assert success is True
    
    def test_special_characters_in_message(self, model):
        """æ¸¬è©¦ç‰¹æ®Šå­—ç¬¦è¨Šæ¯"""
        special_message = "Hello! ä½ å¥½ ğŸµ @#$%^&*()_+"
        messages = [ChatMessage(role="user", content=special_message)]
        
        mock_response = {
            "choices": [{"message": {"content": "Response with special chars"}, "finish_reason": "stop"}]
        }
        
        with patch.object(model, '_request', return_value=(True, mock_response, None)):
            success, chat_response, error = model.chat_completion(messages)
            
            assert success is True
    
    def test_network_timeout_handling(self, model):
        """æ¸¬è©¦ç¶²è·¯è¶…æ™‚è™•ç†"""
        with patch.object(model, '_request', side_effect=RequestException("Timeout")):
            success, error = model.check_connection()
            
            assert success is False
            assert "Timeout" in error
    
    def test_malformed_response_handling(self, model):
        """æ¸¬è©¦æ ¼å¼éŒ¯èª¤çš„å›æ‡‰è™•ç†"""
        # æ¨¡æ“¬ç„¡æ•ˆçš„ JSON å›æ‡‰
        with patch('requests.get') as mock_get:
            mock_response = Mock()
            mock_response.status_code = 200
            mock_response.json.side_effect = ValueError("Invalid JSON")
            mock_get.return_value = mock_response
            
            success, data, error = model._request('GET', '/test')
            
            assert success is False
            assert "OpenAI API ç³»çµ±ä¸ç©©å®š" in error
    
    def test_empty_response_content(self, model):
        """æ¸¬è©¦ç©ºå›æ‡‰å…§å®¹"""
        mock_response = {
            "choices": [{"message": {"content": ""}, "finish_reason": "stop"}]
        }
        
        with patch.object(model, '_request', return_value=(True, mock_response, None)):
            success, chat_response, error = model.chat_completion([ChatMessage(role="user", content="test")])
            
            assert success is True
            assert chat_response.content == ""
    
    def test_missing_choices_in_response(self, model):
        """æ¸¬è©¦å›æ‡‰ä¸­ç¼ºå°‘ choices"""
        mock_response = {"usage": {"total_tokens": 10}}
        
        with patch.object(model, '_request', return_value=(True, mock_response, None)):
            with pytest.raises((KeyError, IndexError)):
                model.chat_completion([ChatMessage(role="user", content="test")])
    
    def test_file_not_found_error(self, model):
        """æ¸¬è©¦æª”æ¡ˆä¸å­˜åœ¨éŒ¯èª¤"""
        with patch('builtins.open', side_effect=FileNotFoundError("File not found")):
            success, file_info, error = model.upload_knowledge_file("/nonexistent/file.txt")
            
            assert success is False
            assert file_info is None
            assert "File not found" in error


if __name__ == "__main__":
    pytest.main([__file__])