import requests
import json
import time
import uuid
from typing import List, Dict, Tuple, Optional
from .base import (
    FullLLMInterface, 
    ModelProvider, 
    ChatMessage, 
    ChatResponse, 
    ThreadInfo, 
    FileInfo,
    KnowledgeBase,
    RAGResponse
)
from ..utils.retry import retry_on_rate_limit
from ..services.conversation import get_conversation_manager
from ..core.logger import get_logger

logger = get_logger(__name__)


class GeminiModel(FullLLMInterface):
    """
    Google Gemini 2024 æ¨¡å‹å¯¦ä½œ
    
    ğŸ“‹ æ¶æ§‹è·è²¬åˆ†å·¥ï¼š
    âœ… RESPONSIBILITIES (æ¨¡å‹å±¤è·è²¬):
      - å¯¦ä½œçµ±ä¸€çš„ FullLLMInterface æ¥å£
      - æä¾› chat_with_user() æ–‡å­—å°è©±åŠŸèƒ½
      - æä¾› transcribe_audio() éŸ³è¨Šè½‰éŒ„åŠŸèƒ½
      - ç®¡ç†å°è©±æ­·å²å’Œä¸Šä¸‹æ–‡
      - è™•ç† Google AI API é™æµå’Œé‡è©¦é‚è¼¯

    ğŸ¯ æ¨¡å‹ç‰¹è‰²ï¼š
    - Semantic Retrieval API: Google çš„èªç¾©æª¢ç´¢æœå‹™
    - Multimodal RAG: æ”¯æ´æ–‡å­—ã€åœ–ç‰‡ã€å½±ç‰‡çš„æ··åˆæª¢ç´¢
    - Long Context Window: Gemini Pro 1.5 æ”¯æ´ç™¾è¬ token ä¸Šä¸‹æ–‡

    âš ï¸ åŠŸèƒ½é™åˆ¶ï¼š
    - éŸ³è¨Šè½‰éŒ„: ä½¿ç”¨å¤šæ¨¡æ…‹API (Betaéšæ®µï¼Œå¯èƒ½ä¸ç©©å®š)
    - åœ–ç‰‡ç”Ÿæˆ: ç›®å‰ä¸æ”¯æ´ (è¿”å› "Gemini ç›®å‰ä¸æ”¯æ´åœ–ç‰‡ç”Ÿæˆ")
    - Vertex AI æ•´åˆ: ä¼æ¥­ç´š AI å¹³å°æ•´åˆ
    - Ranking API: æ™ºæ…§é‡æ’åºæå‡æª¢ç´¢å“è³ª
    """
    
    def __init__(self, api_key: str, model_name: str = "gemini-1.5-pro-latest", base_url: str = None, project_id: str = None):
        self.api_key = api_key
        self.model_name = model_name
        self.base_url = base_url or "https://generativelanguage.googleapis.com/v1beta"
        self.project_id = project_id  # Google Cloud å°ˆæ¡ˆ IDï¼Œç”¨æ–¼ Vertex AI
        
        # Semantic Retrieval API æ”¯æ´ - ä½¿ç”¨æœ‰ç•Œå¿«å–
        from ..core.bounded_cache import BoundedCache
        self.corpora = BoundedCache(max_size=50, ttl=7200)  # 50å€‹èªæ–™åº«ï¼Œ2å°æ™‚TTL
        self.default_corpus_name = "chatbot-knowledge"
        
        # Multimodal å’Œé•·ä¸Šä¸‹æ–‡æ”¯æ´
        self.max_context_tokens = 1000000  # Gemini 1.5 Pro æ”¯æ´ç™¾è¬ token
        self.supported_media_types = ['image', 'video', 'audio', 'text']
        
        # Vertex AI å®¢æˆ¶ç«¯ï¼ˆå¯é¸ï¼‰
        self.vertex_ai_client = None
        
        # ç¬¬ä¸‰æ–¹èªéŸ³è½‰éŒ„æœå‹™ï¼ˆGoogle Speech-to-Textï¼‰
        self.speech_service = None
        
        # å°è©±æ­·å²ç®¡ç†
        self.conversation_manager = get_conversation_manager()
    
    def get_provider(self) -> ModelProvider:
        return ModelProvider.GEMINI
    
    def check_connection(self) -> Tuple[bool, Optional[str]]:
        """æª¢æŸ¥ Gemini API é€£ç·š"""
        try:
            test_message = [ChatMessage(role="user", content="Hello")]
            is_successful, response, error = self.chat_completion(test_message)
            return is_successful, error
        except Exception as e:
            return False, str(e)
    
    @retry_on_rate_limit(max_retries=3, base_delay=1.0)
    def chat_completion(self, messages: List[ChatMessage], **kwargs) -> Tuple[bool, Optional[ChatResponse], Optional[str]]:
        """Gemini Chat Completion with Long Context and Multimodal Support"""
        try:
            # è½‰æ›è¨Šæ¯æ ¼å¼
            gemini_contents = []
            system_instruction = None
            
            for msg in messages:
                if msg.role == "system":
                    system_instruction = msg.content
                else:
                    role = "user" if msg.role == "user" else "model"
                    
                    # æ”¯æŒå¤šæ¨¡æ…‹å…§å®¹
                    parts = []
                    if isinstance(msg.content, str):
                        parts.append({"text": msg.content})
                    elif isinstance(msg.content, list):
                        # å¤šæ¨¡æ…‹å…§å®¹ï¼ˆæ–‡å­— + åœ–ç‰‡ç­‰ï¼‰
                        for part in msg.content:
                            if isinstance(part, str):
                                parts.append({"text": part})
                            elif isinstance(part, dict) and 'type' in part:
                                if part['type'] == 'image':
                                    parts.append({
                                        "inline_data": {
                                            "mime_type": part.get('mime_type', 'image/jpeg'),
                                            "data": part['data']
                                        }
                                    })
                                elif part['type'] == 'video':
                                    parts.append({
                                        "file_data": {
                                            "mime_type": part.get('mime_type', 'video/mp4'),
                                            "file_uri": part['uri']
                                        }
                                    })
                    else:
                        parts.append({"text": str(msg.content)})
                    
                    gemini_contents.append({
                        "role": role,
                        "parts": parts
                    })
            
            json_body = {
                "contents": gemini_contents,
                "generationConfig": {
                    "temperature": kwargs.get('temperature', 0.01),
                    "maxOutputTokens": min(kwargs.get('max_tokens', 8192), 8192),  # Gemini Pro é™åˆ¶
                    "topP": kwargs.get('top_p', 0.8),
                    "topK": kwargs.get('top_k', 40)
                }
            }
            
            # ç³»çµ±æŒ‡ä»¤æ”¯æ´
            if system_instruction:
                json_body["systemInstruction"] = {
                    "parts": [{"text": system_instruction}]
                }
            
            # å®‰å…¨è¨­å®š
            json_body["safetySettings"] = [
                {
                    "category": "HARM_CATEGORY_HARASSMENT",
                    "threshold": "BLOCK_MEDIUM_AND_ABOVE"
                },
                {
                    "category": "HARM_CATEGORY_HATE_SPEECH", 
                    "threshold": "BLOCK_MEDIUM_AND_ABOVE"
                }
            ]
            
            endpoint = f'/models/{self.model_name}:generateContent'
            is_successful, response, error_message = self._request('POST', endpoint, body=json_body)
            
            if not is_successful:
                return False, None, error_message
            
            if 'candidates' not in response or not response['candidates']:
                return False, None, "No response generated"
            
            candidate = response['candidates'][0]
            if 'content' not in candidate or not candidate['content']['parts']:
                return False, None, "No content in response"
            
            content = candidate['content']['parts'][0]['text']
            finish_reason = candidate.get('finishReason', 'STOP')
            
            chat_response = ChatResponse(
                content=content,
                finish_reason=finish_reason,
                metadata={
                    'usage': response.get('usageMetadata', {}),
                    'model': response.get('modelVersion', self.model_name),
                    'safety_ratings': candidate.get('safetyRatings', []),
                    'context_tokens': response.get('usageMetadata', {}).get('promptTokenCount', 0)
                }
            )
            
            return True, chat_response, None
            
        except Exception as e:
            return False, None, str(e)
    
    # === RAG ä»‹é¢å¯¦ä½œï¼ˆä½¿ç”¨ Google Semantic Retrieval APIï¼‰ ===
    
    def upload_knowledge_file(self, file_path: str, **kwargs) -> Tuple[bool, Optional[FileInfo], Optional[str]]:
        """ä½¿ç”¨ Semantic Retrieval API ä¸Šå‚³æª”æ¡ˆåˆ°çŸ¥è­˜åº«"""
        try:
            import os
            import mimetypes
            
            filename = os.path.basename(file_path)
            file_size = os.path.getsize(file_path)
            
            # æª¢æŸ¥æª”æ¡ˆå¤§å°é™åˆ¶ï¼ˆSemantic Retrieval API é™åˆ¶ï¼‰
            if file_size > 20 * 1024 * 1024:  # 20MB é™åˆ¶
                return False, None, f"æª”æ¡ˆéå¤§: {file_size / 1024 / 1024:.1f}MBï¼Œè¶…é 20MB é™åˆ¶"
            
            # 1. ç¢ºä¿èªæ–™åº«å­˜åœ¨
            corpus_name = kwargs.get('corpus_name', self.default_corpus_name)
            if self.corpora.get(corpus_name) is None:
                is_successful, corpus, error = self._create_corpus(corpus_name)
                if not is_successful:
                    return False, None, error
                self.corpora.set(corpus_name, corpus)
            
            # 2. è®€å–æª”æ¡ˆå…§å®¹ä¸¦æª¢æ¸¬é¡å‹
            content_type, _ = mimetypes.guess_type(file_path)
            if not content_type:
                content_type = 'text/plain'
            
            # æ”¯æ´å¤šæ¨¡æ…‹æª”æ¡ˆé¡å‹
            is_multimodal = any(media_type in content_type for media_type in ['image', 'video', 'audio'])
            
            if is_multimodal:
                # å¤šæ¨¡æ…‹æª”æ¡ˆè™•ç†
                return self._upload_multimodal_file(file_path, corpus_name, **kwargs)
            else:
                # æ–‡å­—æª”æ¡ˆè™•ç†
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
            
            # 3. å»ºç«‹æ–‡æª”
            document_data = {
                "displayName": filename,
                "customMetadata": [
                    {"key": "source_file", "stringValue": filename},
                    {"key": "file_type", "stringValue": content_type},
                    {"key": "upload_time", "stringValue": str(int(time.time()))},
                    {"key": "file_size", "numericValue": file_size}
                ]
            }
            
            corpus_data = self.corpora.get(corpus_name)
            if not corpus_data:
                return False, None, f"Corpus {corpus_name} not found"
            corpus_name_full = corpus_data['name']
            endpoint = f'{corpus_name_full}/documents'
            is_successful, document_response, error = self._request('POST', endpoint, body=document_data)
            
            if not is_successful:
                return False, None, error
            
            # 4. æ™ºæ…§åˆ†å¡Šä¸¦ä¸Šå‚³
            chunks = self._intelligent_chunk_text(content, kwargs.get('chunk_size', 1000))
            document_name = document_response['name']
            
            successful_chunks = 0
            for i, chunk in enumerate(chunks):
                chunk_data = {
                    "data": {
                        "stringValue": chunk['text']
                    },
                    "customMetadata": [
                        {"key": "chunk_index", "numericValue": i},
                        {"key": "source_file", "stringValue": filename},
                        {"key": "chunk_tokens", "numericValue": chunk.get('tokens', 0)},
                        {"key": "semantic_section", "stringValue": chunk.get('section', 'general')}
                    ]
                }
                
                chunk_endpoint = f'{document_name}/chunks'
                is_successful, _, error = self._request('POST', chunk_endpoint, body=chunk_data)
                if is_successful:
                    successful_chunks += 1
                # ç¹¼çºŒè™•ç†å…¶ä»–å¡Šï¼Œä¸å› å–®å€‹å¡Šå¤±æ•—è€Œåœæ­¢
            
            if successful_chunks == 0:
                return False, None, "æ‰€æœ‰æ–‡æª”å¡Šä¸Šå‚³å¤±æ•—"
            
            file_info = FileInfo(
                file_id=document_response['name'].split('/')[-1],
                filename=filename,
                size=file_size,
                status='processed',
                purpose='knowledge_base',
                metadata={
                    'corpus': corpus_name,
                    'document_name': document_name,
                    'total_chunks': len(chunks),
                    'successful_chunks': successful_chunks,
                    'content_type': content_type,
                    'is_multimodal': is_multimodal,
                    'upload_time': time.time()
                }
            )
            
            return True, file_info, None
            
        except Exception as e:
            return False, None, str(e)

    def _process_inline_citations(self, text: str, retrieved_sources: List[Dict]) -> Tuple[str, List[Dict]]:
        """
        è™•ç†æ¨¡å‹å›æ‡‰ä¸­çš„å…§æ–‡å¼•ç”¨ï¼Œå°‡ [æª”å] æ›¿æ›ç‚º [æ•¸å­—] å¼•ç”¨æ ¼å¼ï¼Œä¸¦ç§»é™¤ç„¡æ•ˆå¼•ç”¨ã€‚
        """
        import re
        
        cited_filenames = set(re.findall(r'\[([^\]]+)\]', text))
        if not cited_filenames:
            # å¦‚æœæ¨¡å‹å›æ‡‰ä¸­æ²’æœ‰å¼•ç”¨ï¼Œå‰‡ä¾†æºåˆ—è¡¨ä¹Ÿæ‡‰è©²æ˜¯ç©ºçš„
            return text, []

        final_sources = []
        citation_map = {}
        next_ref_num = 1
        
        # å¾æª¢ç´¢åˆ°çš„ä¾†æºä¸­ï¼Œå»ºç«‹æœ‰æ•ˆæª”æ¡ˆåçš„é›†åˆ
        valid_source_filenames = {source['filename'] for source in retrieved_sources if 'filename' in source}

        # å»ºç«‹æœ‰æ•ˆå¼•ç”¨çš„å°æ‡‰è¡¨
        for filename in sorted(list(cited_filenames)): # sort for consistent numbering
            if filename in valid_source_filenames:
                if filename not in citation_map:
                    citation_map[filename] = next_ref_num
                    next_ref_num += 1

        # æ›¿æ›æœ‰æ•ˆçš„å¼•ç”¨
        for filename, ref_num in citation_map.items():
            text = text.replace(f'[{filename}]', f'[{ref_num}]')

        # ç§»é™¤ç„¡æ•ˆçš„å¼•ç”¨
        invalid_filenames = cited_filenames - set(citation_map.keys())
        for filename in invalid_filenames:
            text = text.replace(f'[{filename}]', '')
        
        # æ¸…ç†å¯èƒ½ç”¢ç”Ÿçš„å¤šé¤˜ç©ºæ ¼
        text = re.sub(r'\s\s+', ' ', text).strip()

        # æ ¹æ“šæœ‰æ•ˆçš„å¼•ç”¨ï¼Œå»ºç«‹æœ€çµ‚çš„ sources åˆ—è¡¨
        # ä½¿ç”¨æª”æ¡ˆåä¾†ç¢ºä¿æˆ‘å€‘åªç‚ºæ¯å€‹è¢«å¼•ç”¨çš„æ–‡ä»¶æ·»åŠ ä¸€æ¬¡æº
        added_filenames = set()
        for source in retrieved_sources:
            filename = source.get('filename')
            if filename in citation_map and filename not in added_filenames:
                final_sources.append(source)
                added_filenames.add(filename)
                
        return text, final_sources

    def _fallback_chat_completion(self, query: str, context_messages: List[ChatMessage], **kwargs) -> Tuple[bool, Optional[RAGResponse], Optional[str]]:
        """åœ¨ RAG ä¸å¯ç”¨æ™‚ï¼ŒåŸ·è¡Œæ¨™æº–çš„ä¸Šä¸‹æ–‡èŠå¤©"""
        if context_messages:
            messages = context_messages
        else:
            messages = [ChatMessage(role="user", content=query)]
        
        is_successful, response, error = self.chat_completion(messages, **kwargs)
        
        if not is_successful:
            return False, None, error
        
        rag_response = RAGResponse(
            answer=response.content,
            sources=[],
            metadata={
                'model': 'gemini', 
                'no_sources': True,
                'context_messages_count': len(messages)
            }
        )
        return True, rag_response, None

    def query_with_rag(self, query: str, thread_id: str = None, **kwargs) -> Tuple[bool, Optional[RAGResponse], Optional[str]]:
        """ä½¿ç”¨ Google Semantic Retrieval é€²è¡Œ RAG æŸ¥è©¢ï¼ˆæ”¯æ´é•·ä¸Šä¸‹æ–‡ï¼‰"""
        try:
            corpus_name = kwargs.get('corpus_name', self.default_corpus_name)
            context_messages = kwargs.get('context_messages', [])
            
            if self.corpora.get(corpus_name) is None:
                return self._fallback_chat_completion(query, context_messages, **kwargs)
            
            # 1. ä½¿ç”¨ Semantic Retrieval æœå°‹ç›¸é—œå…§å®¹
            corpus_data = self.corpora.get(corpus_name)
            if not corpus_data:
                return False, None, f"Corpus {corpus_name} not found"
            corpus_name_full = corpus_data['name']
            query_corpus_endpoint = f'{corpus_name_full}:queryCorpus'
            
            query_data = {
                "query": query,
                "metadataFilters": [],
                "resultsCount": kwargs.get('top_k', 5)
            }
            
            is_successful, retrieval_response, error = self._request('POST', query_corpus_endpoint, body=query_data)
            
            if not is_successful:
                return False, None, error
            
            # 2. æå–æª¢ç´¢åˆ°çš„å…§å®¹
            relevant_passages = retrieval_response.get('relevantChunks', [])
            
            if not relevant_passages:
                return self._fallback_chat_completion(query, context_messages, **kwargs)
            
            # 3. å»ºç«‹åŒ…å«ä¸Šä¸‹æ–‡çš„æç¤º
            context_parts = []
            retrieved_sources = []
            
            for passage in relevant_passages:
                chunk_data = passage.get('chunk', {}).get('data', {})
                text = chunk_data.get('stringValue', '')
                
                if text:
                    context_parts.append(text)
                    
                    # æå–ä¾†æºè³‡è¨Š
                    metadata = passage.get('chunk', {}).get('customMetadata', [])
                    source_file = "Unknown"
                    for meta in metadata:
                        if meta.get('key') == 'source_file':
                            source_file = meta.get('stringValue', 'Unknown')
                            break
                    
                    retrieved_sources.append({
                        'file_id': passage.get('chunkRelevanceScore', 0),
                        'filename': source_file,
                        'text': text[:200] + "..." if len(text) > 200 else text,
                        'relevance_score': passage.get('chunkRelevanceScore', 0)
                    })
            
            context = "\n\n".join(context_parts)
            
            # 4. æ•´åˆæª¢ç´¢çµæœå’Œé•·ä¸Šä¸‹æ–‡
            if context_messages:
                # æœ‰ä¸Šä¸‹æ–‡è¨Šæ¯ï¼Œæ•´åˆæª¢ç´¢å…§å®¹åˆ°å°è©±ä¸­
                enhanced_system_prompt = f"""ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ AI åŠ©ç†ï¼Œå…·æœ‰ä»¥ä¸‹åƒè€ƒè³‡æ–™ï¼š

{context}

è«‹æ ¹æ“šå°è©±æ­·å²å’Œä¸Šè¿°åƒè€ƒè³‡æ–™å›ç­”ç”¨æˆ¶çš„å•é¡Œã€‚å¦‚æœåƒè€ƒè³‡æ–™ä¸­æ²’æœ‰ç›¸é—œè³‡è¨Šï¼Œè«‹åŸºæ–¼å°è©±æ­·å²æä¾›å»ºè­°ã€‚
ç•¶å¼•ç”¨åƒè€ƒè³‡æ–™æ™‚ï¼Œè«‹ä½¿ç”¨ [æ–‡æª”åç¨±] æ ¼å¼æ¨™è¨»ä¾†æºã€‚"""
                
                # æ›´æ–°ç³»çµ±æç¤ºè©
                messages = []
                for msg in context_messages:
                    if msg.role == "system":
                        # æ›¿æ›ç³»çµ±æç¤ºè©
                        messages.append(ChatMessage(role="system", content=enhanced_system_prompt))
                    else:
                        messages.append(msg)
                
                # å¦‚æœæ²’æœ‰ç³»çµ±è¨Šæ¯ï¼Œæ·»åŠ ä¸€å€‹
                if not any(msg.role == "system" for msg in messages):
                    messages.insert(0, ChatMessage(role="system", content=enhanced_system_prompt))
            else:
                # æ²’æœ‰ä¸Šä¸‹æ–‡ï¼Œä½¿ç”¨å‚³çµ± RAG æ–¹å¼
                enhanced_query = f"""è«‹æ ¹æ“šä»¥ä¸‹æ–‡æª”å…§å®¹å›ç­”å•é¡Œï¼š

åƒè€ƒè³‡æ–™ï¼š
{context}

å•é¡Œï¼š{query}

è«‹åŸºæ–¼ä¸Šè¿°åƒè€ƒè³‡æ–™å›ç­”å•é¡Œã€‚å¦‚æœåƒè€ƒè³‡æ–™ä¸­æ²’æœ‰ç›¸é—œè³‡è¨Šï¼Œè«‹æ˜ç¢ºèªªæ˜ã€‚"""
                messages = [ChatMessage(role="user", content=enhanced_query)]
            
            is_successful, response, error = self.chat_completion(messages, **kwargs)
            
            if not is_successful:
                return False, None, error

            # 5. è™•ç†å…§æ–‡å¼•ç”¨
            processed_answer, final_sources = self._process_inline_citations(response.content, retrieved_sources)
            
            rag_response = RAGResponse(
                answer=processed_answer,
                sources=final_sources,
                metadata={
                    'model': 'gemini',
                    'corpus': corpus_name,
                    'num_sources': len(final_sources),
                    'context_length': len(context),
                    'context_messages_count': len(context_messages),
                    'long_context_enabled': len(context_messages) > 5
                }
            )
            
            return True, rag_response, None
            
        except Exception as e:
            return False, None, str(e)
    
    def get_knowledge_files(self) -> Tuple[bool, Optional[List[FileInfo]], Optional[str]]:
        """å–å¾—èªæ–™åº«æª”æ¡ˆåˆ—è¡¨"""
        try:
            files = []
            
            # ç²å–æ‰€æœ‰èªæ–™åº«æ•¸æ“š
            if hasattr(self.corpora, 'cache'):
                # ç›´æ¥å¾ BoundedCache çš„å…§éƒ¨ cache ç²å–é …ç›®
                with self.corpora.lock:
                    corpora_items = list(self.corpora.cache.items())
            else:
                corpora_items = []
            
            for corpus_name, corpus_data in corpora_items:
                corpus_name_full = corpus_data['name']
                documents_endpoint = f'{corpus_name_full}/documents'
                
                is_successful, response, error = self._request('GET', documents_endpoint)
                if not is_successful:
                    continue
                
                for document in response.get('documents', []):
                    file_info = FileInfo(
                        file_id=document['name'].split('/')[-1],
                        filename=document.get('displayName', 'Unknown'),
                        status='processed',
                        purpose='knowledge_base',
                        metadata={
                            'corpus': corpus_name,
                            'document_name': document['name']
                        }
                    )
                    files.append(file_info)
            
            return True, files, None
            
        except Exception as e:
            return False, None, str(e)
    
    def get_file_references(self) -> Dict[str, str]:
        """å–å¾—æª”æ¡ˆå¼•ç”¨å°æ‡‰è¡¨"""
        try:
            is_successful, files, error = self.get_knowledge_files()
            if not is_successful:
                return {}
            
            return {
                file.file_id: file.filename.replace('.txt', '').replace('.json', '')
                for file in files
            }
        except Exception as e:
            return {}
    
    def _create_corpus(self, corpus_name: str) -> Tuple[bool, Optional[Dict], Optional[str]]:
        """å»ºç«‹èªæ–™åº«"""
        try:
            corpus_data = {
                "displayName": corpus_name,
            }
            
            endpoint = '/corpora'
            is_successful, response, error = self._request('POST', endpoint, body=corpus_data)
            
            return is_successful, response, error
            
        except Exception as e:
            return False, None, str(e)
    
    def _chunk_text(self, text: str, chunk_size: int = 1000, overlap: int = 100) -> List[Dict]:
        """å°‡æ–‡æœ¬åˆ†å¡Š"""
        chunks = []
        start = 0
        
        # ç¢ºä¿ overlap ä¸æœƒå¤§æ–¼ chunk_size ä¸¦ä¸”æ˜¯åˆç†çš„
        overlap = min(overlap, chunk_size // 2)
        
        while start < len(text):
            end = min(start + chunk_size, len(text))
            chunk_text = text[start:end]
            
            chunks.append({
                'text': chunk_text,
                'start': start,
                'end': end
            })
            
            # ç¢ºä¿ä¸‹ä¸€å€‹ä½ç½®ç¸½æ˜¯å‘å‰ç§»å‹•
            next_start = end - overlap
            if next_start <= start:  # é˜²æ­¢ç„¡é™å¾ªç’°
                next_start = start + max(1, chunk_size - overlap)
            
            start = next_start
            
            # å¦‚æœå·²ç¶“åˆ°é”æ–‡æœ¬æœ«å°¾ï¼Œåœæ­¢
            if end >= len(text):
                break
        
        return chunks
    
    def _intelligent_chunk_text(self, text: str, chunk_size: int = 1000) -> List[Dict]:
        """
        æ™ºæ…§åˆ†å¡Šæ–‡æœ¬ - é‡å° Gemini Semantic Retrieval å„ªåŒ–
        
        å˜—è©¦åœ¨èªç¾©é‚Šç•Œåˆ†å¡Šï¼Œä¿æŒå…§å®¹å®Œæ•´æ€§
        """
        import re
        
        # åŸºæœ¬åˆ†å¡Š
        basic_chunks = self._chunk_text(text, chunk_size, overlap=100)
        
        # æ™ºæ…§è™•ç†ï¼šå˜—è©¦åœ¨æ®µè½é‚Šç•Œåˆ†å¡Š
        intelligent_chunks = []
        for i, chunk in enumerate(basic_chunks):
            chunk_text = chunk['text']
            
            # å˜—è©¦åœ¨å¥å­é‚Šç•Œèª¿æ•´
            sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]\s+', chunk_text)
            if len(sentences) > 1:
                # é‡æ–°çµ„åˆå¥å­ï¼Œç¢ºä¿ä¸è¶…é chunk_size
                optimized_text = ""
                for sentence in sentences:
                    if len(optimized_text + sentence) <= chunk_size:
                        optimized_text += sentence + "ã€‚"
                    else:
                        break
                
                if optimized_text:
                    chunk_text = optimized_text.strip()
            
            intelligent_chunks.append({
                'text': chunk_text,
                'start': chunk['start'],
                'end': chunk['end'],
                'tokens': len(chunk_text.split()),  # ä¼°ç®— token æ•¸é‡
                'section': f'chunk_{i}'  # å€æ®µæ¨™è­˜
            })
        
        return intelligent_chunks
    
    @retry_on_rate_limit(max_retries=3, base_delay=1.0)
    def _request(self, method: str, endpoint: str, body=None, files=None):
        """ç™¼é€ HTTP è«‹æ±‚åˆ° Gemini API"""
        
        # è™•ç† endpoint
        if not endpoint.startswith('/'):
            endpoint = '/' + endpoint
        
        url = f'{self.base_url}{endpoint}'
        if '?' in url:
            url += f'&key={self.api_key}'
        else:
            url += f'?key={self.api_key}'
        
        headers = {
            'Content-Type': 'application/json'
        }
        
        try:
            if method == 'POST':
                r = requests.post(url, headers=headers, json=body, timeout=30)
            elif method == 'GET':
                r = requests.get(url, headers=headers, timeout=30)
            else:
                return False, None, f"Unsupported method: {method}"
            
            # æª¢æŸ¥ HTTP ç‹€æ…‹ç¢¼
            if r.status_code == 429:
                raise requests.exceptions.RequestException(f"Rate limit exceeded: {r.status_code}")
            elif r.status_code >= 500:
                raise requests.exceptions.RequestException(f"Server error: {r.status_code}")
            elif r.status_code >= 400:
                try:
                    error_data = r.json()
                    error_msg = error_data.get('error', {}).get('message', f'HTTP {r.status_code}')
                    return False, None, error_msg
                except:
                    return False, None, f'HTTP {r.status_code}: {r.text[:200]}'
            
            response_data = r.json()
            return True, response_data, None
            
        except requests.exceptions.RequestException as e:
            raise e
        except Exception as e:
            return False, None, f'Gemini API ç³»çµ±ä¸ç©©å®šï¼Œè«‹ç¨å¾Œå†è©¦: {str(e)}'
    
    # === å…¶ä»–ä»‹é¢ï¼ˆéƒ¨åˆ†å¯¦ä½œï¼‰ ===
    
    def create_thread(self) -> Tuple[bool, Optional[ThreadInfo], Optional[str]]:
        """å»ºç«‹ç°¡å–®çš„å°è©±ä¸² ID"""
        import uuid
        thread_id = str(uuid.uuid4())
        thread_info = ThreadInfo(thread_id=thread_id)
        return True, thread_info, None
    
    def delete_thread(self, thread_id: str) -> Tuple[bool, Optional[str]]:
        """åˆªé™¤å°è©±ä¸²ï¼ˆGemini ä¸æ”¯æ´ï¼Œç›´æ¥è¿”å›æˆåŠŸï¼‰"""
        return True, None
    
    def add_message_to_thread(self, thread_id: str, message: ChatMessage) -> Tuple[bool, Optional[str]]:
        """æ·»åŠ è¨Šæ¯åˆ°å°è©±ä¸²ï¼ˆGemini ä¸æ”¯æ´ï¼Œç›´æ¥è¿”å›æˆåŠŸï¼‰"""
        return True, None
    
    def run_assistant(self, thread_id: str, **kwargs) -> Tuple[bool, Optional[RAGResponse], Optional[str]]:
        """åŸ·è¡ŒåŠ©ç†ï¼ˆä½¿ç”¨ RAG æŸ¥è©¢ï¼‰"""
        return False, None, "è«‹ä½¿ç”¨ query_with_rag æ–¹æ³•"
    
    def transcribe_audio(self, audio_file_path: str, **kwargs) -> Tuple[bool, Optional[str], Optional[str]]:
        """ä½¿ç”¨ Gemini Pro çš„å¤šæ¨¡æ…‹èƒ½åŠ›é€²è¡ŒéŸ³è¨Šè½‰éŒ„"""
        try:
            import base64
            import mimetypes

            # è®€å–éŸ³è¨Šæª”æ¡ˆä¸¦é€²è¡Œ Base64 ç·¨ç¢¼
            with open(audio_file_path, "rb") as f:
                audio_data = f.read()
            
            encoded_audio = base64.b64encode(audio_data).decode("utf-8")
            
            # çŒœæ¸¬ MIME é¡å‹
            mime_type, _ = mimetypes.guess_type(audio_file_path)
            if not mime_type:
                mime_type = "audio/wav"  # é è¨­å€¼
            if mime_type == "audio/x-wav":
                mime_type = "audio/wav" # æ¨™æº–åŒ– MIME é¡å‹

            # å»ºç«‹è«‹æ±‚
            json_body = {
                "contents": [
                    {
                        "parts": [
                            {"text": "è«‹å°‡é€™æ®µéŸ³è¨Šè½‰éŒ„æˆæ–‡å­—ã€‚"},
                            {
                                "inline_data": {
                                    "mime_type": mime_type,
                                    "data": encoded_audio
                                }
                            }
                        ]
                    }
                ]
            }
            
            endpoint = f'/models/{self.model_name}:generateContent'
            is_successful, response, error_message = self._request('POST', endpoint, body=json_body)

            if not is_successful:
                return False, None, error_message

            if 'candidates' not in response or not response['candidates']:
                return False, None, "No response generated from audio"

            candidate = response['candidates'][0]
            if 'content' not in candidate or not candidate['content']['parts']:
                return False, None, "No content in response from audio"

            transcribed_text = candidate['content']['parts'][0]['text']
            return True, transcribed_text.strip(), None

        except FileNotFoundError:
            return False, None, f"Audio file not found at: {audio_file_path}"
        except Exception as e:
            logger.error(f"Gemini audio transcription failed: {e}")
            return False, None, str(e)
    
    def generate_image(self, prompt: str, **kwargs) -> Tuple[bool, Optional[str], Optional[str]]:
        """åœ–ç‰‡ç”Ÿæˆï¼ˆGemini ä¸æ”¯æ´ï¼‰"""
        return False, None, "Gemini ç›®å‰ä¸æ”¯æ´åœ–ç‰‡ç”Ÿæˆ"
    
    # === ğŸ†• æ–°çš„ç”¨æˆ¶ç´šå°è©±ç®¡ç†æ¥å£ ===
    
    def chat_with_user(self, user_id: str, message: str, platform: str = 'line', **kwargs) -> Tuple[bool, Optional[RAGResponse], Optional[str]]:
        """
        ä¸»è¦å°è©±æ¥å£ï¼šé•·ä¸Šä¸‹æ–‡å°è©±æ­·å² + Semantic Retrieval RAG
        
        åˆ©ç”¨ Gemini 1.5 Pro çš„ 1M token ä¸Šä¸‹æ–‡çª—å£å„ªå‹¢
        
        Args:
            user_id: ç”¨æˆ¶ ID (å¦‚ Line user ID)
            message: ç”¨æˆ¶è¨Šæ¯
            platform: å¹³å°è­˜åˆ¥ ('line', 'discord', 'telegram')
            **kwargs: é¡å¤–åƒæ•¸
                - conversation_limit: å°è©±æ­·å²è¼ªæ•¸ï¼Œé è¨­20ï¼ˆåˆ©ç”¨é•·ä¸Šä¸‹æ–‡ï¼‰
                - corpus_name: èªæ–™åº«åç¨±
                
        Returns:
            (is_successful, rag_response, error_message)
        """
        try:
            # 1. å–å¾—è¼ƒé•·çš„å°è©±æ­·å²ï¼ˆåˆ©ç”¨ 1M token å„ªå‹¢ï¼‰
            conversation_limit = kwargs.get('conversation_limit', 20)  # æ¯”å…¶ä»–æ¨¡å‹æ›´å¤š
            recent_conversations = self._get_recent_conversations(user_id, platform, limit=conversation_limit)
            
            # 2. å„²å­˜ç”¨æˆ¶è¨Šæ¯
            self.conversation_manager.add_message(user_id, 'gemini', 'user', message, platform)
            
            # 3. å»ºç«‹é•·ä¸Šä¸‹æ–‡å°è©±
            messages = self._build_long_conversation_context(recent_conversations, message)
            
            # 4. ä½¿ç”¨ Semantic Retrieval API é€²è¡Œ RAG æŸ¥è©¢ï¼ˆåŒ…å«é•·ä¸Šä¸‹æ–‡ï¼‰
            rag_kwargs = {**kwargs, 'context_messages': messages}
            is_successful, rag_response, error = self.query_with_rag(message, **rag_kwargs)
            
            if not is_successful:
                return False, None, error
            
            # 5. å„²å­˜åŠ©ç†å›æ‡‰
            self.conversation_manager.add_message(user_id, 'gemini', 'assistant', rag_response.answer, platform)
            
            # 6. æ›´æ–° metadataï¼ˆåŠ å…¥é•·ä¸Šä¸‹æ–‡è³‡è¨Šï¼‰
            rag_response.metadata.update({
                'conversation_turns': len(recent_conversations),
                'context_tokens_used': len(str(messages)),  # ä¼°ç®—å€¼
                'long_context_enabled': len(recent_conversations) > 10,
                'user_id': user_id,
                'model_provider': 'gemini'
            })
            
            logger.info(f"Completed Gemini chat with user {user_id}, context turns: {len(recent_conversations)}, response length: {len(rag_response.answer)}")
            return True, rag_response, None
            
        except Exception as e:
            logger.error(f"Error in chat_with_user for user {user_id}: {e}")
            return False, None, str(e)
    
    def clear_user_history(self, user_id: str, platform: str = 'line') -> Tuple[bool, Optional[str]]:
        """æ¸…é™¤ç”¨æˆ¶å°è©±æ­·å²"""
        try:
            success = self.conversation_manager.clear_user_history(user_id, 'gemini', platform)
            if success:
                logger.info(f"Cleared conversation history for user {user_id}")
                return True, None
            else:
                return False, "Failed to clear conversation history"
        except Exception as e:
            logger.error(f"Error clearing history for user {user_id}: {e}")
            return False, str(e)
    
    def _get_recent_conversations(self, user_id: str, platform: str = 'line', limit: int = 20) -> List[Dict]:
        """å–å¾—ç”¨æˆ¶æœ€è¿‘çš„å°è©±æ­·å²ï¼ˆæ”¯æ´é•·ä¸Šä¸‹æ–‡ï¼‰"""
        try:
            return self.conversation_manager.get_recent_conversations(user_id, 'gemini', limit, platform)
        except Exception as e:
            logger.warning(f"Failed to get recent conversations for user {user_id}: {e}")
            return []
    
    def _build_long_conversation_context(self, recent_conversations: List[Dict], current_message: str) -> List[ChatMessage]:
        """
        å»ºç«‹é•·ä¸Šä¸‹æ–‡å°è©±ï¼ˆåˆ©ç”¨ Gemini 1M token å„ªå‹¢ï¼‰
        
        èˆ‡å…¶ä»–æ¨¡å‹ä¸åŒï¼ŒGemini å¯ä»¥åŒ…å«æ›´å¤šæ­·å²å°è©±
        """
        messages = []
        
        # æ·»åŠ ç³»çµ±è¨Šæ¯
        system_prompt = self._build_system_prompt_with_context()
        messages.append(ChatMessage(role='system', content=system_prompt))
        
        # æ·»åŠ æ›´å¤šå°è©±æ­·å²ï¼ˆæœ€å¤šå–æœ€è¿‘ 40 è¼ªå°è©±ï¼Œå—é™æ–¼å¯¦éš›å¯ç”¨ä¸Šä¸‹æ–‡ï¼‰
        max_history = min(len(recent_conversations), 40)
        for conv in recent_conversations[-max_history:]:
            messages.append(ChatMessage(
                role=conv['role'],
                content=conv['content']
            ))
        
        # æ·»åŠ ç•¶å‰è¨Šæ¯
        messages.append(ChatMessage(role='user', content=current_message))
        
        logger.debug(f"Built long context with {len(messages)} messages")
        return messages
    
    def _build_system_prompt_with_context(self) -> str:
        """å»ºç«‹åŒ…å«èªç¾©æª¢ç´¢æŒ‡å°çš„ç³»çµ±æç¤ºè©"""
        return """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ AI åŠ©ç†ï¼Œå…·æœ‰ä»¥ä¸‹èƒ½åŠ›å’Œç‰¹è³ªï¼š

## æ ¸å¿ƒèƒ½åŠ›
- é•·æœŸå°è©±è¨˜æ†¶ï¼šå¯ä»¥è¨˜ä½ä¸¦å¼•ç”¨è¼ƒé•·çš„å°è©±æ­·å²
- èªç¾©æª¢ç´¢ï¼šåŸºæ–¼ Google Semantic Retrieval API çš„çŸ¥è­˜æª¢ç´¢
- å¤šæ¨¡æ…‹ç†è§£ï¼šæ”¯æ´æ–‡å­—ã€åœ–ç‰‡ã€å½±ç‰‡ç­‰å¤šç¨®å…§å®¹æ ¼å¼
- ä¸Šä¸‹æ–‡åˆ†æï¼šå……åˆ†åˆ©ç”¨ 100è¬ token çš„é•·ä¸Šä¸‹æ–‡çª—å£

## å›ç­”åŸå‰‡
1. å……åˆ†åˆ©ç”¨æä¾›çš„å°è©±æ­·å²ï¼Œå»ºç«‹é€£è²«çš„å°è©±é«”é©—
2. ç•¶å¼•ç”¨çŸ¥è­˜æ–‡æª”æ™‚ï¼Œè«‹ä½¿ç”¨ [æ–‡æª”åç¨±] æ ¼å¼æ¨™è¨»ä¾†æº
3. å°æ–¼è¤‡é›œå•é¡Œï¼Œå¯ä»¥åƒè€ƒå‰é¢çš„å°è©±å…§å®¹æä¾›æ›´å¥½çš„å›ç­”
4. å¦‚æ–‡æª”ä¸­ç„¡ç›¸é—œè³‡è¨Šï¼Œæ˜ç¢ºèªªæ˜ä¸¦æä¾›åŸºæ–¼å°è©±æ­·å²çš„å»ºè­°
5. ä¿æŒå°ˆæ¥­ä½†å‹å–„çš„èªèª¿ï¼Œå±•ç¾é•·æœŸå°è©±çš„é€£çºŒæ€§

## å›ç­”æ ¼å¼
- é©ç•¶å¼•ç”¨å‰é¢çš„å°è©±å…§å®¹ï¼ˆå¦‚ï¼š"å¦‚æˆ‘å€‘ä¹‹å‰è¨è«–çš„..."ï¼‰
- ä½¿ç”¨æ¸…æ™°çš„æ®µè½çµæ§‹å’Œæ¢åˆ—å¼é‡é»
- é‡è¦è³‡è¨Šä½¿ç”¨é©ç•¶çš„æ ¼å¼çªå‡º
- åœ¨å›ç­”æœ«å°¾æ¨™è¨»ç›¸é—œçš„çŸ¥è­˜æ–‡æª”ä¾†æº

è«‹å§‹çµ‚ä¿æŒé€™å€‹è§’è‰²è¨­å®šï¼Œå……åˆ†ç™¼æ®é•·ä¸Šä¸‹æ–‡å’Œèªç¾©æª¢ç´¢çš„å„ªå‹¢ã€‚"""

    def _upload_multimodal_file(self, file_path: str, corpus_name: str, **kwargs) -> Tuple[bool, Optional[FileInfo], Optional[str]]:
        """
        ä¸Šå‚³å¤šæ¨¡æ…‹æª”æ¡ˆåˆ° Google Cloud Storage ä¸¦èˆ‡ Semantic Retrieval æ•´åˆ
        
        æ­¤åŠŸèƒ½ç‚ºé€²éšå¯¦ä½œï¼Œéœ€è¦å®Œæ•´çš„ Google Cloud Storage (GCS) æ•´åˆï¼Œç›®å‰å°šæœªå•Ÿç”¨ã€‚
        å•Ÿç”¨æ­¤åŠŸèƒ½å‰ï¼Œè«‹ç¢ºä¿ï¼š
        1. å·²å®‰è£ `google-cloud-storage` Python å¥—ä»¶ã€‚
        2. æ‡‰ç”¨ç¨‹å¼åŸ·è¡Œç’°å¢ƒå·²è¨­å®š GCS æœå‹™å¸³è™Ÿæ†‘è­‰ã€‚
        3. å·²åœ¨ Google Cloud å°ˆæ¡ˆä¸­å»ºç«‹ä¸€å€‹ GCS å„²å­˜æ¡¶ (bucket)ã€‚
        """
        logger.warning("Multimodal file upload is not fully implemented and requires Google Cloud Storage integration.")
        return False, None, "å¤šæ¨¡æ…‹æª”æ¡ˆä¸Šå‚³åŠŸèƒ½å°šæœªå®Œæ•´å¯¦ä½œï¼Œéœ€è¦ Google Cloud Storage æ•´åˆã€‚"
        
        # å®Œæ•´å¯¦ä½œæµç¨‹ç¯„ä¾‹ï¼š
        # try:
        #     from google.cloud import storage
        #     import os

        #     # å¾è¨­å®šæˆ–ç’°å¢ƒè®Šæ•¸è®€å– GCS bucket åç¨±
        #     bucket_name = os.environ.get("GCS_BUCKET_NAME")
        #     if not bucket_name:
        #         logger.error("GCS_BUCKET_NAME environment variable is not set.")
        #         return False, None, "GCS bucket æœªè¨­å®š"

        #     storage_client = storage.Client()
        #     bucket = storage_client.bucket(bucket_name)
            
        #     destination_blob_name = f"multimodal_uploads/{os.path.basename(file_path)}"
        #     blob = bucket.blob(destination_blob_name)

        #     logger.info(f"Uploading {file_path} to GCS bucket {bucket_name}...")
        #     blob.upload_from_filename(file_path)
        #     gcs_uri = f"gs://{bucket_name}/{destination_blob_name}"
        #     logger.info(f"File uploaded to {gcs_uri}")

        #     # TODO: å¾ŒçºŒéœ€è¦èª¿ç”¨ Google File API æˆ–é¡ä¼¼æœå‹™ï¼Œ
        #     # å°‡ GCS URI èˆ‡ Semantic Retrieval çš„æ–‡æª”é€²è¡Œé—œè¯ã€‚
        #     # é€™éƒ¨åˆ†çš„ API å¯èƒ½æœƒéš¨ Google Cloud çš„æ›´æ–°è€Œè®ŠåŒ–ã€‚

        #     file_info = FileInfo(
        #         file_id=gcs_uri, # æš«æ™‚ä½¿ç”¨ GCS URI ä½œç‚º ID
        #         filename=os.path.basename(file_path),
        #         status='uploaded_to_gcs',
        #         purpose='multimodal_knowledge',
        #         metadata={'gcs_uri': gcs_uri, 'corpus': corpus_name}
        #     )
            
        #     return True, file_info, None

        # except ImportError:
        #     logger.error("google-cloud-storage is not installed. Please install it to use multimodal features.")
        #     return False, None, "ç¼ºå°‘ google-cloud-storage å¥—ä»¶"
        # except Exception as e:
        #     logger.error(f"Failed to upload multimodal file to GCS: {e}")
        #     return False, None, f"å¤šæ¨¡æ…‹æª”æ¡ˆä¸Šå‚³å¤±æ•—: {e}"