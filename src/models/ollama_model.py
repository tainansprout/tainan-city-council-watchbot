import requests
import json
import hashlib
from typing import List, Dict, Tuple, Optional
from .base import (
    FullLLMInterface, 
    ModelProvider, 
    ChatMessage, 
    ChatResponse, 
    ThreadInfo, 
    FileInfo,
    KnowledgeBase,
    RAGResponse
)
from ..utils.retry import retry_on_rate_limit
from ..services.conversation import get_conversation_manager
import logging
import time

logger = logging.getLogger(__name__)


class OllamaModel(FullLLMInterface):
    """
    Ollama æœ¬åœ°æ¨¡å‹å¯¦ä½œ
    
    ä½¿ç”¨ Ollama API + æœ¬åœ°å‘é‡è³‡æ–™åº«å¯¦ç¾ RAG åŠŸèƒ½
    æ”¯æ´ llama2, codellama, mistral ç­‰æœ¬åœ°æ¨¡å‹
    """
    
    def __init__(self, base_url: str = "http://localhost:11434", model_name: str = "llama3.1:8b", embedding_model: str = "nomic-embed-text"):
        self.base_url = base_url.rstrip('/')
        self.model_name = model_name
        self.embedding_model = embedding_model  # æœ¬åœ° embedding æ¨¡å‹
        
        # æœ¬åœ°çŸ¥è­˜åº«å’Œå‘é‡å¿«å–
        self.knowledge_store = {}  # æœ¬åœ°çŸ¥è­˜åº«
        self.embeddings_cache = {}  # åµŒå…¥å‘é‡å¿«å–
        
        # æœ¬åœ°å¿«å–é…ç½®ï¼ˆéš±ç§ä¿è­·ï¼‰
        self.local_cache_enabled = True
        self.max_cache_size = 1000
        self.conversation_cache = {}  # æœ¬åœ°å°è©±å¿«å–
        
        # å°è©±æ­·å²ç®¡ç†
        self.conversation_manager = get_conversation_manager()
        
        # æœ¬åœ° Whisper æ”¯æ´
        self.whisper_model = None  # éœ€è¦é¡å¤–è¨­å®š
    
    def get_provider(self) -> ModelProvider:
        return ModelProvider.OLLAMA
    
    def check_connection(self) -> Tuple[bool, Optional[str]]:
        """æª¢æŸ¥ Ollama é€£ç·šå’Œæ¨¡å‹å¯ç”¨æ€§"""
        try:
            # æª¢æŸ¥ Ollama æœå‹™
            response = requests.get(f"{self.base_url}/api/tags", timeout=10)
            if response.status_code != 200:
                return False, f"Ollama æœå‹™ä¸å¯ç”¨: {response.status_code}"
            
            # æª¢æŸ¥æŒ‡å®šæ¨¡å‹æ˜¯å¦å¯ç”¨
            models = response.json().get('models', [])
            model_names = [model['name'] for model in models]
            
            if not any(self.model_name in name for name in model_names):
                return False, f"æ¨¡å‹ {self.model_name} ä¸å¯ç”¨ï¼Œå¯ç”¨æ¨¡å‹ï¼š{model_names}"
            
            return True, None
            
        except Exception as e:
            return False, str(e)
    
    @retry_on_rate_limit(max_retries=3, base_delay=1.0)
    def chat_completion(self, messages: List[ChatMessage], **kwargs) -> Tuple[bool, Optional[ChatResponse], Optional[str]]:
        """Ollama Chat Completion"""
        try:
            # è½‰æ›è¨Šæ¯æ ¼å¼
            ollama_messages = [
                {"role": msg.role, "content": msg.content}
                for msg in messages
            ]
            
            json_body = {
                "model": kwargs.get('model', self.model_name),
                "messages": ollama_messages,
                "stream": False,
                "options": {
                    "temperature": kwargs.get('temperature', 0.1),
                    "num_predict": kwargs.get('max_tokens', 2000)
                }
            }
            
            is_successful, response, error_message = self._request('POST', '/api/chat', body=json_body)
            
            if not is_successful:
                return False, None, error_message
            
            content = response.get('message', {}).get('content', '')
            
            chat_response = ChatResponse(
                content=content,
                finish_reason=response.get('done_reason', 'stop'),
                metadata={
                    'model': response.get('model'),
                    'total_duration': response.get('total_duration'),
                    'load_duration': response.get('load_duration'),
                    'prompt_eval_count': response.get('prompt_eval_count'),
                    'eval_count': response.get('eval_count')
                }
            )
            
            return True, chat_response, None
            
        except Exception as e:
            return False, None, str(e)
    
    # === RAG ä»‹é¢å¯¦ä½œï¼ˆä½¿ç”¨æœ¬åœ°å‘é‡æœå°‹ï¼‰ ===
    
    def upload_knowledge_file(self, file_path: str, **kwargs) -> Tuple[bool, Optional[FileInfo], Optional[str]]:
        """ä¸Šå‚³æª”æ¡ˆåˆ°æœ¬åœ°çŸ¥è­˜åº«"""
        try:
            import os
            
            # ç”Ÿæˆæª”æ¡ˆ ID
            file_id = hashlib.md5(file_path.encode()).hexdigest()
            filename = os.path.basename(file_path)
            
            # è®€å–æª”æ¡ˆå…§å®¹
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # åˆ†å¡Š
            chunks = self._chunk_text(content, chunk_size=kwargs.get('chunk_size', 800))
            
            # ç”ŸæˆåµŒå…¥å‘é‡ï¼ˆä½¿ç”¨ Ollama çš„ embedding åŠŸèƒ½ï¼‰
            chunk_embeddings = []
            for chunk in chunks:
                embedding = self._get_embedding(chunk['text'])
                if embedding:
                    chunk['embedding'] = embedding
                    chunk_embeddings.append(chunk)
            
            # å„²å­˜åˆ°çŸ¥è­˜åº«
            self.knowledge_store[file_id] = {
                'filename': filename,
                'content': content,
                'chunks': chunk_embeddings,
                'metadata': kwargs
            }
            
            file_info = FileInfo(
                file_id=file_id,
                filename=filename,
                size=len(content),
                status='processed',
                purpose='knowledge_base',
                metadata={'chunks': len(chunk_embeddings)}
            )
            
            return True, file_info, None
            
        except Exception as e:
            return False, None, str(e)
    
    def query_with_rag(self, query: str, thread_id: str = None, **kwargs) -> Tuple[bool, Optional[RAGResponse], Optional[str]]:
        """ä½¿ç”¨æœ¬åœ°å‘é‡æœå°‹é€²è¡Œ RAG æŸ¥è©¢ï¼ˆæ”¯æ´ä¸Šä¸‹æ–‡å’Œéš±ç§æ¨¡å¼ï¼‰"""
        try:
            context_messages = kwargs.get('context_messages', [])  # æœ¬åœ°ä¸Šä¸‹æ–‡æ”¯æ´
            local_only = kwargs.get('local_only', True)  # æœ¬åœ°å„ªå…ˆæ¨¡å¼
            
            # ç”ŸæˆæŸ¥è©¢çš„åµŒå…¥å‘é‡ï¼ˆæœ¬åœ°è™•ç†ï¼‰
            query_embedding = self._get_embedding(query)
            
            if not query_embedding:
                # ç„¡æ³•ç”ŸæˆåµŒå…¥å‘é‡ï¼Œä½¿ç”¨æœ¬åœ°å°è©±
                if context_messages:
                    # ä½¿ç”¨ä¸Šä¸‹æ–‡è¨Šæ¯
                    messages = context_messages
                else:
                    # åªä½¿ç”¨ç•¶å‰æŸ¥è©¢
                    messages = [ChatMessage(role="user", content=query)]
                
                is_successful, response, error = self.chat_completion(messages, **kwargs)
                
                if not is_successful:
                    return False, None, error
                
                rag_response = RAGResponse(
                    answer=response.content,
                    sources=[],
                    metadata={
                        'model': 'ollama', 
                        'no_embedding': True,
                        'local_processing': local_only,
                        'context_messages_count': len(context_messages)
                    }
                )
                return True, rag_response, None
            
            # æœå°‹ç›¸é—œæ–‡æª”ç‰‡æ®µ
            relevant_chunks = self._vector_search(query_embedding, top_k=kwargs.get('top_k', 3))
            
            if not relevant_chunks:
                # æ²’æœ‰ç›¸é—œæ–‡æª”ï¼Œä½¿ç”¨æœ¬åœ°å°è©±
                if context_messages:
                    # ä½¿ç”¨ä¸Šä¸‹æ–‡è¨Šæ¯
                    messages = context_messages
                else:
                    # åªä½¿ç”¨ç•¶å‰æŸ¥è©¢
                    messages = [ChatMessage(role="user", content=query)]
                
                is_successful, response, error = self.chat_completion(messages, **kwargs)
                
                if not is_successful:
                    return False, None, error
                
                rag_response = RAGResponse(
                    answer=response.content,
                    sources=[],
                    metadata={
                        'model': 'ollama', 
                        'no_sources': True,
                        'local_processing': local_only,
                        'context_messages_count': len(context_messages)
                    }
                )
                return True, rag_response, None
            
            # æ•´åˆæœ¬åœ°çŸ¥è­˜åº«å’Œå°è©±ä¸Šä¸‹æ–‡
            context = "\\n\\n".join([chunk['text'] for chunk in relevant_chunks])
            
            if context_messages:
                # æœ‰ä¸Šä¸‹æ–‡è¨Šæ¯ï¼Œæ•´åˆæœ¬åœ°çŸ¥è­˜åº«
                enhanced_system_prompt = f"""ä½ æ˜¯ä¸€å€‹æœ¬åœ°åŒ–çš„ AI åŠ©ç†ï¼Œå…·æœ‰ä»¥ä¸‹æœ¬åœ°çŸ¥è­˜åº«è³‡æ–™ï¼š

{context}

è«‹æ ¹æ“šå°è©±æ­·å²å’Œä¸Šè¿°æœ¬åœ°çŸ¥è­˜åº«å›ç­”ç”¨æˆ¶çš„å•é¡Œã€‚æ‰€æœ‰è™•ç†éƒ½åœ¨æœ¬åœ°é€²è¡Œï¼Œä¿è­·ç”¨æˆ¶éš±ç§ã€‚
ç•¶å¼•ç”¨çŸ¥è­˜åº«æ™‚ï¼Œè«‹ä½¿ç”¨ [æ–‡æª”åç¨±] æ ¼å¼æ¨™è¨»ä¾†æºã€‚"""
                
                # æ›´æ–°ç³»çµ±æç¤ºè©
                messages = []
                for msg in context_messages:
                    if msg.role == "system":
                        # æ›¿æ›ç³»çµ±æç¤ºè©
                        messages.append(ChatMessage(role="system", content=enhanced_system_prompt))
                    else:
                        messages.append(msg)
                
                # å¦‚æœæ²’æœ‰ç³»çµ±è¨Šæ¯ï¼Œæ·»åŠ ä¸€å€‹
                if not any(msg.role == "system" for msg in messages):
                    messages.insert(0, ChatMessage(role="system", content=enhanced_system_prompt))
            else:
                # å‚³çµ± RAG æ–¹å¼
                enhanced_query = f"""æ ¹æ“šä»¥ä¸‹æœ¬åœ°çŸ¥è­˜åº«è³‡è¨Šå›ç­”å•é¡Œï¼š

æœ¬åœ°çŸ¥è­˜åº«ï¼š
{context}

å•é¡Œï¼š{query}

è«‹åŸºæ–¼æœ¬åœ°çŸ¥è­˜åº«è³‡è¨Šå›ç­”å•é¡Œã€‚å¦‚æœçŸ¥è­˜åº«ä¸­æ²’æœ‰ç›¸é—œè³‡è¨Šï¼Œè«‹æ˜ç¢ºèªªæ˜ã€‚æ‰€æœ‰è™•ç†éƒ½åœ¨æœ¬åœ°é€²è¡Œã€‚"""
                messages = [ChatMessage(role="user", content=enhanced_query)]
            
            is_successful, response, error = self.chat_completion(messages, **kwargs)
            
            if not is_successful:
                return False, None, error
            
            # æº–å‚™ä¾†æºè³‡è¨Š
            sources = [
                {
                    'file_id': chunk['file_id'],
                    'filename': chunk['filename'],
                    'text': chunk['text'][:200] + "..." if len(chunk['text']) > 200 else chunk['text'],
                    'similarity': chunk.get('similarity', 0.0)
                }
                for chunk in relevant_chunks
            ]
            
            rag_response = RAGResponse(
                answer=response.content,
                sources=sources,
                metadata={
                    'model': 'ollama',
                    'context_length': len(context),
                    'num_sources': len(sources),
                    'local_processing': local_only,
                    'context_messages_count': len(context_messages),
                    'embedding_model': self.embedding_model,
                    'privacy_protected': True
                }
            )
            
            return True, rag_response, None
            
        except Exception as e:
            return False, None, str(e)
    
    def get_knowledge_files(self) -> Tuple[bool, Optional[List[FileInfo]], Optional[str]]:
        """å–å¾—çŸ¥è­˜åº«æª”æ¡ˆåˆ—è¡¨"""
        try:
            files = []
            for file_id, data in self.knowledge_store.items():
                file_info = FileInfo(
                    file_id=file_id,
                    filename=data['filename'],
                    size=len(data['content']),
                    status='processed',
                    purpose='knowledge_base',
                    metadata=data.get('metadata', {})
                )
                files.append(file_info)
            
            return True, files, None
            
        except Exception as e:
            return False, None, str(e)
    
    def get_file_references(self) -> Dict[str, str]:
        """å–å¾—æª”æ¡ˆå¼•ç”¨å°æ‡‰è¡¨"""
        return {
            file_id: data['filename'].replace('.txt', '').replace('.json', '')
            for file_id, data in self.knowledge_store.items()
        }
    
    def _get_embedding(self, text: str) -> Optional[List[float]]:
        """ä½¿ç”¨ Ollama ç”ŸæˆåµŒå…¥å‘é‡"""
        try:
            # æª¢æŸ¥å¿«å–
            text_hash = hashlib.md5(text.encode()).hexdigest()
            if text_hash in self.embeddings_cache:
                return self.embeddings_cache[text_hash]
            
            # ä½¿ç”¨ Ollama çš„ embedding åŠŸèƒ½
            json_body = {
                "model": self.model_name,
                "prompt": text
            }
            
            is_successful, response, error = self._request('POST', '/api/embeddings', body=json_body)
            
            if not is_successful:
                return None
            
            embedding = response.get('embedding')
            if embedding:
                # å¿«å–çµæœ
                self.embeddings_cache[text_hash] = embedding
                return embedding
            
            return None
            
        except Exception:
            return None
    
    def _chunk_text(self, text: str, chunk_size: int = 800, overlap: int = 100) -> List[Dict]:
        """å°‡æ–‡æœ¬åˆ†å¡Š"""
        chunks = []
        start = 0
        
        while start < len(text):
            end = min(start + chunk_size, len(text))
            chunk_text = text[start:end]
            
            chunks.append({
                'text': chunk_text,
                'start': start,
                'end': end
            })
            
            start = end - overlap
            if start <= 0:
                break
        
        return chunks
    
    def _vector_search(self, query_embedding: List[float], top_k: int = 3) -> List[Dict]:
        """å‘é‡ç›¸ä¼¼åº¦æœå°‹"""
        try:
            scored_chunks = []
            
            for file_id, data in self.knowledge_store.items():
                for chunk in data['chunks']:
                    if 'embedding' not in chunk:
                        continue
                    
                    # è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦
                    similarity = self._cosine_similarity(query_embedding, chunk['embedding'])
                    
                    if similarity > 0.1:  # è¨­å®šæœ€ä½ç›¸ä¼¼åº¦é–¾å€¼
                        scored_chunks.append({
                            'file_id': file_id,
                            'filename': data['filename'],
                            'text': chunk['text'],
                            'similarity': similarity
                        })
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åºä¸¦è¿”å›å‰ top_k å€‹
            scored_chunks.sort(key=lambda x: x['similarity'], reverse=True)
            return scored_chunks[:top_k]
            
        except Exception:
            return []
    
    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦"""
        try:
            dot_product = sum(a * b for a, b in zip(vec1, vec2))
            norm1 = sum(a * a for a in vec1) ** 0.5
            norm2 = sum(b * b for b in vec2) ** 0.5
            
            if norm1 == 0 or norm2 == 0:
                return 0
            
            return dot_product / (norm1 * norm2)
            
        except Exception:
            return 0
    
    def _request(self, method: str, endpoint: str, body=None, files=None):
        """ç™¼é€ HTTP è«‹æ±‚åˆ° Ollama API"""
        try:
            url = f'{self.base_url}{endpoint}'
            headers = {'Content-Type': 'application/json'}
            
            if method == 'POST':
                r = requests.post(url, headers=headers, json=body, timeout=60)
            elif method == 'GET':
                r = requests.get(url, headers=headers, timeout=30)
            else:
                return False, None, f"Unsupported method: {method}"
            
            # æª¢æŸ¥ HTTP ç‹€æ…‹ç¢¼
            if r.status_code >= 400:
                try:
                    error_data = r.json()
                    error_msg = error_data.get('error', f'HTTP {r.status_code}')
                    return False, None, error_msg
                except:
                    return False, None, f'HTTP {r.status_code}: {r.text[:200]}'
            
            response_data = r.json()
            return True, response_data, None
            
        except requests.exceptions.RequestException as e:
            return False, None, f'Ollama é€£ç·šéŒ¯èª¤: {str(e)}'
        except Exception as e:
            return False, None, f'Ollama API éŒ¯èª¤: {str(e)}'
    
    # === å…¶ä»–ä»‹é¢ï¼ˆéƒ¨åˆ†å¯¦ä½œï¼‰ ===
    
    def create_thread(self) -> Tuple[bool, Optional[ThreadInfo], Optional[str]]:
        """å»ºç«‹ç°¡å–®çš„å°è©±ä¸² ID"""
        import uuid
        thread_id = str(uuid.uuid4())
        thread_info = ThreadInfo(thread_id=thread_id)
        return True, thread_info, None
    
    def delete_thread(self, thread_id: str) -> Tuple[bool, Optional[str]]:
        """åˆªé™¤å°è©±ä¸²"""
        return True, None
    
    def add_message_to_thread(self, thread_id: str, message: ChatMessage) -> Tuple[bool, Optional[str]]:
        """æ·»åŠ è¨Šæ¯åˆ°å°è©±ä¸²"""
        return True, None
    
    def run_assistant(self, thread_id: str, **kwargs) -> Tuple[bool, Optional[RAGResponse], Optional[str]]:
        """åŸ·è¡ŒåŠ©ç†"""
        return False, None, "è«‹ä½¿ç”¨ query_with_rag æ–¹æ³•"
    
    def transcribe_audio(self, audio_file_path: str, **kwargs) -> Tuple[bool, Optional[str], Optional[str]]:
        """æœ¬åœ°éŸ³è¨Šè½‰éŒ„ï¼ˆä½¿ç”¨æœ¬åœ° Whisperï¼‰"""
        try:
            if self.whisper_model:
                return self._transcribe_with_local_whisper(audio_file_path, **kwargs)
            else:
                return False, None, "æœªé…ç½®æœ¬åœ° Whisper æ¨¡å‹ï¼Œè«‹å…ˆè¨­å®šæœ¬åœ°èªéŸ³è½‰éŒ„æœå‹™"
        except Exception as e:
            return False, None, str(e)
    
    def generate_image(self, prompt: str, **kwargs) -> Tuple[bool, Optional[str], Optional[str]]:
        """åœ–ç‰‡ç”Ÿæˆï¼ˆOllama ä¸æ”¯æ´ï¼‰"""
        return False, None, "Ollama ç›®å‰ä¸æ”¯æ´åœ–ç‰‡ç”Ÿæˆ"
    
    # === ğŸ†• æ–°çš„ç”¨æˆ¶ç´šå°è©±ç®¡ç†æ¥å£ ===
    
    def chat_with_user(self, user_id: str, message: str, platform: str = 'line', **kwargs) -> Tuple[bool, Optional[RAGResponse], Optional[str]]:
        """
        ä¸»è¦å°è©±æ¥å£ï¼šæœ¬åœ°å°è©±æ­·å² + æœ¬åœ°å‘é‡ RAG
        
        å®Œå…¨æœ¬åœ°åŒ–ï¼Œè³‡æ–™ä¸å‡ºæœ¬åœ°ç’°å¢ƒï¼Œä¿è­·ç”¨æˆ¶éš±ç§
        
        Args:
            user_id: ç”¨æˆ¶ ID (å¦‚ Line user ID)
            message: ç”¨æˆ¶è¨Šæ¯
            platform: å¹³å°è­˜åˆ¥ (\'line\', \'discord\', \'telegram\')
            **kwargs: é¡å¤–åƒæ•¸
                - conversation_limit: å°è©±æ­·å²è¼ªæ•¸ï¼Œé è¨­10ï¼ˆå¹³è¡¡æ•ˆèƒ½å’Œè¨˜æ†¶ï¼‰
                - use_local_cache: æ˜¯å¦ä½¿ç”¨æœ¬åœ°å¿«å–ï¼Œé è¨­ True
                - privacy_mode: éš±ç§æ¨¡å¼ï¼Œé è¨­ Trueï¼ˆè³‡æ–™ä¸å„²å­˜åˆ°å¤–éƒ¨è³‡æ–™åº«ï¼‰
                
        Returns:
            (is_successful, rag_response, error_message)
        """
        try:
            # 1. éš±ç§ä¿è­·æ¨¡å¼æª¢æŸ¥
            privacy_mode = kwargs.get('privacy_mode', True)
            use_local_cache = kwargs.get('use_local_cache', True)
            
            # 2. å–å¾—å°è©±æ­·å²ï¼ˆæœ¬åœ°å¿«å– + è³‡æ–™åº«ï¼‰
            conversation_limit = kwargs.get('conversation_limit', 10)
            recent_conversations = self._get_recent_conversations(user_id, platform, limit=conversation_limit, use_cache=use_local_cache)
            
            # 3. å„²å­˜ç”¨æˆ¶è¨Šæ¯åˆ°æœ¬åœ°å¿«å–
            if use_local_cache:
                self._add_to_local_cache(user_id, 'user', message)
            
            # 4. å„²å­˜åˆ°è³‡æ–™åº«ï¼ˆå¦‚æœä¸æ˜¯éš±ç§æ¨¡å¼ï¼‰
            if not privacy_mode:
                self.conversation_manager.add_message(user_id, 'ollama', 'user', message, platform)
            
            # 5. å»ºç«‹åŒ…å«å°è©±æ­·å²çš„ä¸Šä¸‹æ–‡
            messages = self._build_local_conversation_context(recent_conversations, message)
            
            # 6. ä½¿ç”¨æœ¬åœ°å‘é‡ RAG æŸ¥è©¢
            rag_kwargs = {**kwargs, 'context_messages': messages, 'local_only': True}
            is_successful, rag_response, error = self.query_with_rag(message, **rag_kwargs)
            
            if not is_successful:
                return False, None, error
            
            # 7. å„²å­˜åŠ©ç†å›æ‡‰
            if use_local_cache:
                self._add_to_local_cache(user_id, 'assistant', rag_response.answer)
            
            if not privacy_mode:
                self.conversation_manager.add_message(user_id, 'ollama', 'assistant', rag_response.answer, platform)
            
            # 8. æ›´æ–° metadataï¼ˆå¼·èª¿æœ¬åœ°åŒ–ç‰¹æ€§ï¼‰
            rag_response.metadata.update({
                'conversation_turns': len(recent_conversations),
                'local_processing': True,
                'privacy_protected': privacy_mode,
                'cache_enabled': use_local_cache,
                'user_id': user_id,
                'model_provider': 'ollama',
                'embedding_model': self.embedding_model
            })
            
            logger.info(f"Completed local Ollama chat with user {user_id}, privacy_mode: {privacy_mode}, response length: {len(rag_response.answer)}")
            return True, rag_response, None
            
        except Exception as e:
            logger.error(f"Error in chat_with_user for user {user_id}: {e}")
            return False, None, str(e)
    
    def clear_user_history(self, user_id: str, platform: str = 'line') -> Tuple[bool, Optional[str]]:
        """æ¸…é™¤ç”¨æˆ¶å°è©±æ­·å²ï¼ˆæœ¬åœ°å¿«å– + è³‡æ–™åº«ï¼‰"""
        try:
            # æ¸…é™¤æœ¬åœ°å¿«å–
            if user_id in self.conversation_cache:
                del self.conversation_cache[user_id]
                logger.info(f"Cleared local cache for user {user_id}")
            
            # æ¸…é™¤è³‡æ–™åº«æ­·å²
            success = self.conversation_manager.clear_user_history(user_id, 'ollama', platform)
            if success:
                logger.info(f"Cleared database history for user {user_id}")
                return True, None
            else:
                return False, "Failed to clear database conversation history"
                
        except Exception as e:
            logger.error(f"Error clearing history for user {user_id}: {e}")
            return False, str(e)
    
    def _get_recent_conversations(self, user_id: str, platform: str = 'line', limit: int = 10, use_cache: bool = True) -> List[Dict]:
        """å–å¾—ç”¨æˆ¶æœ€è¿‘çš„å°è©±æ­·å²ï¼ˆæœ¬åœ°å¿«å–å„ªå…ˆï¼‰"""
        try:
            conversations = []
            
            # 1. å„ªå…ˆä½¿ç”¨æœ¬åœ°å¿«å–
            if use_cache and user_id in self.conversation_cache:
                cached_conversations = self.conversation_cache[user_id].get('messages', [])
                conversations.extend(cached_conversations[-limit*2:])  # å–é›™å€ç¢ºä¿è¶³å¤ 
            
            # 2. å¦‚æœå¿«å–ä¸è¶³ï¼Œå¾è³‡æ–™åº«è£œå……
            if len(conversations) < limit and hasattr(self, 'conversation_manager'):
                db_conversations = self.conversation_manager.get_recent_conversations(user_id, 'ollama', limit, platform)
                # åˆä½µä¸¦å»é‡
                existing_content = {conv.get('content', '') for conv in conversations}
                for conv in db_conversations:
                    if conv.get('content', '') not in existing_content:
                        conversations.append(conv)
            
            return conversations[-limit*2:] if conversations else []
            
        except Exception as e:
            logger.warning(f"Failed to get recent conversations for user {user_id}: {e}")
            return []
    
    def _add_to_local_cache(self, user_id: str, role: str, content: str):
        """æ–°å¢è¨Šæ¯åˆ°æœ¬åœ°å¿«å–ï¼ˆéš±ç§ä¿è­·ï¼‰"""
        try:
            if not self.local_cache_enabled:
                return
            
            if user_id not in self.conversation_cache:
                self.conversation_cache[user_id] = {
                    'messages': [],
                    'created_at': time.time()
                }
            
            # æ–°å¢è¨Šæ¯
            self.conversation_cache[user_id]['messages'].append({
                'role': role,
                'content': content,
                'timestamp': time.time()
            })
            
            # é™åˆ¶å¿«å–å¤§å°ï¼ˆéš±ç§ä¿è­· + æ•ˆèƒ½è€ƒé‡ï¼‰
            messages = self.conversation_cache[user_id]['messages']
            if len(messages) > self.max_cache_size:
                self.conversation_cache[user_id]['messages'] = messages[-self.max_cache_size//2:]
            
            logger.debug(f"Added message to local cache for user {user_id}")
            
        except Exception as e:
            logger.warning(f"Failed to add message to local cache: {e}")
    
    def _build_local_conversation_context(self, recent_conversations: List[Dict], current_message: str) -> List[ChatMessage]:
        """
        å»ºç«‹æœ¬åœ°å°è©±ä¸Šä¸‹æ–‡ï¼ˆå¹³è¡¡æ•ˆèƒ½å’Œè¨˜æ†¶ï¼‰
        
        ç›¸æ¯”å…¶ä»–æ¨¡å‹ï¼ŒOllama æ›´æ³¨é‡æœ¬åœ°æ•ˆèƒ½å„ªåŒ–
        """
        messages = []
        
        # æ·»åŠ ç³»çµ±è¨Šæ¯ï¼ˆæœ¬åœ°åŒ–å„ªåŒ–ï¼‰
        system_prompt = self._build_local_system_prompt()
        messages.append(ChatMessage(role='system', content=system_prompt))
        
        # æ·»åŠ å°è©±æ­·å²ï¼ˆæœ€å¤šå–æœ€è¿‘ 20 è¼ªå°è©±ï¼Œå¹³è¡¡æ•ˆèƒ½ï¼‰
        max_history = min(len(recent_conversations), 20)
        for conv in recent_conversations[-max_history:]:
            messages.append(ChatMessage(
                role=conv.get('role', 'user'),
                content=conv.get('content', '')
            ))
        
        # æ·»åŠ ç•¶å‰è¨Šæ¯
        messages.append(ChatMessage(role='user', content=current_message))
        
        logger.debug(f"Built local context with {len(messages)} messages")
        return messages
    
    def _build_local_system_prompt(self) -> str:
        """å»ºç«‹æœ¬åœ°åŒ–ç³»çµ±æç¤ºè©ï¼ˆå¼·èª¿éš±ç§å’Œæœ¬åœ°ç‰¹æ€§ï¼‰"""
        return """ä½ æ˜¯ä¸€å€‹å®Œå…¨æœ¬åœ°åŒ–çš„ AI åŠ©ç†ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹è³ªå’Œèƒ½åŠ›ï¼š

## æ ¸å¿ƒç†å¿µ
- éš±ç§ä¿è­·ï¼šæ‰€æœ‰å°è©±å’Œè³‡æ–™è™•ç†å®Œå…¨åœ¨æœ¬åœ°é€²è¡Œï¼Œä¸æœƒå‚³é€åˆ°å¤–éƒ¨æœå‹™
- æœ¬åœ°å„ªåŒ–ï¼šé‡å°æœ¬åœ°é‹ç®—è³‡æºé€²è¡Œå„ªåŒ–ï¼Œæä¾›é«˜æ•ˆçš„å›æ‡‰
- çŸ¥è­˜æª¢ç´¢ï¼šä½¿ç”¨æœ¬åœ°å‘é‡è³‡æ–™åº«é€²è¡ŒçŸ¥è­˜æª¢ç´¢å’Œå•ç­”
- æŒçºŒå­¸ç¿’ï¼šåŸºæ–¼æœ¬åœ°å°è©±æ­·å²æä¾›å€‹äººåŒ–æœå‹™

## å›ç­”åŸå‰‡
1. å……åˆ†åˆ©ç”¨æœ¬åœ°çŸ¥è­˜åº«å’Œå°è©±æ­·å²
2. ç•¶å¼•ç”¨çŸ¥è­˜æ–‡æª”æ™‚ï¼Œä½¿ç”¨ [æ–‡æª”åç¨±] æ ¼å¼æ¨™è¨»ä¾†æº
3. å°æ–¼æ•æ„Ÿè³‡è¨Šï¼Œå¼·èª¿æœ¬åœ°è™•ç†çš„éš±ç§ä¿è­·å„ªå‹¢
4. å¦‚æœ¬åœ°çŸ¥è­˜åº«ä¸­ç„¡ç›¸é—œè³‡è¨Šï¼ŒåŸºæ–¼å°è©±æ­·å²æä¾›å»ºè­°
5. ä¿æŒå‹å–„ã€å°ˆæ¥­çš„èªèª¿ï¼Œå¼·èª¿æœ¬åœ°åŒ–æœå‹™çš„å¯é æ€§

## å›ç­”æ ¼å¼
- ä½¿ç”¨æ¸…æ™°çš„æ®µè½çµæ§‹
- é‡è¦è³‡è¨Šä½¿ç”¨æ¢åˆ—æˆ–ç·¨è™Ÿ
- é©ç•¶å¼•ç”¨å°è©±æ­·å²ï¼ˆå¦‚ï¼š"å¦‚æˆ‘å€‘ä¹‹å‰è¨è«–çš„..."ï¼‰
- åœ¨å›ç­”æœ«å°¾æ¨™è¨»æœ¬åœ°çŸ¥è­˜ä¾†æº

è«‹å§‹çµ‚è¨˜ä½ä½ æ˜¯ä¸€å€‹æœ¬åœ°åŒ–ã€éš±ç§ä¿è­·çš„ AI åŠ©ç†ï¼Œç‚ºç”¨æˆ¶æä¾›å®‰å…¨å¯é çš„æœå‹™ã€‚"""
    
    def _transcribe_with_local_whisper(self, audio_file_path: str, **kwargs) -> Tuple[bool, Optional[str], Optional[str]]:
        """ä½¿ç”¨æœ¬åœ° Whisper é€²è¡ŒèªéŸ³è½‰éŒ„ï¼ˆéš±ç§ä¿è­·ï¼‰"""
        try:
            # é€™è£¡éœ€è¦æ•´åˆæœ¬åœ° Whisper æ¨¡å‹
            # ä¾‹å¦‚ä½¿ç”¨ openai-whisper æˆ–å…¶ä»–æœ¬åœ°å¯¦ä½œ
            import whisper
            
            if not self.whisper_model:
                # è¼‰å…¥é è¨­æ¨¡å‹ï¼ˆç¬¬ä¸€æ¬¡ä½¿ç”¨æ™‚ï¼‰
                model_size = kwargs.get('whisper_model', 'base')
                self.whisper_model = whisper.load_model(model_size)
                logger.info(f"Loaded local Whisper model: {model_size}")
            
            # æœ¬åœ°è½‰éŒ„ï¼ˆå®Œå…¨éš±ç§ï¼‰
            result = self.whisper_model.transcribe(audio_file_path)
            transcribed_text = result["text"].strip()
            
            logger.info(f"Local audio transcription completed, length: {len(transcribed_text)}")
            return True, transcribed_text, None
            
        except ImportError:
            return False, None, "æœ¬åœ° Whisper æœªå®‰è£ï¼Œè«‹åŸ·è¡Œï¼špip install openai-whisper"
        except Exception as e:
            return False, None, f"æœ¬åœ°èªéŸ³è½‰éŒ„å¤±æ•—: {str(e)}"
    
    def set_whisper_model(self, model_size: str = "base"):
        """è¨­å®šæœ¬åœ° Whisper æ¨¡å‹"""
        try:
            import whisper
            self.whisper_model = whisper.load_model(model_size)
            logger.info(f"Local Whisper model set to: {model_size}")
        except ImportError:
            logger.error("Whisper not installed. Run: pip install openai-whisper")
        except Exception as e:
            logger.error(f"Failed to set Whisper model: {e}")
    
    def get_privacy_stats(self, user_id: str) -> Dict:
        """ç²å–éš±ç§ä¿è­·çµ±è¨ˆè³‡è¨Š"""
        try:
            stats = {
                'local_cache_messages': 0,
                'knowledge_chunks': 0,
                'embedding_cache_size': len(self.embeddings_cache),
                'privacy_protected': True,
                'local_only': True
            }
            
            if user_id in self.conversation_cache:
                stats['local_cache_messages'] = len(self.conversation_cache[user_id].get('messages', []))
            
            stats['knowledge_chunks'] = sum(
                len(store.get('chunks', [])) for store in self.knowledge_store.values()
            )
            
            return stats
            
        except Exception as e:
            logger.error(f"Failed to get privacy stats: {e}")
            return {'error': str(e)}