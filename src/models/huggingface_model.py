"""
Hugging Face API æ¨¡å‹æ•´åˆ
æ”¯æ´ Inference API, Serverless Inference å’Œå¤šç¨®é–‹æºæ¨¡å‹
"""
import requests
import json
import time
import uuid
import base64
from typing import List, Dict, Tuple, Optional, Any
from ..core.logger import get_logger
from .base import (
    FullLLMInterface, 
    ModelProvider, 
    ChatMessage, 
    ChatResponse, 
    ThreadInfo, 
    FileInfo,
    RAGResponse
)
from ..utils.retry import retry_on_rate_limit
from ..services.conversation import get_conversation_manager

logger = get_logger(__name__)

class HuggingFaceModel(FullLLMInterface):
    """
    Hugging Face API æ¨¡å‹å¯¦ä½œ
    
    ğŸ“‹ æ¶æ§‹è·è²¬åˆ†å·¥ï¼š
    âœ… RESPONSIBILITIES (æ¨¡å‹å±¤è·è²¬):
      - å¯¦ä½œçµ±ä¸€çš„ FullLLMInterface æ¥å£
      - æä¾› chat_with_user() æ–‡å­—å°è©±åŠŸèƒ½
      - æä¾› transcribe_audio() éŸ³è¨Šè½‰éŒ„åŠŸèƒ½
      - ç®¡ç†å°è©±æ­·å²å’Œä¸Šä¸‹æ–‡
      - è™•ç† Hugging Face API é™æµå’Œé‡è©¦é‚è¼¯

    ğŸ¯ æ¨¡å‹ç‰¹è‰²ï¼š
    - æ”¯æ´å¤šç¨®é–‹æºæ¨¡å‹ (Mistral, Llama, CodeLlamaç­‰)
    - Inference API å½ˆæ€§ä½¿ç”¨ä¸åŒæ¨¡å‹
    - èªéŸ³è½‰æ–‡å­— (Whisper/Wav2Vec2 æ¨¡å‹)
    - åœ–ç‰‡ç”Ÿæˆ (Stable Diffusionç³»åˆ—)
    - å°è©±æ­·å²ç®¡ç† (æœ¬åœ°è³‡æ–™åº«)

    âš ï¸ åŠŸèƒ½é™åˆ¶ï¼š
    - æ¨¡å‹å¯ç”¨æ€§: ä¾è³´ Hugging Face æœå‹™å’Œæ¨¡å‹ç‹€æ…‹
    - éŸ³è¨Šè½‰éŒ„: ä¾è³´ç‰¹å®šASRæ¨¡å‹æ˜¯å¦å¯ç”¨
    - é€£ç·šç‹€æ…‹: æœå‹™å¯èƒ½å› è² è¼‰éé«˜è€Œä¸å¯ç”¨
    """
    
    def __init__(self, 
                 api_key: str,
                 model_name: str = "mistralai/Mistral-7B-Instruct-v0.1",
                 api_type: str = "inference_api",
                 base_url: str = "https://api-inference.huggingface.co",
                 enable_mcp: bool = False,
                 **kwargs):
        """
        åˆå§‹åŒ– Hugging Face æ¨¡å‹
        
        Args:
            api_key: Hugging Face API é‡‘é‘°
            model_name: ä¸»è¦èŠå¤©æ¨¡å‹åç¨±
            api_type: API é¡å‹ (inference_api, serverless, dedicated)
            base_url: API åŸºç¤ URL
            **kwargs: é¡å¤–é…ç½®åƒæ•¸
                - fallback_models: å‚™ç”¨æ¨¡å‹åˆ—è¡¨
                - embedding_model: åµŒå…¥æ¨¡å‹
                - speech_model: èªéŸ³æ¨¡å‹
                - image_model: åœ–ç‰‡ç”Ÿæˆæ¨¡å‹
                - temperature: ç”Ÿæˆæº«åº¦
                - max_tokens: æœ€å¤§tokenæ•¸
                - timeout: è«‹æ±‚è¶…æ™‚æ™‚é–“
        """
        self.api_key = api_key
        self.model_name = model_name
        self.api_type = api_type
        self.base_url = base_url
        
        # åŠŸèƒ½å°ˆç”¨æ¨¡å‹é…ç½®ï¼ˆæ”¯æ´å¾é…ç½®è¦†è“‹ï¼‰
        self.embedding_model = kwargs.get('embedding_model', "sentence-transformers/all-MiniLM-L6-v2")
        self.speech_model = kwargs.get('speech_model', "openai/whisper-large-v3")
        self.image_model = kwargs.get('image_model', "stabilityai/stable-diffusion-xl-base-1.0")
        
        # å‚™ç”¨æ¨¡å‹åˆ—è¡¨ï¼ˆæ”¯æ´å¾é…ç½®è¦†è“‹ï¼‰
        self.fallback_models = kwargs.get('fallback_models', [
            "microsoft/DialoGPT-medium",
            "HuggingFaceH4/zephyr-7b-beta",
            "mistralai/Mistral-7B-Instruct-v0.2"
        ])
        
        # ç”Ÿæˆåƒæ•¸ï¼ˆæ”¯æ´å¾é…ç½®è¦†è“‹ï¼‰
        self.temperature = kwargs.get('temperature', 0.7)
        self.max_tokens = kwargs.get('max_tokens', 512)
        
        # HTTP è«‹æ±‚è¨­å®š
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        self.timeout = kwargs.get('timeout', 60)  # æ”¯æ´å¾é…ç½®è¦†è“‹
        
        # å°è©±ç®¡ç†å’Œæœ¬åœ°å­˜å„²
        self.conversation_manager = get_conversation_manager()
        self.local_threads = {}  # æœ¬åœ°ç·šç¨‹ç®¡ç†
        self.knowledge_store = {}  # æœ¬åœ°çŸ¥è­˜åº«
        self.embeddings_cache = {}  # åµŒå…¥å‘é‡ç·©å­˜

        # MCP æ”¯æ´
        if enable_mcp:
            self.enable_mcp = True
        else:
            try:
                from ..core.config import get_value
                feature_enabled = get_value('features.enable_mcp', False)
                mcp_enabled = get_value('mcp.enabled', False)
                self.enable_mcp = feature_enabled and mcp_enabled
            except Exception as e:
                logger.warning(f"Error reading MCP config: {e}")
                self.enable_mcp = False
        
        self.mcp_service = None
        if self.enable_mcp:
            self._init_mcp_service()
        
        logger.info(f"HuggingFace model initialized: {self.model_name}")

    def get_provider(self) -> ModelProvider:
        """è¿”å› Hugging Face æä¾›å•†æ¨™è­˜""" 
        return ModelProvider.HUGGINGFACE

    def _init_mcp_service(self) -> None:
        """åˆå§‹åŒ– MCP æœå‹™"""
        try:
            from ..services.mcp_service import get_mcp_service
            
            mcp_service = get_mcp_service()
            if mcp_service.is_enabled:
                self.mcp_service = mcp_service
                logger.info("HuggingFace Model: MCP service initialized successfully")
            else:
                logger.warning("HuggingFace Model: MCP service is not enabled")
                self.enable_mcp = False
        except Exception as e:
            logger.warning(f"HuggingFace Model: Failed to initialize MCP service: {e}")
            self.enable_mcp = False
            self.mcp_service = None

    def check_connection(self) -> Tuple[bool, Optional[str]]:
        """
        æª¢æŸ¥ Hugging Face API é€£æ¥ç‹€æ…‹
        
        Returns:
            Tuple[bool, Optional[str]]: (é€£æ¥æˆåŠŸ, éŒ¯èª¤è¨Šæ¯)
        """
        try:
            # ä½¿ç”¨ç°¡å–®çš„æ–‡æœ¬ç”Ÿæˆæ¸¬è©¦é€£æ¥
            test_message = ChatMessage(role="user", content="Hello")
            is_successful, response, error = self.chat_completion([test_message], max_tokens=10)
            
            if is_successful:
                logger.info("HuggingFace API connection verified")
                return True, None
            else:
                logger.error(f"HuggingFace API connection failed: {error}")
                return False, error
                
        except Exception as e:
            error_msg = f"HuggingFace connection check failed: {str(e)}"
            logger.error(error_msg)
            return False, error_msg

    @retry_on_rate_limit(max_retries=3, base_delay=2.0)
    def chat_completion(self, messages: List[ChatMessage], **kwargs) -> Tuple[bool, Optional[ChatResponse], Optional[str]]:
        """
        Hugging Face èŠå¤©å®ŒæˆåŠŸèƒ½
        
        Args:
            messages: å°è©±è¨Šæ¯åˆ—è¡¨
            **kwargs: ç”Ÿæˆåƒæ•¸
                - max_tokens: æœ€å¤§ç”Ÿæˆé•·åº¦ (default: 512)
                - temperature: å‰µé€ æ€§æ§åˆ¶ (default: 0.7)
                - do_sample: æ˜¯å¦ä½¿ç”¨æ¡æ¨£ (default: True)
                - top_p: æ ¸æ¡æ¨£åƒæ•¸ (default: 0.9)
        
        Returns:
            Tuple[bool, Optional[ChatResponse], Optional[str]]
        """
        try:
            # æå–åƒæ•¸
            max_tokens = kwargs.get('max_tokens', 512)
            temperature = kwargs.get('temperature', 0.7)
            do_sample = kwargs.get('do_sample', True)
            top_p = kwargs.get('top_p', 0.9)
            
            # æ§‹å»º Hugging Face æ ¼å¼çš„è¼¸å…¥
            prompt = self._build_chat_prompt(messages)
            
            # æº–å‚™ API è«‹æ±‚
            payload = {
                "inputs": prompt,
                "parameters": {
                    "max_new_tokens": max_tokens,
                    "temperature": temperature,
                    "do_sample": do_sample,
                    "top_p": top_p,
                    "return_full_text": False  # åªè¿”å›ç”Ÿæˆçš„éƒ¨åˆ†
                },
                "options": {
                    "wait_for_model": True,  # ç­‰å¾…æ¨¡å‹è¼‰å…¥
                    "use_cache": False  # é¿å…ç·©å­˜å•é¡Œ
                }
            }
            
            # ç™¼é€è«‹æ±‚
            response = self._make_request(self.model_name, payload)
            
            if not response:
                return False, None, "Failed to get response from Hugging Face API"
            
            # è§£æå›æ‡‰
            if isinstance(response, list) and len(response) > 0:
                generated_text = response[0].get('generated_text', '').strip()
            elif isinstance(response, dict):
                generated_text = response.get('generated_text', '').strip()
            else:
                generated_text = str(response).strip()
            
            if not generated_text:
                return False, None, "Empty response from model"
            
            # å‰µå»º ChatResponse
            chat_response = ChatResponse(
                content=generated_text,
                finish_reason="stop",
                metadata={
                    "model": self.model_name,
                    "provider": "huggingface",
                    "api_type": self.api_type,
                    "input_tokens": len(prompt.split()),
                    "output_tokens": len(generated_text.split())
                }
            )
            
            logger.debug(f"HuggingFace chat completion successful: {len(generated_text)} chars")
            return True, chat_response, None
            
        except Exception as e:
            error_msg = f"HuggingFace chat completion failed: {str(e)}"
            logger.error(error_msg)
            
            # å˜—è©¦å‚™ç”¨æ¨¡å‹
            if hasattr(self, '_retry_count') and self._retry_count < len(self.fallback_models):
                fallback_model = self.fallback_models[self._retry_count]
                logger.info(f"Trying fallback model: {fallback_model}")
                
                original_model = self.model_name
                self.model_name = fallback_model
                self._retry_count += 1
                
                try:
                    result = self.chat_completion(messages, **kwargs)
                    self.model_name = original_model  # æ¢å¾©åŸå§‹æ¨¡å‹
                    return result
                except:
                    self.model_name = original_model
            
            return False, None, error_msg

    def _build_chat_prompt(self, messages: List[ChatMessage]) -> str:
        """
        å°‡ ChatMessage åˆ—è¡¨è½‰æ›ç‚º Hugging Face æ¨¡å‹æ ¼å¼çš„æç¤ºè©
        
        ä¸åŒæ¨¡å‹å¯èƒ½éœ€è¦ä¸åŒçš„æ ¼å¼ï¼Œé€™è£¡ä½¿ç”¨é€šç”¨çš„èŠå¤©æ ¼å¼
        """
        if "mistral" in self.model_name.lower():
            # Mistral æ ¼å¼: <s>[INST] prompt [/INST]
            user_messages = [msg for msg in messages if msg.role == "user"]
            assistant_messages = [msg for msg in messages if msg.role == "assistant"]
            system_messages = [msg for msg in messages if msg.role == "system"]
            
            prompt_parts = []
            
            # æ·»åŠ ç³»çµ±è¨Šæ¯
            if system_messages:
                system_content = " ".join([msg.content for msg in system_messages])
                prompt_parts.append(f"<s>[INST] {system_content}")
            
            # æ§‹å»ºå°è©±
            for i, msg in enumerate(messages):
                if msg.role == "user":
                    if i == 0 and not system_messages:
                        prompt_parts.append(f"<s>[INST] {msg.content} [/INST]")
                    else:
                        prompt_parts.append(f"[INST] {msg.content} [/INST]")
                elif msg.role == "assistant":
                    prompt_parts.append(f" {msg.content}</s>")
            
            return " ".join(prompt_parts)
            
        elif "zephyr" in self.model_name.lower():
            # Zephyr æ ¼å¼: <|system|>, <|user|>, <|assistant|>
            prompt_parts = []
            for msg in messages:
                if msg.role == "system":
                    prompt_parts.append(f"<|system|>\n{msg.content}</s>")
                elif msg.role == "user":
                    prompt_parts.append(f"<|user|>\n{msg.content}</s>")
                elif msg.role == "assistant":
                    prompt_parts.append(f"<|assistant|>\n{msg.content}</s>")
            
            prompt_parts.append("<|assistant|>\n")  # æç¤ºæ¨¡å‹ç”Ÿæˆ
            return "\n".join(prompt_parts)
            
        else:
            # é€šç”¨æ ¼å¼: ç°¡å–®çš„è§’è‰²æ¨™è¨˜
            prompt_parts = []
            for msg in messages:
                role_prefix = {
                    "system": "System:",
                    "user": "Human:",
                    "assistant": "Assistant:"
                }.get(msg.role, f"{msg.role.title()}:")
                
                prompt_parts.append(f"{role_prefix} {msg.content}")
            
            prompt_parts.append("Assistant:")  # æç¤ºæ¨¡å‹ç”Ÿæˆ
            return "\n\n".join(prompt_parts)

    def _make_request(self, model_name: str, payload: Dict[str, Any], timeout: int = None) -> Optional[Any]:
        """
        å‘ Hugging Face API ç™¼é€è«‹æ±‚
        
        Args:
            model_name: æ¨¡å‹åç¨±
            payload: è«‹æ±‚è¼‰è·
            timeout: è«‹æ±‚è¶…æ™‚æ™‚é–“
            
        Returns:
            API å›æ‡‰æ•¸æ“šæˆ– None
        """
        try:
            url = f"{self.base_url}/models/{model_name}"
            
            response = requests.post(
                url,
                headers=self.headers,
                json=payload,
                timeout=timeout or self.timeout
            )
            
            if response.status_code == 200:
                return response.json()
            elif response.status_code == 503:
                # æ¨¡å‹æ­£åœ¨è¼‰å…¥
                logger.warning(f"Model {model_name} is loading, waiting...")
                time.sleep(10)  # ç­‰å¾…æ¨¡å‹è¼‰å…¥
                return self._make_request(model_name, payload, timeout)
            else:
                logger.error(f"HuggingFace API error {response.status_code}: {response.text}")
                return None
                
        except requests.exceptions.Timeout:
            logger.error(f"Request timeout for model {model_name}")
            return None
        except Exception as e:
            logger.error(f"Request failed for model {model_name}: {str(e)}")
            return None

    # ==================== UserConversationInterface ====================
    
    def chat_with_user(self, user_id: str, message: str, platform: str = 'line', **kwargs) -> Tuple[bool, Optional[RAGResponse], Optional[str]]:
        """
        èˆ‡ç”¨æˆ¶é€²è¡Œå°è©±ï¼Œæ•´åˆæ­·å²è¨˜éŒ„å’Œ RAG åŠŸèƒ½
        
        Args:
            user_id: ç”¨æˆ¶ ID
            message: ç”¨æˆ¶è¨Šæ¯
            platform: å¹³å°æ¨™è­˜
            **kwargs: é¡å¤–åƒæ•¸
                - conversation_limit: å°è©±æ­·å²é™åˆ¶ (default: 10)
                - use_rag: æ˜¯å¦ä½¿ç”¨ RAG (default: True)
                - temperature: ç”Ÿæˆæº«åº¦ (default: 0.7)
        
        Returns:
            Tuple[bool, Optional[RAGResponse], Optional[str]]
        """
        try:
            conversation_limit = kwargs.get('conversation_limit', 10)
            use_rag = kwargs.get('use_rag', True)
            
            # 1. å–å¾—å°è©±æ­·å²
            conversation_history = self._get_recent_conversations(user_id, platform, conversation_limit)
            
            # 2. æª¢æŸ¥æ˜¯å¦ç‚ºé‡ç½®å‘½ä»¤
            if message.strip().lower() in ['/reset', 'é‡ç½®', 'æ¸…é™¤æ­·å²']:
                success, error = self.clear_user_history(user_id, platform)
                if success:
                    reset_response = RAGResponse(
                        answer="å·²æ¸…é™¤æ‚¨çš„å°è©±æ­·å²ï¼Œè®“æˆ‘å€‘é‡æ–°é–‹å§‹å§ï¼",
                        sources=[],
                        metadata={
                            "user_id": user_id,
                            "platform": platform,
                            "model_provider": "huggingface",
                            "action": "reset_history"
                        }
                    )
                    return True, reset_response, None
                else:
                    return False, None, f"æ¸…é™¤æ­·å²å¤±æ•—: {error}"
            
            # 3. åŸ·è¡Œ RAG æŸ¥è©¢ï¼ˆå¦‚æœå•Ÿç”¨ä¸”æœ‰çŸ¥è­˜åº«ï¼‰
            if use_rag and self.knowledge_store:
                is_successful, rag_response, error = self.query_with_rag(
                    message, 
                    context_messages=conversation_history,
                    **kwargs
                )
                
                if is_successful and rag_response:
                    # æ›´æ–°å…ƒæ•¸æ“š
                    rag_response.metadata.update({
                        "user_id": user_id,
                        "platform": platform,
                        "model_provider": "huggingface",
                        "conversation_enabled": True
                    })
                    
                    # ä¿å­˜å°è©±æ­·å²
                    self._save_conversation(user_id, platform, message, rag_response.answer)
                    
                    return True, rag_response, None
            
            # 4. æ™®é€šèŠå¤©å°è©±ï¼ˆç„¡ RAGï¼‰
            # æ§‹å»ºå®Œæ•´çš„å°è©±ä¸Šä¸‹æ–‡
            context_messages = self._build_conversation_context(conversation_history, message)
            
            if self.enable_mcp and self.mcp_service:
                # MCP éœ€è¦ asyncï¼Œä½†ç›®å‰åœ¨ sync æ¨¡å¼ä¸‹ç¦ç”¨
                logger.warning("MCP is disabled in sync mode. Falling back to regular chat.")
                is_successful, chat_response, error = self.chat_completion(context_messages, **kwargs)
                if is_successful:
                    rag_response = RAGResponse(
                        answer=chat_response.content,
                        sources=[],
                        metadata=chat_response.metadata
                    )
            else:
                is_successful, chat_response, error = self.chat_completion(context_messages, **kwargs)
                if is_successful:
                    rag_response = RAGResponse(
                        answer=chat_response.content,
                        sources=[],
                        metadata=chat_response.metadata
                    )

            if not is_successful:
                return False, None, error
            
            # è½‰æ›ç‚º RAGResponse æ ¼å¼
            rag_response.metadata.update({
                "user_id": user_id,
                "platform": platform,
                "model_provider": "huggingface",
                "model_name": self.model_name,
                "rag_enabled": False,
                "conversation_enabled": True
            })
            
            # ä¿å­˜å°è©±æ­·å²
            self._save_conversation(user_id, platform, message, rag_response.answer)
            
            logger.info(f"HuggingFace conversation completed for user {user_id}")
            return True, rag_response, None
            
        except Exception as e:
            error_msg = f"HuggingFace chat_with_user failed: {str(e)}"
            logger.error(error_msg)
            return False, None, error_msg

    async def chat_completion_with_mcp(self, messages: List[ChatMessage], **kwargs) -> Tuple[bool, Optional[RAGResponse], Optional[str]]:
        """
        æ”¯æ´ MCP function calling çš„å°è©±å®Œæˆ (åŸºæ–¼æç¤ºå·¥ç¨‹)
        """
        if not self.enable_mcp or not self.mcp_service:
            is_success, response, error = self.chat_completion(messages, **kwargs)
            if not is_success:
                return False, None, error
            return True, RAGResponse(answer=response.content, sources=[], metadata=response.metadata), None

        try:
            # 1. å»ºç«‹åŒ…å«å·¥å…·å®šç¾©çš„ç³»çµ±æç¤º
            system_prompt = self._build_mcp_system_prompt()
            
            # æ›¿æ›æˆ–æ’å…¥ç³»çµ±æç¤º
            final_messages = [msg for msg in messages if msg.role != 'system']
            final_messages.insert(0, ChatMessage(role="system", content=system_prompt))

            # 2. ç¬¬ä¸€æ¬¡å‘¼å«æ¨¡å‹ï¼Œåˆ¤æ–·æ˜¯å¦éœ€è¦å·¥å…·
            is_successful, response, error = self.chat_completion(final_messages, **kwargs)
            if not is_successful:
                return False, None, error

            # 3. è§£æå›æ‡‰ï¼Œæª¢æŸ¥æ˜¯å¦æœ‰å·¥å…·èª¿ç”¨
            tool_call_request = self._parse_for_tool_call(response.content)

            if not tool_call_request:
                # æ²’æœ‰å·¥å…·èª¿ç”¨ï¼Œç›´æ¥è¿”å›çµæœ
                return True, RAGResponse(answer=response.content, sources=[], metadata=response.metadata), None

            # 4. åŸ·è¡Œå·¥å…·èª¿ç”¨
            tool_name = tool_call_request['tool_name']
            arguments = tool_call_request['arguments']
            
            logger.info(f"ğŸ”§ HuggingFace Model: Executing tool '{tool_name}' with args: {arguments}")
            tool_result = self.mcp_service.handle_function_call_sync(tool_name, arguments)
            
            # 5. å°‡å·¥å…·çµæœåŠ åˆ°å°è©±æ­·å²ä¸­
            final_messages.append(ChatMessage(role="assistant", content=response.content)) # åŠ å…¥æ¨¡å‹çš„å·¥å…·è«‹æ±‚
            final_messages.append(ChatMessage(
                role="function",
                content=json.dumps(tool_result, ensure_ascii=False),
                metadata={"function_name": tool_name}
            ))

            # 6. å†æ¬¡å‘¼å«æ¨¡å‹ï¼Œç”Ÿæˆæœ€çµ‚å›è¦†
            logger.info("ğŸ”„ HuggingFace Model: Calling model again with tool result.")
            is_successful, final_response, error = self.chat_completion(final_messages, **kwargs)
            if not is_successful:
                return False, None, error

            # 7. çµ„åˆæœ€çµ‚çš„ RAGResponse
            sources = tool_result.get('metadata', {}).get('sources', [])
            final_rag_response = RAGResponse(
                answer=final_response.content,
                sources=sources,
                metadata={
                    **final_response.metadata,
                    'mcp_enabled': True,
                    'tool_calls': [tool_call_request],
                    'tool_results': [tool_result]
                }
            )
            return True, final_rag_response, None

        except Exception as e:
            logger.error(f"Error in chat_completion_with_mcp for HuggingFace: {e}")
            return False, None, str(e)

    def _parse_for_tool_call(self, response_text: str) -> Optional[Dict[str, Any]]:
        """å¾æ¨¡å‹å›æ‡‰ä¸­è§£æå·¥å…·èª¿ç”¨è«‹æ±‚"""
        try:
            # ç§»é™¤ç¨‹å¼ç¢¼å€å¡Šæ¨™è¨˜
            response_text = response_text.strip()
            if response_text.startswith("```json"):
                response_text = response_text[7:]
            if response_text.endswith("```"):
                response_text = response_text[:-3]
            
            data = json.loads(response_text.strip())
            if isinstance(data, dict) and 'tool_name' in data and 'arguments' in data:
                logger.info(f"âœ… Parsed tool call request: {data['tool_name']}")
                return data
            return None
        except (json.JSONDecodeError, TypeError):
            return None

    def _build_mcp_system_prompt(self) -> str:
        """å»ºç«‹åŒ…å« MCP å·¥å…·å®šç¾©å’ŒæŒ‡ä»¤çš„ç³»çµ±æç¤º"""
        base_prompt = self._build_system_prompt()
        
        if not self.mcp_service:
            return base_prompt

        try:
            tools_description = []
            for func_str in self.mcp_service.get_function_schemas_for_anthropic().split('\n'):
                tools_description.append(f"- {func_str}")

            if not tools_description:
                return base_prompt

            mcp_prompt = f"""{base_prompt}

## å¤–éƒ¨å·¥å…·èª¿ç”¨ (MCP)

é™¤äº†ä½ çš„å…§å»ºèƒ½åŠ›ï¼Œä½ é‚„å¯ä»¥èª¿ç”¨ä»¥ä¸‹å¤–éƒ¨å·¥å…·ä¾†ç²å–å³æ™‚è³‡è¨Šæˆ–åŸ·è¡Œç‰¹å®šä»»å‹™ã€‚

### å¯ç”¨å·¥å…·åˆ—è¡¨:
{chr(10).join(tools_description)}

### èª¿ç”¨æŒ‡ä»¤:
ç•¶ä½ åˆ¤æ–·éœ€è¦ä½¿ç”¨å·¥å…·æ™‚ï¼Œä½ çš„å›è¦†**å¿…é ˆä¸”åªèƒ½**æ˜¯ä¸€å€‹ JSON ç‰©ä»¶ï¼Œæ ¼å¼å¦‚ä¸‹ï¼Œä¸åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—æˆ–è§£é‡‹:
```json
{{
  "tool_name": "å·¥å…·åç¨±",
  "arguments": {{
    "åƒæ•¸1": "å€¼1",
    "åƒæ•¸2": "å€¼2"
  }}
}}
```
ç³»çµ±æœƒåŸ·è¡Œæ­¤å·¥å…·ä¸¦å°‡çµæœå›å‚³çµ¦ä½ ï¼Œç„¶å¾Œä½ å†æ ¹æ“šçµæœç”Ÿæˆæœ€çµ‚çš„è‡ªç„¶èªè¨€å›è¦†."""
            return mcp_prompt
        except Exception as e:
            logger.error(f"Failed to build MCP system prompt for HuggingFace: {e}")
            return base_prompt

    def clear_user_history(self, user_id: str, platform: str = 'line') -> Tuple[bool, Optional[str]]:
        """
        æ¸…é™¤ç”¨æˆ¶å°è©±æ­·å²
        
        Args:
            user_id: ç”¨æˆ¶ ID
            platform: å¹³å°æ¨™è­˜
            
        Returns:
            Tuple[bool, Optional[str]]: (æˆåŠŸç‹€æ…‹, éŒ¯èª¤è¨Šæ¯)
        """
        try:
            # æ¸…é™¤æ•¸æ“šåº«ä¸­çš„å°è©±æ­·å²
            if self.conversation_manager:
                success = self.conversation_manager.clear_user_history(user_id, "huggingface", platform)
                if not success:
                    logger.warning(f"Failed to clear database history for user {user_id}")
            
            # æ¸…é™¤æœ¬åœ°ç·šç¨‹ç·©å­˜
            thread_key = f"{user_id}:{platform}"
            if thread_key in self.local_threads:
                del self.local_threads[thread_key]
            
            logger.info(f"Cleared conversation history for user {user_id} on platform {platform}")
            return True, None
            
        except Exception as e:
            error_msg = f"Failed to clear user history: {str(e)}"
            logger.error(error_msg)
            return False, error_msg

    def _get_recent_conversations(self, user_id: str, platform: str, limit: int = 10) -> List[Dict[str, Any]]:
        """å–å¾—ç”¨æˆ¶æœ€è¿‘çš„å°è©±æ­·å²"""
        try:
            if self.conversation_manager:
                conversations = self.conversation_manager.get_recent_conversations(
                    user_id, "huggingface", platform, limit
                )
                return conversations or []
            return []
        except Exception as e:
            logger.error(f"Failed to get conversation history: {str(e)}")
            return []

    def _save_conversation(self, user_id: str, platform: str, user_message: str, assistant_response: str):
        """ä¿å­˜å°è©±åˆ°æ­·å²è¨˜éŒ„"""
        try:
            if self.conversation_manager:
                # ä¿å­˜ç”¨æˆ¶è¨Šæ¯
                self.conversation_manager.add_message(
                    user_id, "huggingface", "user", user_message, platform
                )
                # ä¿å­˜åŠ©ç†å›æ‡‰
                self.conversation_manager.add_message(
                    user_id, "huggingface", "assistant", assistant_response, platform
                )
        except Exception as e:
            logger.error(f"Failed to save conversation: {str(e)}")

    def _build_conversation_context(self, conversation_history: List[Dict[str, Any]], current_message: str) -> List[ChatMessage]:
        """æ§‹å»ºå°è©±ä¸Šä¸‹æ–‡"""
        messages = []
        
        # æ·»åŠ ç³»çµ±æç¤º
        system_prompt = self._build_system_prompt()
        messages.append(ChatMessage(role="system", content=system_prompt))
        
        # æ·»åŠ æ­·å²å°è©±ï¼ˆé™åˆ¶æ•¸é‡é¿å…è¶…é token é™åˆ¶ï¼‰
        for conv in conversation_history[-20:]:  # æœ€å¤š 20 è¼ªå°è©±
            role = conv.get('role', 'user')
            content = conv.get('content', '')
            if content.strip():
                messages.append(ChatMessage(role=role, content=content))
        
        # æ·»åŠ ç•¶å‰ç”¨æˆ¶è¨Šæ¯
        messages.append(ChatMessage(role="user", content=current_message))
        
        return messages

    def _build_system_prompt(self) -> str:
        """
        æ§‹å»ºç³»çµ±æç¤ºè©
        """
        return """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ AI åŠ©ç†ï¼ŒåŸºæ–¼ Hugging Face é–‹æºæ¨¡å‹æŠ€è¡“ã€‚è«‹éµå¾ªä»¥ä¸‹æº–å‰‡ï¼š

1. æä¾›æº–ç¢ºã€æœ‰ç”¨çš„å›æ‡‰
2. ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼ˆé™¤éç”¨æˆ¶æŒ‡å®šå…¶ä»–èªè¨€ï¼‰
3. ä¿æŒå‹å–„å’Œå°ˆæ¥­çš„èªèª¿
4. å¦‚æœä¸ç¢ºå®šç­”æ¡ˆï¼Œè«‹èª å¯¦èªªæ˜
5. å¯ä»¥åƒè€ƒæä¾›çš„æ–‡æª”ä¾†æºçµ¦å‡ºæ›´æº–ç¢ºçš„å›ç­”

è«‹æ ¹æ“šç”¨æˆ¶çš„å•é¡Œæä¾›æœ€ä½³å›æ‡‰ã€‚"""

    # ==================== RAGInterface ====================
    
    def upload_knowledge_file(self, file_path: str, **kwargs) -> Tuple[bool, Optional[FileInfo], Optional[str]]:
        """
        ä¸Šå‚³æ–‡ä»¶åˆ°æœ¬åœ°çŸ¥è­˜åº«
        
        Hugging Face æ²’æœ‰åŸç”Ÿæ–‡ä»¶å­˜å„²æœå‹™ï¼Œæ‰€ä»¥æˆ‘å€‘ä½¿ç”¨æœ¬åœ°å¯¦ç¾
        """
        try:
            import os
            from pathlib import Path
            
            if not os.path.exists(file_path):
                return False, None, f"File not found: {file_path}"
            
            file_path_obj = Path(file_path)
            filename = file_path_obj.name
            file_size = file_path_obj.stat().st_size
            
            # è®€å–æ–‡ä»¶å…§å®¹
            content = self._read_file_content(file_path)
            if not content:
                return False, None, "Failed to read file content"
            
            # ç”Ÿæˆæ–‡ä»¶ ID
            file_id = f"hf_{uuid.uuid4().hex[:12]}"
            
            # å°‡æ–‡ä»¶åˆ†å¡Š
            chunks = self._chunk_text(content)
            
            # ç‚ºæ¯å€‹å¡Šç”ŸæˆåµŒå…¥å‘é‡
            embedded_chunks = []
            for chunk in chunks:
                embedding = self._get_embedding(chunk['text'])
                if embedding:
                    chunk['embedding'] = embedding
                    embedded_chunks.append(chunk)
            
            # å­˜å„²åˆ°æœ¬åœ°çŸ¥è­˜åº«
            self.knowledge_store[file_id] = {
                'filename': filename,
                'content': content,
                'chunks': embedded_chunks,
                'metadata': {
                    'size': file_size,
                    'chunks_count': len(embedded_chunks),
                    'upload_time': time.time()
                }
            }
            
            # å‰µå»º FileInfo
            file_info = FileInfo(
                file_id=file_id,
                filename=filename,
                size=file_size,
                status="processed",
                metadata={
                    'chunks': len(embedded_chunks),
                    'provider': 'huggingface_local'
                }
            )
            
            logger.info(f"Knowledge file uploaded: {filename} ({len(embedded_chunks)} chunks)")
            return True, file_info, None
            
        except Exception as e:
            error_msg = f"Failed to upload knowledge file: {str(e)}"
            logger.error(error_msg)
            return False, None, error_msg

    def query_with_rag(self, query: str, thread_id: str = None, **kwargs) -> Tuple[bool, Optional[RAGResponse], Optional[str]]:
        """
        ä½¿ç”¨ RAG åŠŸèƒ½é€²è¡ŒæŸ¥è©¢
        
        Args:
            query: æŸ¥è©¢æ–‡å­—
            thread_id: ç·šç¨‹ IDï¼ˆæœªä½¿ç”¨ï¼Œä¿æŒæ¥å£ä¸€è‡´æ€§ï¼‰
            **kwargs: é¡å¤–åƒæ•¸
                - context_messages: å°è©±ä¸Šä¸‹æ–‡
                - top_k: æª¢ç´¢çµæœæ•¸é‡ (default: 3)
                - similarity_threshold: ç›¸ä¼¼åº¦é–¾å€¼ (default: 0.7)
        """
        try:
            if not self.knowledge_store:
                # æ²’æœ‰çŸ¥è­˜åº«ï¼Œä½¿ç”¨æ™®é€šå°è©±
                return self._fallback_chat_completion(query, kwargs.get('context_messages', []))
            
            # 1. ç”ŸæˆæŸ¥è©¢çš„åµŒå…¥å‘é‡
            query_embedding = self._get_embedding(query)
            if not query_embedding:
                logger.warning("Failed to generate query embedding, falling back to normal chat")
                return self._fallback_chat_completion(query, kwargs.get('context_messages', []))
            
            # 2. æª¢ç´¢ç›¸é—œæ–‡æª”ç‰‡æ®µ
            top_k = kwargs.get('top_k', 3)
            similarity_threshold = kwargs.get('similarity_threshold', 0.7)
            
            relevant_chunks = self._vector_search(query_embedding, top_k, similarity_threshold)
            
            if not relevant_chunks:
                logger.info("No relevant documents found, using normal chat")
                return self._fallback_chat_completion(query, kwargs.get('context_messages', []))
            
            # 3. æ§‹å»ºåŒ…å«æª¢ç´¢å…§å®¹çš„ä¸Šä¸‹æ–‡
            context_messages = self._build_rag_context(
                query, 
                relevant_chunks, 
                kwargs.get('context_messages', [])
            )
            
            # 4. ç”Ÿæˆå›æ‡‰
            is_successful, chat_response, error = self.chat_completion(context_messages, **kwargs)
            
            if not is_successful:
                return False, None, error
            
            # 5. è™•ç†å¼•ç”¨å’Œä¾†æº
            answer_with_citations, sources = self._process_inline_citations(
                chat_response.content, 
                relevant_chunks
            )
            
            # 6. å‰µå»º RAGResponse
            rag_response = RAGResponse(
                answer=answer_with_citations,
                sources=sources,
                metadata={
                    "model_provider": "huggingface",
                    "model_name": self.model_name,
                    "rag_enabled": True,
                    "sources_count": len(sources),
                    "similarity_scores": [chunk.get('similarity', 0) for chunk in relevant_chunks],
                    **chat_response.metadata
                }
            )
            
            logger.info(f"RAG query completed with {len(sources)} sources")
            return True, rag_response, None
            
        except Exception as e:
            error_msg = f"RAG query failed: {str(e)}"
            logger.error(error_msg)
            return False, None, error_msg

    def get_knowledge_files(self) -> Tuple[bool, Optional[List[FileInfo]], Optional[str]]:
        """å–å¾—çŸ¥è­˜åº«æ–‡ä»¶åˆ—è¡¨"""
        try:
            files = []
            for file_id, file_data in self.knowledge_store.items():
                file_info = FileInfo(
                    file_id=file_id,
                    filename=file_data['filename'],
                    size=file_data['metadata'].get('size', 0),
                    status="processed",
                    metadata=file_data['metadata']
                )
                files.append(file_info)
            
            return True, files, None
            
        except Exception as e:
            error_msg = f"Failed to get knowledge files: {str(e)}"
            logger.error(error_msg)
            return False, None, error_msg

    def get_file_references(self) -> Dict[str, str]:
        """å–å¾—æ–‡ä»¶å¼•ç”¨æ˜ å°„"""
        references = {}
        for file_id, file_data in self.knowledge_store.items():
            filename = file_data['filename']
            # ç§»é™¤æ“´å±•å
            clean_name = filename.rsplit('.', 1)[0] if '.' in filename else filename
            references[file_id] = clean_name
        return references

    # ==================== è¼”åŠ©æ–¹æ³• ====================
    
    def _read_file_content(self, file_path: str) -> Optional[str]:
        """
        è®€å–æ–‡ä»¶å…§å®¹
        """
        try:
            import os
            from pathlib import Path
            
            path = Path(file_path)
            
            if path.suffix.lower() == '.pdf':
                # PDF æ–‡ä»¶è™•ç†
                try:
                    import PyPDF2
                    with open(file_path, 'rb') as file:
                        reader = PyPDF2.PdfReader(file)
                        text = ""
                        for page in reader.pages:
                            text += page.extract_text() + "\n"
                        return text
                except ImportError:
                    logger.warning("PyPDF2 not installed, cannot read PDF files")
                    return None
            
            elif path.suffix.lower() in ['.txt', '.md', '.json', '.csv']:
                # æ–‡æœ¬æ–‡ä»¶è™•ç†
                encodings = ['utf-8', 'utf-8-sig', 'big5', 'gb2312']
                for encoding in encodings:
                    try:
                        with open(file_path, 'r', encoding=encoding) as file:
                            return file.read()
                    except UnicodeDecodeError:
                        continue
                
                logger.error(f"Cannot decode file {file_path} with any supported encoding")
                return None
            
            else:
                logger.warning(f"Unsupported file type: {path.suffix}")
                return None
                
        except Exception as e:
            logger.error(f"Failed to read file {file_path}: {str(e)}")
            return None

    def _chunk_text(self, text: str, chunk_size: int = 1000, overlap: int = 100) -> List[Dict[str, Any]]:
        """å°‡æ–‡æœ¬åˆ†å¡Š"""
        chunks = []
        start = 0
        
        # ç¢ºä¿ overlap ä¸æœƒå¤§æ–¼ chunk_size ä¸¦ä¸”æ˜¯åˆç†çš„
        overlap = min(overlap, chunk_size // 2)
        
        while start < len(text):
            end = min(start + chunk_size, len(text))
            chunk_text = text[start:end]
            
            chunks.append({
                'text': chunk_text,
                'start': start,
                'end': end
            })
            
            # ç¢ºä¿ä¸‹ä¸€å€‹ä½ç½®ç¸½æ˜¯å‘å‰ç§»å‹•
            next_start = end - overlap
            if next_start <= start:  # é˜²æ­¢ç„¡é™å¾ªç’°
                next_start = start + max(1, chunk_size - overlap)
            
            start = next_start
            
            # å¦‚æœå·²ç¶“åˆ°é”æ–‡æœ¬æœ«å°¾ï¼Œåœæ­¢
            if end >= len(text):
                break
        
        return chunks

    def _get_embedding(self, text: str) -> Optional[List[float]]:
        """
        ç”Ÿæˆæ–‡æœ¬çš„åµŒå…¥å‘é‡
        """
        try:
            # æª¢æŸ¥ç·©å­˜
            text_hash = str(hash(text))
            if text_hash in self.embeddings_cache:
                return self.embeddings_cache[text_hash]
            
            # èª¿ç”¨ Hugging Face Embedding API
            payload = {
                "inputs": text,
                "options": {"wait_for_model": True}
            }
            
            embedding = self._make_request(self.embedding_model, payload)
            
            if embedding and isinstance(embedding, list):
                # ç·©å­˜çµæœ
                self.embeddings_cache[text_hash] = embedding
                return embedding
            
            return None
            
        except Exception as e:
            logger.error(f"Failed to generate embedding: {str(e)}")
            return None

    def _vector_search(self, query_embedding: List[float], top_k: int = 3, threshold: float = 0.7) -> List[Dict[str, Any]]:
        """å‘é‡æœç´¢ç›¸é—œæ–‡æª”"""
        try:
            results = []
            
            for file_id, file_data in self.knowledge_store.items():
                for chunk in file_data['chunks']:
                    if 'embedding' in chunk:
                        similarity = self._cosine_similarity(query_embedding, chunk['embedding'])
                        
                        if similarity >= threshold:
                            results.append({
                                'file_id': file_id,
                                'filename': file_data['filename'],
                                'text': chunk['text'],
                                'similarity': similarity
                            })
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            results.sort(key=lambda x: x['similarity'], reverse=True)
            return results[:top_k]
            
        except Exception as e:
            logger.error(f"Vector search failed: {str(e)}")
            return []

    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """
        è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦
        """
        try:
            import math
            
            dot_product = sum(a * b for a, b in zip(vec1, vec2))
            magnitude1 = math.sqrt(sum(a * a for a in vec1))
            magnitude2 = math.sqrt(sum(a * a for a in vec2))
            
            if magnitude1 == 0 or magnitude2 == 0:
                return 0.0
            
            return dot_product / (magnitude1 * magnitude2)
            
        except Exception as e:
            logger.error(f"Failed to calculate cosine similarity: {str(e)}")
            return 0.0

    def _build_rag_context(self, query: str, relevant_chunks: List[Dict[str, Any]], conversation_history: List[Dict[str, Any]]) -> List[ChatMessage]:
        """æ§‹å»º RAG ä¸Šä¸‹æ–‡"""
        messages = []
        
        # ç³»çµ±æç¤º
        system_prompt = self._build_rag_system_prompt(relevant_chunks)
        messages.append(ChatMessage(role="system", content=system_prompt))
        
        # å°è©±æ­·å²
        for conv in conversation_history[-10:]:  # é™åˆ¶æ­·å²é•·åº¦
            role = conv.get('role', 'user')
            content = conv.get('content', '')
            if content.strip():
                messages.append(ChatMessage(role=role, content=content))
        
        # ç•¶å‰æŸ¥è©¢
        messages.append(ChatMessage(role="user", content=query))
        
        return messages

    def _build_rag_system_prompt(self, relevant_chunks: List[Dict[str, Any]]) -> str:
        """
        æ§‹å»º RAG ç³»çµ±æç¤º
        """
        context_parts = []
        for i, chunk in enumerate(relevant_chunks, 1):
            filename = chunk['filename']
            text = chunk['text']
            context_parts.append(f"[{i}] ä¾†æºæ–‡ä»¶: {filename}\nå…§å®¹: {text}")
        
        context_text = "\n\n".join(context_parts)
        
        return f"""ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ AI åŠ©ç†ã€‚è«‹åŸºæ–¼ä»¥ä¸‹æä¾›çš„æ–‡æª”å…§å®¹ä¾†å›ç­”ç”¨æˆ¶å•é¡Œã€‚

ç›¸é—œæ–‡æª”å…§å®¹ï¼š
{context_text}

å›ç­”æŒ‡å—ï¼š
1. ä¸»è¦åŸºæ–¼æä¾›çš„æ–‡æª”å…§å®¹å›ç­”
2. åœ¨å›ç­”ä¸­ä½¿ç”¨ [1], [2] ç­‰æ•¸å­—ä¾†æ¨™è¨»å¼•ç”¨ä¾†æº
3. å¦‚æœæ–‡æª”å…§å®¹ä¸è¶³ä»¥å›ç­”å•é¡Œï¼Œå¯ä»¥çµåˆä½ çš„çŸ¥è­˜è£œå……
4. ä¿æŒå›ç­”æº–ç¢ºã€å®¢è§€ã€æœ‰ç”¨
5. ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”"""

    def _process_inline_citations(self, text: str, relevant_chunks: List[Dict[str, Any]]) -> Tuple[str, List[Dict[str, Any]]]:
        """è™•ç†å…§è¯å¼•ç”¨"""
        import re
        
        sources = []
        
        # æŸ¥æ‰¾ [æ•¸å­—] æ ¼å¼çš„å¼•ç”¨
        citations = re.findall(r'\[(\d+)\]', text)
        
        for citation in set(citations):
            citation_num = int(citation)
            if 0 < citation_num <= len(relevant_chunks):
                chunk = relevant_chunks[citation_num - 1]
                sources.append({
                    'file_id': chunk['file_id'],
                    'filename': chunk['filename'],
                    'quote': chunk['text'][:200] + "..." if len(chunk['text']) > 200 else chunk['text'],
                    'type': 'file_citation'
                })
        
        # å¦‚æœæ²’æœ‰æ‰¾åˆ°å¼•ç”¨ï¼Œä½¿ç”¨æ‰€æœ‰æª¢ç´¢åˆ°çš„æ–‡æª”ä½œç‚ºä¾†æº
        if not sources:
            for chunk in relevant_chunks:
                sources.append({
                    'file_id': chunk['file_id'],
                    'filename': chunk['filename'],
                    'quote': chunk['text'][:200] + "..." if len(chunk['text']) > 200 else chunk['text'],
                    'type': 'file_citation'
                })
        
        return text, sources

    def _fallback_chat_completion(self, query: str, context_messages: List[Dict[str, Any]]) -> Tuple[bool, Optional[RAGResponse], Optional[str]]:
        """
        å‚™ç”¨èŠå¤©å®Œæˆï¼ˆç„¡ RAGï¼‰
        """
        try:
            messages = self._build_conversation_context(context_messages, query)
            is_successful, chat_response, error = self.chat_completion(messages)
            
            if not is_successful:
                return False, None, error
            
            rag_response = RAGResponse(
                answer=chat_response.content,
                sources=[],
                metadata={
                    "model_provider": "huggingface",
                    "rag_enabled": False,
                    "no_sources": True,
                    **chat_response.metadata
                }
            )
            
            return True, rag_response, None
            
        except Exception as e:
            return False, None, str(e)

    # ==================== AssistantInterface ====================
    
    def create_thread(self) -> Tuple[bool, Optional[ThreadInfo], Optional[str]]:
        """å‰µå»ºæ–°çš„å°è©±ç·šç¨‹"""
        try:
            thread_id = f"hf_thread_{uuid.uuid4().hex[:12]}"
            thread_info = ThreadInfo(
                thread_id=thread_id,
                created_at=str(int(time.time())),
                metadata={"provider": "huggingface", "messages": []}
            )
            
            self.local_threads[thread_id] = {
                "created_at": time.time(),
                "messages": []
            }
            
            return True, thread_info, None
            
        except Exception as e:
            return False, None, str(e)

    def delete_thread(self, thread_id: str) -> Tuple[bool, Optional[str]]:
        """åˆªé™¤å°è©±ç·šç¨‹"""
        try:
            if thread_id in self.local_threads:
                del self.local_threads[thread_id]
            return True, None
        except Exception as e:
            return False, str(e)

    def add_message_to_thread(self, thread_id: str, message: ChatMessage) -> Tuple[bool, Optional[str]]:
        """æ·»åŠ è¨Šæ¯åˆ°ç·šç¨‹"""
        try:
            if thread_id not in self.local_threads:
                return False, f"Thread {thread_id} not found"
            
            self.local_threads[thread_id]["messages"].append({
                "role": message.role,
                "content": message.content,
                "timestamp": time.time()
            })
            
            return True, None
        except Exception as e:
            return False, str(e)

    def run_assistant(self, thread_id: str, **kwargs) -> Tuple[bool, Optional[RAGResponse], Optional[str]]:
        """
        åŸ·è¡ŒåŠ©ç†
        """
        try:
            if thread_id not in self.local_threads:
                return False, None, f"Thread {thread_id} not found"
            
            # å–å¾—ç·šç¨‹ä¸­çš„æœ€å¾Œä¸€å€‹ç”¨æˆ¶è¨Šæ¯
            messages = self.local_threads[thread_id]["messages"]
            user_messages = [msg for msg in messages if msg["role"] == "user"]
            
            if not user_messages:
                return False, None, "No user message in thread"
            
            last_message = user_messages[-1]["content"]
            
            # ä½¿ç”¨ query_with_rag è™•ç†
            return self.query_with_rag(last_message, thread_id, **kwargs)
            
        except Exception as e:
            return False, None, str(e)

    # ==================== AudioInterface ====================
    
    def transcribe_audio(self, audio_file_path: str, **kwargs) -> Tuple[bool, Optional[str], Optional[str]]:
        """
        èªéŸ³è½‰æ–‡å­—
        """
        try:
            import os
            
            if not os.path.exists(audio_file_path):
                return False, None, f"Audio file not found: {audio_file_path}"
            
            # è®€å–éŸ³é »æ–‡ä»¶
            with open(audio_file_path, 'rb') as audio_file:
                audio_data = audio_file.read()
            
            # ä½¿ç”¨ Hugging Face Automatic Speech Recognition API
            headers = {
                "Authorization": f"Bearer {self.api_key}"
            }
            
            response = requests.post(
                f"{self.base_url}/models/{self.speech_model}",
                headers=headers,
                data=audio_data,
                timeout=120  # èªéŸ³è™•ç†å¯èƒ½éœ€è¦æ›´é•·æ™‚é–“
            )
            
            if response.status_code == 200:
                result = response.json()
                if isinstance(result, dict) and 'text' in result:
                    transcribed_text = result['text']
                elif isinstance(result, list) and len(result) > 0:
                    transcribed_text = result[0].get('text', str(result))
                else:
                    transcribed_text = str(result)
                
                logger.info(f"Audio transcription successful: {len(transcribed_text)} chars")
                return True, transcribed_text, None
            else:
                error_msg = f"Transcription failed: {response.status_code} - {response.text}"
                logger.error(error_msg)
                return False, None, error_msg
                
        except Exception as e:
            error_msg = f"Audio transcription failed: {str(e)}"
            logger.error(error_msg)
            return False, None, error_msg

    # ==================== ImageInterface ====================
    
    def generate_image(self, prompt: str, **kwargs) -> Tuple[bool, Optional[str], Optional[str]]:
        """
        åœ–ç‰‡ç”Ÿæˆ
        """
        try:
            # æº–å‚™è«‹æ±‚åƒæ•¸
            payload = {
                "inputs": prompt,
                "parameters": {
                    "num_inference_steps": kwargs.get('steps', 20),
                    "guidance_scale": kwargs.get('guidance_scale', 7.5),
                    "width": kwargs.get('width', 512),
                    "height": kwargs.get('height', 512)
                },
                "options": {
                    "wait_for_model": True
                }
            }
            
            response = requests.post(
                f"{self.base_url}/models/{self.image_model}",
                headers=self.headers,
                json=payload,
                timeout=180  # åœ–ç‰‡ç”Ÿæˆéœ€è¦è¼ƒé•·æ™‚é–“
            )
            
            if response.status_code == 200:
                # Hugging Face è¿”å›çš„æ˜¯åœ–ç‰‡çš„äºŒé€²åˆ¶æ•¸æ“š
                image_data = response.content
                
                # å°‡åœ–ç‰‡æ•¸æ“šè½‰æ›ç‚º base64 ç·¨ç¢¼
                image_base64 = base64.b64encode(image_data).decode('utf-8')
                image_url = f"data:image/png;base64,{image_base64}"
                
                logger.info(f"Image generation successful: {len(image_data)} bytes")
                return True, image_url, None
            else:
                error_msg = f"Image generation failed: {response.status_code} - {response.text}"
                logger.error(error_msg)
                return False, None, error_msg
                
        except Exception as e:
            error_msg = f"Image generation failed: {str(e)}"
            logger.error(error_msg)
            return False, None, error_msg