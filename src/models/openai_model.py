"""
OpenAI Model å¯¦ä½œ
ä½¿ç”¨ OpenAI Assistant API æä¾›èŠå¤©å’ŒéŸ³è¨Šè½‰éŒ„åŠŸèƒ½

ğŸ“‹ æ¶æ§‹è·è²¬åˆ†å·¥ï¼š
âœ… RESPONSIBILITIES (æ¨¡å‹å±¤è·è²¬):
  - å¯¦ä½œçµ±ä¸€çš„ FullLLMInterface æ¥å£
  - æä¾› chat_with_user() æ–‡å­—å°è©±åŠŸèƒ½
  - æä¾› transcribe_audio() éŸ³è¨Šè½‰éŒ„åŠŸèƒ½
  - ç®¡ç† OpenAI Assistant threads å’Œå°è©±æ­·å²
  - è™•ç† OpenAI API é™æµå’Œé‡è©¦é‚è¼¯

âŒ NEVER DO (çµ•å°ç¦æ­¢):
  - çŸ¥é“è¨Šæ¯ä¾†æºå¹³å° (LINEã€Telegram ç­‰)
  - è™•ç†å¹³å°ç‰¹å®šçš„è¨Šæ¯æ ¼å¼
  - ç›´æ¥è™•ç† webhook æˆ–ç¶²è·¯è«‹æ±‚
  - è·¯ç”±è¨Šæ¯æˆ–å”èª¿æœå‹™

ğŸ”„ çµ±ä¸€æ¥å£ï¼š
  - chat_with_user(user_id, message, platform) -> (bool, str, str)
  - transcribe_audio(file_path) -> (bool, str, str)
  - clear_user_history(user_id, platform) -> (bool, str)
  - check_connection() -> (bool, str)

ğŸ¯ æ¨¡å‹ç‰¹è‰²ï¼š
  - ä½¿ç”¨ Assistant API é€²è¡Œå°è©±ç®¡ç†
  - æ”¯æ´ RAG (æª¢ç´¢å¢å¼·ç”Ÿæˆ)
  - ä½¿ç”¨ Whisper API é€²è¡ŒéŸ³è¨Šè½‰éŒ„
  - DALL-E API åœ–ç‰‡ç”Ÿæˆ
  - æ™ºæ…§é‡è©¦å’ŒéŒ¯èª¤è™•ç†
  - Thread-based å°è©±æ­·å²ç®¡ç†

âœ… å®Œæ•´åŠŸèƒ½æ”¯æ´ï¼š
  - å°è©±: Assistant API (æœ€ç©©å®š)
  - éŸ³è¨Šè½‰éŒ„: Whisper API (åŸç”Ÿæ”¯æ´ï¼Œæœ€ä½³å“è³ª)
  - åœ–ç‰‡ç”Ÿæˆ: DALL-E API (åŸç”Ÿæ”¯æ´)
  - é€£ç·šç‹€æ…‹: ä¼æ¥­ç´šç©©å®šæ€§
"""

import requests
from ..core.logger import get_logger
from ..core.api_timeouts import SmartTimeoutConfig, TimeoutContext
from ..core.smart_polling import OpenAIPollingStrategy, PollingContext
import re
from typing import List, Dict, Tuple, Optional, Any
import time

logger = get_logger(__name__)
from .base import (
    FullLLMInterface, 
    ModelProvider, 
    ChatMessage, 
    ChatResponse, 
    ThreadInfo, 
    FileInfo,
    KnowledgeBase,
    RAGResponse
)
from ..utils.retry import retry_with_backoff, retry_on_rate_limit, CircuitBreaker
from ..utils import s2t_converter, dedup_citation_blocks


class OpenAIModel(FullLLMInterface):
    """OpenAI æ¨¡å‹å¯¦ä½œ"""
    
    def __init__(self, api_key: str, assistant_id: str = None, base_url: str = None, enable_mcp: bool = None):
        self.api_key = api_key
        self.assistant_id = assistant_id
        self.base_url = base_url or 'https://api.openai.com/v1'
        self.polling_strategy = OpenAIPollingStrategy()
        
        # MCP æ”¯æ´ - å¾è¨­å®šæª”è®€å–
        if enable_mcp is None:
            try:
                from ..core.config import get_value
                feature_enabled = get_value('features.enable_mcp', False)
                mcp_enabled = get_value('mcp.enabled', False)
                self.enable_mcp = feature_enabled and mcp_enabled
            except Exception as e:
                logger.warning(f"Error reading MCP config: {e}")
                self.enable_mcp = False
        else:
            self.enable_mcp = enable_mcp
            
        self.mcp_service = None
        if self.enable_mcp:
            self._init_mcp_service()
        
        # å»ºæ§‹ system promptï¼ˆç‚ºæœªä¾†çš„ Chat Completion API æº–å‚™ï¼‰
        self.system_prompt = self._build_system_prompt()
    
    def get_provider(self) -> ModelProvider:
        return ModelProvider.OPENAI
    
    def _init_mcp_service(self) -> None:
        """åˆå§‹åŒ– MCP æœå‹™"""
        try:
            from ..services.mcp_service import get_mcp_service
            
            mcp_service = get_mcp_service()
            if mcp_service.is_enabled:
                self.mcp_service = mcp_service
                logger.info("OpenAI Model: MCP service initialized successfully")
            else:
                logger.warning("OpenAI Model: MCP service is not enabled")
                self.enable_mcp = False
        except Exception as e:
            logger.warning(f"OpenAI Model: Failed to initialize MCP service: {e}")
            self.enable_mcp = False
            self.mcp_service = None
    
    def create_assistant_with_mcp_functions(self, **kwargs) -> Tuple[bool, Optional[str], Optional[str]]:
        """å‰µå»ºåŒ…å« MCP functions çš„ Assistant"""
        try:
            if not self.enable_mcp or not self.mcp_service:
                logger.info("Creating regular assistant (MCP disabled)")
                return self._create_regular_assistant(**kwargs)
            
            # å–å¾— MCP function schemas
            function_schemas = self.mcp_service.get_function_schemas_for_openai()
            if not function_schemas:
                logger.warning("No MCP function schemas available, creating regular assistant")
                return self._create_regular_assistant(**kwargs)
            
            logger.info(f"Creating MCP-enabled assistant with {len(function_schemas)} functions")
            
            # å‰µå»ºåŒ…å« MCP functions çš„ Assistant
            json_body = {
                'name': kwargs.get('name', 'MCP-Enabled Assistant'),
                'instructions': kwargs.get('instructions', 'You are a helpful assistant with access to external tools.'),
                'model': kwargs.get('model', 'gpt-4'),
                'tools': function_schemas,
                'temperature': kwargs.get('temperature', 0.01)
            }
            
            is_successful, response, error_message = self._request('POST', '/assistants', body=json_body, assistant=True)
            
            if is_successful:
                assistant_id = response['id']
                self.assistant_id = assistant_id
                logger.info(f"Created MCP-enabled assistant: {assistant_id}")
                return True, assistant_id, None
            else:
                logger.error(f"Failed to create MCP assistant: {error_message}")
                return False, None, error_message
                
        except Exception as e:
            logger.error(f"Error creating MCP assistant: {e}")
            return False, None, str(e)
    
    def _create_regular_assistant(self, **kwargs) -> Tuple[bool, Optional[str], Optional[str]]:
        """å‰µå»ºä¸€èˆ¬çš„ Assistantï¼ˆç„¡ MCP functionsï¼‰"""
        try:
            json_body = {
                'name': kwargs.get('name', 'Regular Assistant'),
                'instructions': kwargs.get('instructions', 'You are a helpful assistant.'),
                'model': kwargs.get('model', 'gpt-4'),
                'temperature': kwargs.get('temperature', 0.01)
            }
            
            is_successful, response, error_message = self._request('POST', '/assistants', body=json_body, assistant=True)
            
            if is_successful:
                assistant_id = response['id']
                self.assistant_id = assistant_id
                logger.info(f"Created regular assistant: {assistant_id}")
                return True, assistant_id, None
            else:
                return False, None, error_message
                
        except Exception as e:
            return False, None, str(e)
    
    def check_connection(self) -> Tuple[bool, Optional[str]]:
        """æª¢æŸ¥ OpenAI API é€£ç·š"""
        try:
            is_successful, response, error_message = self._request('GET', '/models', operation='health_check')
            if is_successful:
                return True, None
            else:
                return False, error_message
        except Exception as e:
            return False, str(e)
    
    def chat_completion(self, messages: List[ChatMessage], **kwargs) -> Tuple[bool, Optional[ChatResponse], Optional[str]]:
        """OpenAI Chat Completion"""
        try:
            # è½‰æ›è¨Šæ¯æ ¼å¼
            openai_messages = [
                {"role": msg.role, "content": msg.content} 
                for msg in messages
            ]
            
            json_body = {
                'model': kwargs.get('model', 'gpt-4'),
                'messages': openai_messages,
                'temperature': kwargs.get('temperature', 0.01)
            }
            
            is_successful, response, error_message = self._request('POST', '/chat/completions', body=json_body)
            
            if not is_successful:
                return False, None, error_message
            
            content = response['choices'][0]['message']['content']
            finish_reason = response['choices'][0].get('finish_reason')
            
            chat_response = ChatResponse(
                content=content,
                finish_reason=finish_reason,
                metadata={'usage': response.get('usage')}
            )
            
            return True, chat_response, None
            
        except Exception as e:
            return False, None, str(e)
    
    def create_thread(self) -> Tuple[bool, Optional[ThreadInfo], Optional[str]]:
        """å»ºç«‹ OpenAI Assistant å°è©±ä¸²"""
        try:
            is_successful, response, error_message = self._request('POST', '/threads', assistant=True)
            
            if not is_successful:
                return False, None, error_message
            
            thread_info = ThreadInfo(
                thread_id=response['id'],
                created_at=response.get('created_at'),
                metadata=response
            )
            
            return True, thread_info, None
            
        except Exception as e:
            return False, None, str(e)
    
    def delete_thread(self, thread_id: str) -> Tuple[bool, Optional[str]]:
        """åˆªé™¤å°è©±ä¸²"""
        try:
            endpoint = f'/threads/{thread_id}'
            is_successful, response, error_message = self._request('DELETE', endpoint, assistant=True)
            return is_successful, error_message
        except Exception as e:
            return False, str(e)
    
    def add_message_to_thread(self, thread_id: str, message: ChatMessage) -> Tuple[bool, Optional[str]]:
        """æ–°å¢è¨Šæ¯åˆ°å°è©±ä¸²"""
        try:
            endpoint = f'/threads/{thread_id}/messages'
            json_body = {
                'role': message.role,
                'content': message.content
            }
            is_successful, response, error_message = self._request('POST', endpoint, body=json_body, assistant=True)
            return is_successful, error_message
        except Exception as e:
            return False, str(e)
    
    def run_assistant(self, thread_id: str, **kwargs) -> Tuple[bool, Optional[ChatResponse], Optional[str]]:
        """åŸ·è¡Œ OpenAI Assistantï¼ˆæ”¯æ´ MCP function callingï¼‰"""
        try:
            # å•Ÿå‹•åŸ·è¡Œ
            endpoint = f'/threads/{thread_id}/runs'
            json_body = {
                'assistant_id': self.assistant_id,
                'temperature': kwargs.get('temperature', 0.01)
            }
            
            is_successful, run_response, error_message = self._request('POST', endpoint, body=json_body, assistant=True)
            if not is_successful:
                return False, None, error_message
            
            # ç­‰å¾…å®Œæˆï¼ˆåŒ…å« MCP function calling è™•ç†ï¼‰
            run_id = run_response['id']
            if self.enable_mcp and self.mcp_service:
                import asyncio
                is_successful, final_response, error_message = asyncio.run(
                    self._wait_for_run_completion_with_mcp(thread_id, run_id)
                )
            else:
                is_successful, final_response, error_message = self._wait_for_run_completion(thread_id, run_id)

            if not is_successful:
                return False, None, f"Assistant run failed: {error_message}"
            
            # å–å¾—å›æ‡‰
            return self._get_thread_messages(thread_id)
            
        except Exception as e:
            return False, None, str(e)
    
    async def _wait_for_run_completion_with_mcp(self, thread_id: str, run_id: str, max_wait_time: int = None) -> Tuple[bool, Optional[Dict], Optional[str]]:
        """æ™ºæ…§ç­‰å¾…åŸ·è¡Œå®Œæˆï¼ˆæ”¯æ´ MCP function callingï¼‰"""
        
        if max_wait_time:
            self.polling_strategy.max_wait_time = max_wait_time
        
        max_iterations = 10  # é˜²æ­¢ç„¡é™å¾ªç’°
        iteration = 0
        
        while iteration < max_iterations:
            iteration += 1
            
            # æª¢æŸ¥åŸ·è¡Œç‹€æ…‹
            is_successful, response, error_message = self.retrieve_thread_run(thread_id, run_id)
            if not is_successful:
                return False, None, error_message
            
            status = response['status']
            logger.debug(f"Run {run_id} status: {status} (iteration {iteration})")
            
            if status == 'completed':
                return True, response, None
            elif status in ['failed', 'expired', 'cancelled']:
                return False, None, f"Run {status}: {response.get('last_error', {}).get('message', 'Unknown error')}"
            elif status == 'requires_action':
                # è™•ç† MCP function calling
                logger.info("Run requires action - processing MCP function calls")
                success = await self._handle_mcp_function_calls(thread_id, run_id, response)
                if not success:
                    return False, None, "Failed to handle MCP function calls"
                # ç¹¼çºŒè¼ªè©¢
            elif status in ['queued', 'in_progress']:
                # ç¹¼çºŒç­‰å¾…
                pass
            
            # ç­‰å¾…ä¸€æ®µæ™‚é–“å†æª¢æŸ¥
            import time
            time.sleep(1)
        
        return False, None, f"Run did not complete within {max_iterations} iterations"
    
    async def _handle_mcp_function_calls(self, thread_id: str, run_id: str, run_response: Dict) -> bool:
        """è™•ç† MCP function calls"""
        try:
            required_action = run_response.get('required_action', {})
            tool_calls = required_action.get('submit_tool_outputs', {}).get('tool_calls', [])
            
            if not tool_calls:
                logger.warning("No tool calls found in requires_action")
                return False
            
            logger.info(f"Processing {len(tool_calls)} MCP function calls")
            tool_outputs = []
            
            for tool_call in tool_calls:
                function_name = tool_call['function']['name']
                arguments_str = tool_call['function']['arguments']
                
                try:
                    import json
                    arguments = json.loads(arguments_str)
                except json.JSONDecodeError as e:
                    logger.error(f"Invalid JSON in function arguments: {e}")
                    tool_outputs.append({
                        "tool_call_id": tool_call['id'],
                        "output": json.dumps({
                            "success": False,
                            "error": "Invalid function arguments format"
                        }, ensure_ascii=False)
                    })
                    continue
                
                logger.info(f"Executing MCP function: {function_name} with args: {arguments}")
                
                # åŸ·è¡Œ MCP function call
                result = await self.mcp_service.handle_function_call(function_name, arguments)
                
                tool_outputs.append({
                    "tool_call_id": tool_call['id'],
                    "output": json.dumps(result, ensure_ascii=False)
                })
            
            # æäº¤ tool outputs åˆ° OpenAI
            endpoint = f'/threads/{thread_id}/runs/{run_id}/submit_tool_outputs'
            json_body = {
                "tool_outputs": tool_outputs
            }
            
            is_successful, response, error_message = self._request('POST', endpoint, body=json_body, assistant=True)
            
            if is_successful:
                logger.info(f"Successfully submitted {len(tool_outputs)} tool outputs")
                return True
            else:
                logger.error(f"Failed to submit tool outputs: {error_message}")
                return False
                
        except Exception as e:
            logger.error(f"Error handling MCP function calls: {e}")
            return False
    
    def get_mcp_status(self) -> Dict[str, Any]:
        """å–å¾— MCP æœå‹™ç‹€æ…‹"""
        return {
            "enabled": self.enable_mcp,
            "service_available": self.mcp_service is not None,
            "service_info": self.mcp_service.get_service_info() if self.mcp_service else None
        }
    
    def reload_mcp_config(self) -> bool:
        """é‡æ–°è¼‰å…¥ MCP è¨­å®š"""
        if self.mcp_service:
            success = self.mcp_service.reload_config()
            if success:
                # é‡æ–°å»ºæ§‹ system prompt
                self.system_prompt = self._build_system_prompt()
                logger.info("OpenAI Model: MCP config reloaded and system prompt updated")
            return success
        return False
    
    def _build_system_prompt(self) -> str:
        """å»ºæ§‹ system promptï¼ˆç‚ºæœªä¾† Chat Completion API ä½¿ç”¨ï¼‰
        
        æ³¨æ„ï¼šOpenAI Assistant API ä½¿ç”¨é è¨­çš„ instructionsï¼Œä¸æœƒç›´æ¥ä½¿ç”¨æ­¤ system promptã€‚
        ä½†ä¿ç•™æ­¤åŠŸèƒ½ä»¥ä¾¿æœªä¾†å¯èƒ½çš„ Chat Completion API æ•´åˆã€‚
        """
        # å¾è¨­å®šæª”è®€å–åŸºç¤ system prompt
        if self.enable_mcp:
            try:
                from ..core.config import get_value
                base_prompt = get_value('mcp.system_prompt', "You are a helpful AI assistant.")
            except Exception:
                base_prompt = "You are a helpful AI assistant."
        else:
            base_prompt = "You are a helpful AI assistant."
        
        if self.enable_mcp and self.mcp_service:
            try:
                # å–å¾—å¯ç”¨çš„ function schemas
                function_schemas = self.mcp_service.get_function_schemas_for_openai()
                if function_schemas:
                    base_prompt += """

## å·¥å…·èª¿ç”¨èƒ½åŠ› (Function Calling Capabilities)

æ‚¨å…·å‚™èª¿ç”¨å¤–éƒ¨å·¥å…·çš„èƒ½åŠ›ï¼Œè«‹éµå¾ªä»¥ä¸‹æŒ‡å¼•ï¼š

### èª¿ç”¨åŸå‰‡ï¼š
- åƒ…åœ¨ç”¨æˆ¶æ˜ç¢ºéœ€è¦æˆ–æœ‰æ˜ç¢ºæŒ‡ç¤ºæ™‚èª¿ç”¨å·¥å…·
- èª¿ç”¨å‰å‘ç”¨æˆ¶èªªæ˜å°‡è¦åŸ·è¡Œçš„æ“ä½œ
- å°å·¥å…·è¿”å›çš„çµæœé€²è¡Œé©ç•¶çš„è§£é‡‹å’Œåˆ†æ
- æ˜ç¢ºæ¨™ç¤ºè³‡è¨Šä¾†æºï¼Œæå‡å›æ‡‰çš„é€æ˜åº¦

### å®‰å…¨è€ƒé‡ï¼š
- ç¢ºä¿åƒæ•¸çš„æº–ç¢ºæ€§å’Œå®Œæ•´æ€§
- å°æ•æ„ŸæŸ¥è©¢æä¾›é©ç•¶çš„ä¸Šä¸‹æ–‡
- ä¿è­·ç”¨æˆ¶æŸ¥è©¢çš„éš±ç§æ€§
- éµå¾ªæœ€å°æ¬Šé™åŸå‰‡

### éŒ¯èª¤è™•ç†ï¼š
- å¦‚æœå·¥å…·èª¿ç”¨å¤±æ•—ï¼Œè§£é‡‹å•é¡Œä¸¦æä¾›æ›¿ä»£æ–¹æ¡ˆ
- å°ä¸ç¢ºå®šçš„çµæœé€²è¡Œé©ç•¶çš„è­¦ç¤º
- å¼•å°ç”¨æˆ¶æä¾›æ›´æ˜ç¢ºçš„æŸ¥è©¢æ¢ä»¶

é€™äº›å·¥å…·å°‡é€šé OpenAI function calling æ©Ÿåˆ¶è‡ªå‹•èª¿ç”¨ï¼Œæ‚¨ç„¡éœ€æ‰‹å‹•æ ¼å¼åŒ–å‡½æ•¸èª¿ç”¨ã€‚"""
                    
                    logger.info("OpenAI Model: Added MCP tool usage guidelines to system prompt")
            except Exception as e:
                logger.error(f"Failed to add MCP guidelines to system prompt: {e}")
        
        return base_prompt
    
    # === RAG ä»‹é¢å¯¦ä½œï¼ˆä½¿ç”¨ OpenAI Assistant APIï¼‰ ===
    
    def upload_knowledge_file(self, file_path: str, **kwargs) -> Tuple[bool, Optional[FileInfo], Optional[str]]:
        """ä¸Šå‚³æª”æ¡ˆåˆ° OpenAIï¼ˆç”¨æ–¼ Assistant APIï¼‰"""
        try:
            with open(file_path, 'rb') as f:
                files = {
                    'file': f,
                    'purpose': (None, 'assistants')
                }
                is_successful, response, error_message = self._request('POST', '/files', files=files)
            
            if not is_successful:
                return False, None, error_message
            
            file_info = FileInfo(
                file_id=response['id'],
                filename=response['filename'],
                size=response.get('bytes'),
                status=response.get('status'),
                purpose=response.get('purpose'),
                metadata=response
            )
            
            return True, file_info, None
            
        except Exception as e:
            return False, None, str(e)
    
    def query_with_rag(self, query: str, thread_id: str = None, **kwargs) -> Tuple[bool, Optional[RAGResponse], Optional[str]]:
        """ä½¿ç”¨ OpenAI Assistant API é€²è¡Œ RAG æŸ¥è©¢"""
        try:
            # å¦‚æœæ²’æœ‰ thread_idï¼Œå»ºç«‹æ–°çš„
            if not thread_id:
                is_successful, thread_info, error_message = self.create_thread()
                if not is_successful:
                    return False, None, error_message
                thread_id = thread_info.thread_id
            
            # æ–°å¢è¨Šæ¯åˆ°å°è©±ä¸²
            message = ChatMessage(role='user', content=query)
            is_successful, error_message = self.add_message_to_thread(thread_id, message)
            if not is_successful:
                return False, None, error_message
            
            # åŸ·è¡ŒåŠ©ç†
            is_successful, chat_response, error_message = self.run_assistant(thread_id, **kwargs)
            if not is_successful:
                return False, None, error_message
            
            # ä½¿ç”¨ OpenAI ç‰¹å®šçš„å¼•ç”¨è™•ç†é‚è¼¯
            thread_messages = chat_response.metadata.get('thread_messages', {})
            formatted_content, sources = self._process_openai_response(thread_messages)
            
            rag_response = RAGResponse(
                answer=formatted_content,
                sources=sources,
                metadata={
                    'thread_id': thread_id,
                    'model': 'openai-assistant',
                    'thread_messages': thread_messages,
                    'raw_content': chat_response.content
                }
            )
            
            return True, rag_response, None
            
        except Exception as e:
            return False, None, str(e)
    
    def get_knowledge_files(self) -> Tuple[bool, Optional[List[FileInfo]], Optional[str]]:
        """å–å¾— OpenAI æª”æ¡ˆåˆ—è¡¨"""
        return self.list_files()
    
    def list_files(self) -> Tuple[bool, Optional[List[FileInfo]], Optional[str]]:
        """åˆ—å‡ºæª”æ¡ˆ"""
        try:
            is_successful, response, error_message = self._request('GET', '/files', assistant=True)
            
            if not is_successful:
                return False, None, error_message
            
            files = [
                FileInfo(
                    file_id=file['id'],
                    filename=file['filename'],
                    size=file.get('bytes'),
                    status=file.get('status'),
                    purpose=file.get('purpose'),
                    metadata=file
                )
                for file in response['data']
            ]
            
            return True, files, None
            
        except Exception as e:
            return False, None, str(e)
    
    def get_file_references(self) -> Dict[str, str]:
        """å–å¾—æª”æ¡ˆå¼•ç”¨å°æ‡‰è¡¨"""
        try:
            is_successful, files, error_message = self.list_files()
            if not is_successful:
                logger.warning(f"Failed to get file references: {error_message}")
                return {}
            
            file_dict = {}
            for file in files:
                filename = file.filename.replace('.txt', '').replace('.json', '')
                file_dict[file.file_id] = filename
            
            logger.debug(f"Loaded {len(file_dict)} file references")
            return file_dict
            
        except Exception as e:
            logger.error(f"Error getting file references: {e}")
            return {}
    
    def _process_openai_response(self, thread_messages: Dict) -> Tuple[str, List[Dict[str, str]]]:
        """
        è™•ç† OpenAI Assistant API çš„å›æ‡‰ï¼ŒåŒ…æ‹¬å¼•ç”¨æ ¼å¼åŒ–
        é€™å€‹æ–¹æ³•å°è£äº†åŸæœ¬çš„ get_content_and_reference é‚è¼¯
        """
        try:
            # å–å¾—åŠ©ç†å›æ‡‰æ•¸æ“š
            data = self._get_response_data(thread_messages)
            logger.debug("OpenAI Assistant response data:")
            logger.debug(data)
            if not data:
                logger.debug("_process_openai_response: æ²’æœ‰æ‰¾åˆ°åŠ©ç†å›æ‡‰æ•¸æ“š")
                return '', []
            text = data['content'][0]['text']['value']
            annotations = data['content'][0]['text']['annotations']
            
            logger.debug(f"_process_openai_response: è¨»è§£æ•¸é‡={len(annotations)}")
            
            # æª¢æŸ¥æ˜¯å¦æœ‰è¤‡é›œå¼•ç”¨æ ¼å¼åœ¨åŸå§‹æ–‡æœ¬ä¸­
            complex_citations = re.findall(r'ã€[^ã€‘]+ã€‘', text)
            if complex_citations:
                logger.debug(f"_process_openai_response: ç™¼ç¾ {len(complex_citations)} å€‹è¤‡é›œå¼•ç”¨æ ¼å¼")
            
            # è½‰æ›ç‚ºç¹é«”ä¸­æ–‡
            text = s2t_converter.convert(text)
            
            # å–å¾—æª”æ¡ˆå­—å…¸ç”¨æ–¼å¼•ç”¨è™•ç†
            file_dict = self.get_file_references()
            
            # æ›¿æ›è¨»é‡‹æ–‡æœ¬å’Œå»ºç«‹ä¾†æºæ¸…å–®
            citation_map: dict[str, int] = {}
            sources: list[dict] = []
            next_num = 1  # ä¸‹ä¸€å€‹å¯ç”¨çš„å¼•ç”¨ç·¨è™Ÿ

            for annotation in annotations:
                original_text = s2t_converter.convert(annotation["text"])
                file_id = annotation["file_citation"]["file_id"]
                filename = file_dict.get(file_id, "Unknown")

                # 2) å–å¾—ï¼ˆæˆ–ç”¢ç”Ÿï¼‰æ­¤æª”æ¡ˆçš„ç·¨è™Ÿ
                if filename in citation_map:
                    ref_num = citation_map[filename]          # å·²ç¶“æœ‰ â†’ ç›´æ¥é‡ç”¨
                else:
                    ref_num = next_num                        # ç¬¬ä¸€æ¬¡çœ‹åˆ° â†’ æŒ‡æ´¾æ–°è™Ÿç¢¼
                    citation_map[filename] = ref_num
                    next_num += 1

                    # åªåœ¨ç¬¬ä¸€æ¬¡çœ‹åˆ°æ™‚ï¼ŒæŠŠå®ƒæ”¾é€² sourcesï¼Œé¿å…é‡è¤‡
                    sources.append({
                        "file_id": file_id,
                        "filename": filename,
                        "quote": annotation["file_citation"].get("quote", ""),
                        "type": "file_citation",
                    })

                # 3) å–ä»£æ­£æ–‡ä¸­çš„å¼•ç”¨æ¨™ç±¤
                replacement_text = f"[{ref_num}]"
                text = text.replace(original_text, replacement_text)
            
            # ç›´æ¥è¿”å›è™•ç†å¾Œçš„æ–‡æœ¬ï¼Œè®“ ResponseFormatter çµ±ä¸€è™•ç† sources
            final_text = dedup_citation_blocks(text.strip())
            
            logger.debug(f"_process_openai_response: æœ€çµ‚æ–‡æœ¬é•·åº¦={len(final_text)}, ç”Ÿæˆäº† {len(sources)} å€‹ä¾†æº")
            
            return final_text, sources
            
        except Exception as e:
            logger.error(f"Error processing OpenAI response: {e}")
            return '', []
    
    def _get_response_data(self, response: Dict) -> Dict:
        """å¾ OpenAI å›æ‡‰ä¸­æå–åŠ©ç†æ•¸æ“š"""
        try:
            for item in response.get('data', []):
                if item.get('role') == 'assistant' and item.get('content') and item['content'][0].get('type') == 'text':
                    return item
            return None
        except Exception as e:
            logger.error(f"Error getting response data: {e}")
            return None
    
    def transcribe_audio(self, audio_file_path: str, **kwargs) -> Tuple[bool, Optional[str], Optional[str]]:
        """éŸ³è¨Šè½‰æ–‡å­—"""
        try:
            # OpenAI æ¨¡å‹é è¨­ä½¿ç”¨ whisper-1
            model = kwargs.get('model', 'whisper-1')
            
            files = {
                'file': open(audio_file_path, 'rb'),
                'model': (None, model),
            }
            is_successful, response, error_message = self._request('POST', '/audio/transcriptions', files=files, operation='audio_transcription')
            
            if not is_successful:
                return False, None, error_message
            
            return True, response['text'], None
            
        except Exception as e:
            return False, None, str(e)
    
    def generate_image(self, prompt: str, **kwargs) -> Tuple[bool, Optional[str], Optional[str]]:
        """ç”Ÿæˆåœ–ç‰‡"""
        try:
            json_body = {
                "prompt": prompt,
                "n": kwargs.get('n', 1),
                "size": kwargs.get('size', '512x512')
            }
            is_successful, response, error_message = self._request('POST', '/images/generations', body=json_body)
            
            if not is_successful:
                return False, None, error_message
            
            image_url = response['data'][0]['url']
            return True, image_url, None
            
        except Exception as e:
            return False, None, str(e)
    
    # === å‘å¾Œç›¸å®¹çš„æ–¹æ³• ===
    def check_token_valid(self):
        """å‘å¾Œç›¸å®¹æ–¹æ³•"""
        is_successful, error = self.check_connection()
        return is_successful, None, error
    
    def retrieve_thread(self, thread_id: str):
        """å‘å¾Œç›¸å®¹æ–¹æ³•"""
        try:
            endpoint = f'/threads/{thread_id}'
            return self._request('GET', endpoint, assistant=True)
        except Exception as e:
            return False, None, str(e)
    
    def create_thread_message(self, thread_id: str, content: str):
        """å‘å¾Œç›¸å®¹æ–¹æ³•"""
        message = ChatMessage(role='user', content=content)
        is_successful, error = self.add_message_to_thread(thread_id, message)
        return is_successful, None, error
    
    def create_thread_run(self, thread_id: str):
        """å‘å¾Œç›¸å®¹æ–¹æ³•"""
        try:
            endpoint = f'/threads/{thread_id}/runs'
            json_body = {
                'assistant_id': self.assistant_id,
                'temperature': 0
            }
            return self._request('POST', endpoint, body=json_body, assistant=True, operation='assistant_run')
        except Exception as e:
            return False, None, str(e)
    
    def retrieve_thread_run(self, thread_id: str, run_id: str):
        """å‘å¾Œç›¸å®¹æ–¹æ³•"""
        try:
            endpoint = f'/threads/{thread_id}/runs/{run_id}'
            return self._request('GET', endpoint, assistant=True)
        except Exception as e:
            return False, None, str(e)
    
    def list_thread_messages(self, thread_id: str):
        """å‘å¾Œç›¸å®¹æ–¹æ³•"""
        try:
            endpoint = f'/threads/{thread_id}/messages'
            return self._request('GET', endpoint, assistant=True)
        except Exception as e:
            return False, None, str(e)
    
    def audio_transcriptions(self, file_path: str, model: str):
        """å‘å¾Œç›¸å®¹æ–¹æ³•"""
        return self.transcribe_audio(file_path, model=model)
    
    # === å…§éƒ¨æ–¹æ³• ===
    @retry_on_rate_limit(max_retries=3, base_delay=1.0)
    def _request(self, method: str, endpoint: str, body=None, files=None, assistant=False, operation='chat_completion'):
        """ç™¼é€ HTTP è«‹æ±‚ï¼ˆæ™ºæ…§è¶…æ™‚é…ç½®ï¼‰"""
        headers = {
            'Authorization': f'Bearer {self.api_key}'
        }
        
        # æ ¹æ“šæ“ä½œé¡å‹æ±ºå®šè¶…æ™‚æ™‚é–“
        timeout = SmartTimeoutConfig.get_timeout_for_model(operation, 'openai')
        
        try:
            if method == 'GET':
                if assistant:
                    headers['Content-Type'] = 'application/json'
                    headers['OpenAI-Beta'] = 'assistants=v2'
                if 'models' in endpoint:
                    timeout = SmartTimeoutConfig.get_timeout('model_list')
                r = requests.get(f'{self.base_url}{endpoint}', headers=headers, timeout=timeout)
            elif method == 'POST':
                if body:
                    headers['Content-Type'] = 'application/json'
                if assistant:
                    headers['OpenAI-Beta'] = 'assistants=v2'
                if files:  # æª”æ¡ˆä¸Šå‚³
                    timeout = SmartTimeoutConfig.get_timeout('file_upload')
                r = requests.post(f'{self.base_url}{endpoint}', headers=headers, json=body, files=files, timeout=timeout)
            elif method == 'DELETE':
                if assistant:
                    headers['OpenAI-Beta'] = 'assistants=v2'
                r = requests.delete(f'{self.base_url}{endpoint}', headers=headers, timeout=timeout)
            
            # æª¢æŸ¥ HTTP ç‹€æ…‹ç¢¼
            if r.status_code == 429:  # Rate limit
                raise requests.exceptions.RequestException(f"Rate limit exceeded: {r.status_code}")
            elif r.status_code >= 500:  # Server error
                raise requests.exceptions.RequestException(f"Server error: {r.status_code}")
            elif r.status_code >= 400:  # Client error
                try:
                    error_data = r.json()
                    error_msg = error_data.get('error', {}).get('message', f'HTTP {r.status_code}')
                    return False, None, error_msg
                except:
                    return False, None, f'HTTP {r.status_code}: {r.text[:200]}'
            
            response_data = r.json()
            if response_data.get('error'):
                return False, None, response_data.get('error', {}).get('message')
                
            return True, response_data, None
            
        except requests.exceptions.RequestException as e:
            # ç¶²è·¯ç›¸é—œéŒ¯èª¤æœƒè¢«é‡è©¦è£é£¾å™¨è™•ç†
            raise e
        except Exception as e:
            return False, None, f'OpenAI API ç³»çµ±ä¸ç©©å®šï¼Œè«‹ç¨å¾Œå†è©¦: {str(e)}'
    
    def _wait_for_run_completion(self, thread_id: str, run_id: str, max_wait_time: int = None) -> Tuple[bool, Optional[Dict], Optional[str]]:
        """æ™ºæ…§ç­‰å¾…åŸ·è¡Œå®Œæˆ - ä½¿ç”¨ 5sâ†’3sâ†’2sâ†’1sâ†’1s ç­–ç•¥"""
        
        # ä½¿ç”¨æ™ºæ…§è¼ªè©¢ç­–ç•¥
        if max_wait_time:
            self.polling_strategy.max_wait_time = max_wait_time
        
        def check_run_status():
            """æª¢æŸ¥åŸ·è¡Œç‹€æ…‹çš„å›èª¿å‡½æ•¸"""
            is_successful, response, error_message = self.retrieve_thread_run(thread_id, run_id)
            if not is_successful:
                return False, 'error', error_message
            
            status = response['status']
            return True, status, response
        
        # ä½¿ç”¨æ™ºæ…§è¼ªè©¢ç­‰å¾…
        with PollingContext(f"OpenAI Assistant Run {run_id}", self.polling_strategy) as context:
            return context.wait_for_condition(
                check_function=check_run_status,
                completion_statuses=['completed'],
                failure_statuses=['failed', 'expired', 'cancelled', 'requires_action']
            )
    
    def _get_thread_messages(self, thread_id: str) -> Tuple[bool, Optional[ChatResponse], Optional[str]]:
        """å–å¾—å°è©±ä¸²è¨Šæ¯"""
        try:
            is_successful, response, error_message = self.list_thread_messages(thread_id)
            if not is_successful:
                return False, None, error_message
            
            # è¨˜éŒ„å®Œæ•´çš„APIå›æ‡‰ç”¨æ–¼é™¤éŒ¯
            logger.debug(f"OpenAI Assistant API å®Œæ•´å›æ‡‰: {response}")
            # å–å¾—æœ€æ–°çš„åŠ©ç†å›æ‡‰
            for message in response['data']:
                if message['role'] == 'assistant' and message['content']:
                    content = message['content'][0]['text']['value']
                    chat_response = ChatResponse(
                        content=content,
                        metadata={'thread_messages': response}
                    )
                    return True, chat_response, None
            
            return False, None, "No assistant response found"
            
        except Exception as e:
            return False, None, str(e)
    
    # === ğŸ†• æ–°çš„ç”¨æˆ¶ç´šå°è©±ç®¡ç†æ¥å£ ===
    
    def chat_with_user(self, user_id: str, message: str, platform: str = 'line', **kwargs) -> Tuple[bool, Optional[RAGResponse], Optional[str]]:
        """
        ä¸»è¦å°è©±æ¥å£ï¼šä½¿ç”¨ OpenAI Assistant API çš„ thread ç³»çµ±
        
        OpenAI ä½¿ç”¨åŸç”Ÿ thread ç®¡ç†ï¼Œèˆ‡å…¶ä»–æ¨¡å‹çš„ç°¡åŒ–å°è©±æ­·å²ä¸åŒ
        
        Args:
            user_id: ç”¨æˆ¶ ID (å¦‚ Line user ID)
            message: ç”¨æˆ¶è¨Šæ¯
            platform: å¹³å°è­˜åˆ¥ (\'line\', \'discord\', \'telegram\')
            **kwargs: é¡å¤–åƒæ•¸
                
        Returns:
            (is_successful, rag_response, error_message)
        """
        try:
            # 1. å–å¾—æˆ–å‰µå»ºç”¨æˆ¶çš„ thread
            from ..database.connection import get_thread_id_by_user_id, save_thread_id
            
            thread_id = get_thread_id_by_user_id(user_id, platform)
            
            if not thread_id:
                # å‰µå»ºæ–° thread
                is_successful, thread_info, error = self.create_thread()
                if not is_successful:
                    return False, None, f"Failed to create thread: {error}"
                
                thread_id = thread_info.thread_id
                save_thread_id(user_id, thread_id, platform)
                logger.info(f"Created new thread {thread_id} for user {user_id} on platform {platform}")
            
            # 2. æ·»åŠ ç”¨æˆ¶è¨Šæ¯åˆ° thread
            user_message = ChatMessage(role='user', content=message)
            is_successful, error = self.add_message_to_thread(thread_id, user_message)
            if not is_successful:
                return False, None, f"Failed to add message to thread: {error}"
            
            # 3. åŸ·è¡Œ Assistant
            is_successful, chat_response, error = self.run_assistant(thread_id, **kwargs)
            if not is_successful:
                return False, None, error
            
            # 4. è™•ç† OpenAI å›æ‡‰æ ¼å¼ï¼ˆå¼•ç”¨ç­‰ï¼‰
            thread_messages = chat_response.metadata.get('thread_messages', {})
            formatted_content, sources = self._process_openai_response(thread_messages)
            
            # 5. å°‡è™•ç†å¾Œçš„å…§å®¹è½‰æ›ç‚º RAGResponse
            rag_response = RAGResponse(
                answer=formatted_content,
                sources=sources,  # å‚³é sources çµ¦ ResponseFormatter çµ±ä¸€è™•ç†
                metadata={
                    'user_id': user_id,
                    'thread_id': thread_id,
                    'model_provider': 'openai',
                    'uses_native_threads': True,
                    'finish_reason': chat_response.finish_reason,
                    'raw_metadata': chat_response.metadata,
                    'raw_content': chat_response.content
                }
            )
            
            logger.info(f"Completed OpenAI chat with user {user_id}, thread {thread_id}, response length: {len(rag_response.answer) if rag_response else 0}")
            return True, rag_response, None
            
        except Exception as e:
            logger.error(f"Error in chat_with_user for user {user_id}: {e}")
            return False, None, str(e)
    
    def clear_user_history(self, user_id: str, platform: str = 'line') -> Tuple[bool, Optional[str]]:
        """æ¸…é™¤ç”¨æˆ¶å°è©±æ­·å²ï¼ˆåˆªé™¤ OpenAI threadï¼‰"""
        try:
            from ..database.connection import get_thread_id_by_user_id, delete_thread_id
            
            # 1. å–å¾—ç”¨æˆ¶çš„ thread ID
            thread_id = get_thread_id_by_user_id(user_id, platform)
            if not thread_id:
                logger.info(f"No thread found for user {user_id} on platform {platform}")
                return True, None  # æ²’æœ‰ thread ä¹Ÿç®—æˆåŠŸ
            
            # 2. åˆªé™¤ OpenAI thread
            is_successful, error = self.delete_thread(thread_id)
            if not is_successful:
                logger.error(f"Failed to delete OpenAI thread {thread_id}: {error}")
                # ç¹¼çºŒåŸ·è¡Œï¼Œè‡³å°‘æ¸…é™¤æœ¬åœ°è¨˜éŒ„
            
            # 3. åˆªé™¤æœ¬åœ° thread è¨˜éŒ„
            delete_thread_id(user_id, platform)
            
            logger.info(f"Cleared conversation history for user {user_id} on platform {platform}, thread {thread_id}")
            return True, None
            
        except Exception as e:
            logger.error(f"Error clearing history for user {user_id}: {e}")
            return False, str(e)