import requests
from ..core.logger import get_logger
import re
from typing import List, Dict, Tuple, Optional
import time

logger = get_logger(__name__)
from .base import (
    FullLLMInterface, 
    ModelProvider, 
    ChatMessage, 
    ChatResponse, 
    ThreadInfo, 
    FileInfo,
    KnowledgeBase,
    RAGResponse
)
from ..utils.retry import retry_with_backoff, retry_on_rate_limit, CircuitBreaker
from ..utils import s2t_converter, dedup_citation_blocks


class OpenAIModel(FullLLMInterface):
    """OpenAI æ¨¡å‹å¯¦ä½œ"""
    
    def __init__(self, api_key: str, assistant_id: str = None, base_url: str = None):
        self.api_key = api_key
        self.assistant_id = assistant_id
        self.base_url = base_url or 'https://api.openai.com/v1'
    
    def get_provider(self) -> ModelProvider:
        return ModelProvider.OPENAI
    
    def check_connection(self) -> Tuple[bool, Optional[str]]:
        """æª¢æŸ¥ OpenAI API é€£ç·š"""
        try:
            is_successful, response, error_message = self._request('GET', '/models')
            if is_successful:
                return True, None
            else:
                return False, error_message
        except Exception as e:
            return False, str(e)
    
    def chat_completion(self, messages: List[ChatMessage], **kwargs) -> Tuple[bool, Optional[ChatResponse], Optional[str]]:
        """OpenAI Chat Completion"""
        try:
            # è½‰æ›è¨Šæ¯æ ¼å¼
            openai_messages = [
                {"role": msg.role, "content": msg.content} 
                for msg in messages
            ]
            
            json_body = {
                'model': kwargs.get('model', 'gpt-4'),
                'messages': openai_messages,
                'temperature': kwargs.get('temperature', 0)
            }
            
            is_successful, response, error_message = self._request('POST', '/chat/completions', body=json_body)
            
            if not is_successful:
                return False, None, error_message
            
            content = response['choices'][0]['message']['content']
            finish_reason = response['choices'][0].get('finish_reason')
            
            chat_response = ChatResponse(
                content=content,
                finish_reason=finish_reason,
                metadata={'usage': response.get('usage')}
            )
            
            return True, chat_response, None
            
        except Exception as e:
            return False, None, str(e)
    
    def create_thread(self) -> Tuple[bool, Optional[ThreadInfo], Optional[str]]:
        """å»ºç«‹ OpenAI Assistant å°è©±ä¸²"""
        try:
            is_successful, response, error_message = self._request('POST', '/threads', assistant=True)
            
            if not is_successful:
                return False, None, error_message
            
            thread_info = ThreadInfo(
                thread_id=response['id'],
                created_at=response.get('created_at'),
                metadata=response
            )
            
            return True, thread_info, None
            
        except Exception as e:
            return False, None, str(e)
    
    def delete_thread(self, thread_id: str) -> Tuple[bool, Optional[str]]:
        """åˆªé™¤å°è©±ä¸²"""
        try:
            endpoint = f'/threads/{thread_id}'
            is_successful, response, error_message = self._request('DELETE', endpoint, assistant=True)
            return is_successful, error_message
        except Exception as e:
            return False, str(e)
    
    def add_message_to_thread(self, thread_id: str, message: ChatMessage) -> Tuple[bool, Optional[str]]:
        """æ–°å¢è¨Šæ¯åˆ°å°è©±ä¸²"""
        try:
            endpoint = f'/threads/{thread_id}/messages'
            json_body = {
                'role': message.role,
                'content': message.content
            }
            is_successful, response, error_message = self._request('POST', endpoint, body=json_body, assistant=True)
            return is_successful, error_message
        except Exception as e:
            return False, str(e)
    
    def run_assistant(self, thread_id: str, **kwargs) -> Tuple[bool, Optional[ChatResponse], Optional[str]]:
        """åŸ·è¡Œ OpenAI Assistant"""
        try:
            # å•Ÿå‹•åŸ·è¡Œ
            endpoint = f'/threads/{thread_id}/runs'
            json_body = {
                'assistant_id': self.assistant_id,
                'temperature': kwargs.get('temperature', 0)
            }
            
            is_successful, run_response, error_message = self._request('POST', endpoint, body=json_body, assistant=True)
            if not is_successful:
                return False, None, error_message
            
            # ç­‰å¾…å®Œæˆ
            run_id = run_response['id']
            is_successful, final_response, error_message = self._wait_for_run_completion(thread_id, run_id)

            if not is_successful:
                return False, None, f"Assistant run failed: {error_message}"
            
            # å–å¾—å›æ‡‰
            return self._get_thread_messages(thread_id)
            
        except Exception as e:
            return False, None, str(e)
    
    # === RAG ä»‹é¢å¯¦ä½œï¼ˆä½¿ç”¨ OpenAI Assistant APIï¼‰ ===
    
    def upload_knowledge_file(self, file_path: str, **kwargs) -> Tuple[bool, Optional[FileInfo], Optional[str]]:
        """ä¸Šå‚³æª”æ¡ˆåˆ° OpenAIï¼ˆç”¨æ–¼ Assistant APIï¼‰"""
        try:
            with open(file_path, 'rb') as f:
                files = {
                    'file': f,
                    'purpose': (None, 'assistants')
                }
                is_successful, response, error_message = self._request('POST', '/files', files=files)
            
            if not is_successful:
                return False, None, error_message
            
            file_info = FileInfo(
                file_id=response['id'],
                filename=response['filename'],
                size=response.get('bytes'),
                status=response.get('status'),
                purpose=response.get('purpose'),
                metadata=response
            )
            
            return True, file_info, None
            
        except Exception as e:
            return False, None, str(e)
    
    def query_with_rag(self, query: str, thread_id: str = None, **kwargs) -> Tuple[bool, Optional[RAGResponse], Optional[str]]:
        """ä½¿ç”¨ OpenAI Assistant API é€²è¡Œ RAG æŸ¥è©¢"""
        try:
            # å¦‚æœæ²’æœ‰ thread_idï¼Œå»ºç«‹æ–°çš„
            if not thread_id:
                is_successful, thread_info, error_message = self.create_thread()
                if not is_successful:
                    return False, None, error_message
                thread_id = thread_info.thread_id
            
            # æ–°å¢è¨Šæ¯åˆ°å°è©±ä¸²
            message = ChatMessage(role='user', content=query)
            is_successful, error_message = self.add_message_to_thread(thread_id, message)
            if not is_successful:
                return False, None, error_message
            
            # åŸ·è¡ŒåŠ©ç†
            is_successful, chat_response, error_message = self.run_assistant(thread_id, **kwargs)
            if not is_successful:
                return False, None, error_message
            
            # ä½¿ç”¨ OpenAI ç‰¹å®šçš„å¼•ç”¨è™•ç†é‚è¼¯
            thread_messages = chat_response.metadata.get('thread_messages', {})
            formatted_content, sources = self._process_openai_response(thread_messages)
            
            rag_response = RAGResponse(
                answer=formatted_content,
                sources=sources,
                metadata={
                    'thread_id': thread_id,
                    'model': 'openai-assistant',
                    'thread_messages': thread_messages,
                    'raw_content': chat_response.content
                }
            )
            
            return True, rag_response, None
            
        except Exception as e:
            return False, None, str(e)
    
    def get_knowledge_files(self) -> Tuple[bool, Optional[List[FileInfo]], Optional[str]]:
        """å–å¾— OpenAI æª”æ¡ˆåˆ—è¡¨"""
        return self.list_files()
    
    def list_files(self) -> Tuple[bool, Optional[List[FileInfo]], Optional[str]]:
        """åˆ—å‡ºæª”æ¡ˆ"""
        try:
            is_successful, response, error_message = self._request('GET', '/files', assistant=True)
            
            if not is_successful:
                return False, None, error_message
            
            files = [
                FileInfo(
                    file_id=file['id'],
                    filename=file['filename'],
                    size=file.get('bytes'),
                    status=file.get('status'),
                    purpose=file.get('purpose'),
                    metadata=file
                )
                for file in response['data']
            ]
            
            return True, files, None
            
        except Exception as e:
            return False, None, str(e)
    
    def get_file_references(self) -> Dict[str, str]:
        """å–å¾—æª”æ¡ˆå¼•ç”¨å°æ‡‰è¡¨"""
        try:
            is_successful, files, error_message = self.list_files()
            if not is_successful:
                logger.warning(f"Failed to get file references: {error_message}")
                return {}
            
            file_dict = {}
            for file in files:
                filename = file.filename.replace('.txt', '').replace('.json', '')
                file_dict[file.file_id] = filename
            
            logger.debug(f"Loaded {len(file_dict)} file references")
            return file_dict
            
        except Exception as e:
            logger.error(f"Error getting file references: {e}")
            return {}
    
    def _process_openai_response(self, thread_messages: Dict) -> Tuple[str, List[Dict[str, str]]]:
        """
        è™•ç† OpenAI Assistant API çš„å›æ‡‰ï¼ŒåŒ…æ‹¬å¼•ç”¨æ ¼å¼åŒ–
        é€™å€‹æ–¹æ³•å°è£äº†åŸæœ¬çš„ get_content_and_reference é‚è¼¯
        """
        try:
            # å–å¾—åŠ©ç†å›æ‡‰æ•¸æ“š
            data = self._get_response_data(thread_messages)
            logger.debug("OpenAI Assistant response data:")
            logger.debug(data)
            if not data:
                logger.debug("_process_openai_response: æ²’æœ‰æ‰¾åˆ°åŠ©ç†å›æ‡‰æ•¸æ“š")
                return '', []
            text = data['content'][0]['text']['value']
            annotations = data['content'][0]['text']['annotations']
            
            logger.debug(f"_process_openai_response: è¨»è§£æ•¸é‡={len(annotations)}")
            
            # æª¢æŸ¥æ˜¯å¦æœ‰è¤‡é›œå¼•ç”¨æ ¼å¼åœ¨åŸå§‹æ–‡æœ¬ä¸­
            complex_citations = re.findall(r'ã€[^ã€‘]+ã€‘', text)
            if complex_citations:
                logger.debug(f"_process_openai_response: ç™¼ç¾ {len(complex_citations)} å€‹è¤‡é›œå¼•ç”¨æ ¼å¼")
            
            # è½‰æ›ç‚ºç¹é«”ä¸­æ–‡
            text = s2t_converter.convert(text)
            
            # å–å¾—æª”æ¡ˆå­—å…¸ç”¨æ–¼å¼•ç”¨è™•ç†
            file_dict = self.get_file_references()
            
            # æ›¿æ›è¨»é‡‹æ–‡æœ¬å’Œå»ºç«‹ä¾†æºæ¸…å–®
            citation_map: dict[str, int] = {}
            sources: list[dict] = []
            next_num = 1  # ä¸‹ä¸€å€‹å¯ç”¨çš„å¼•ç”¨ç·¨è™Ÿ

            for annotation in annotations:
                original_text = s2t_converter.convert(annotation["text"])
                file_id = annotation["file_citation"]["file_id"]
                filename = file_dict.get(file_id, "Unknown")

                # 2) å–å¾—ï¼ˆæˆ–ç”¢ç”Ÿï¼‰æ­¤æª”æ¡ˆçš„ç·¨è™Ÿ
                if filename in citation_map:
                    ref_num = citation_map[filename]          # å·²ç¶“æœ‰ â†’ ç›´æ¥é‡ç”¨
                else:
                    ref_num = next_num                        # ç¬¬ä¸€æ¬¡çœ‹åˆ° â†’ æŒ‡æ´¾æ–°è™Ÿç¢¼
                    citation_map[filename] = ref_num
                    next_num += 1

                    # åªåœ¨ç¬¬ä¸€æ¬¡çœ‹åˆ°æ™‚ï¼ŒæŠŠå®ƒæ”¾é€² sourcesï¼Œé¿å…é‡è¤‡
                    sources.append({
                        "file_id": file_id,
                        "filename": filename,
                        "quote": annotation["file_citation"].get("quote", ""),
                        "type": "file_citation",
                    })

                # 3) å–ä»£æ­£æ–‡ä¸­çš„å¼•ç”¨æ¨™ç±¤
                replacement_text = f"[{ref_num}]"
                text = text.replace(original_text, replacement_text)
            
            # ç›´æ¥è¿”å›è™•ç†å¾Œçš„æ–‡æœ¬ï¼Œè®“ ResponseFormatter çµ±ä¸€è™•ç† sources
            final_text = dedup_citation_blocks(text.strip())
            
            logger.debug(f"_process_openai_response: æœ€çµ‚æ–‡æœ¬é•·åº¦={len(final_text)}, ç”Ÿæˆäº† {len(sources)} å€‹ä¾†æº")
            
            return final_text, sources
            
        except Exception as e:
            logger.error(f"Error processing OpenAI response: {e}")
            return '', []
    
    def _get_response_data(self, response: Dict) -> Dict:
        """å¾ OpenAI å›æ‡‰ä¸­æå–åŠ©ç†æ•¸æ“š"""
        try:
            for item in response.get('data', []):
                if item.get('role') == 'assistant' and item.get('content') and item['content'][0].get('type') == 'text':
                    return item
            return None
        except Exception as e:
            logger.error(f"Error getting response data: {e}")
            return None
    
    def transcribe_audio(self, audio_file_path: str, **kwargs) -> Tuple[bool, Optional[str], Optional[str]]:
        """éŸ³è¨Šè½‰æ–‡å­—"""
        try:
            # OpenAI æ¨¡å‹é è¨­ä½¿ç”¨ whisper-1
            model = kwargs.get('model', 'whisper-1')
            
            files = {
                'file': open(audio_file_path, 'rb'),
                'model': (None, model),
            }
            is_successful, response, error_message = self._request('POST', '/audio/transcriptions', files=files)
            
            if not is_successful:
                return False, None, error_message
            
            return True, response['text'], None
            
        except Exception as e:
            return False, None, str(e)
    
    def generate_image(self, prompt: str, **kwargs) -> Tuple[bool, Optional[str], Optional[str]]:
        """ç”Ÿæˆåœ–ç‰‡"""
        try:
            json_body = {
                "prompt": prompt,
                "n": kwargs.get('n', 1),
                "size": kwargs.get('size', '512x512')
            }
            is_successful, response, error_message = self._request('POST', '/images/generations', body=json_body)
            
            if not is_successful:
                return False, None, error_message
            
            image_url = response['data'][0]['url']
            return True, image_url, None
            
        except Exception as e:
            return False, None, str(e)
    
    # === å‘å¾Œç›¸å®¹çš„æ–¹æ³• ===
    def check_token_valid(self):
        """å‘å¾Œç›¸å®¹æ–¹æ³•"""
        is_successful, error = self.check_connection()
        return is_successful, None, error
    
    def retrieve_thread(self, thread_id: str):
        """å‘å¾Œç›¸å®¹æ–¹æ³•"""
        try:
            endpoint = f'/threads/{thread_id}'
            return self._request('GET', endpoint, assistant=True)
        except Exception as e:
            return False, None, str(e)
    
    def create_thread_message(self, thread_id: str, content: str):
        """å‘å¾Œç›¸å®¹æ–¹æ³•"""
        message = ChatMessage(role='user', content=content)
        is_successful, error = self.add_message_to_thread(thread_id, message)
        return is_successful, None, error
    
    def create_thread_run(self, thread_id: str):
        """å‘å¾Œç›¸å®¹æ–¹æ³•"""
        try:
            endpoint = f'/threads/{thread_id}/runs'
            json_body = {
                'assistant_id': self.assistant_id,
                'temperature': 0
            }
            return self._request('POST', endpoint, body=json_body, assistant=True)
        except Exception as e:
            return False, None, str(e)
    
    def retrieve_thread_run(self, thread_id: str, run_id: str):
        """å‘å¾Œç›¸å®¹æ–¹æ³•"""
        try:
            endpoint = f'/threads/{thread_id}/runs/{run_id}'
            return self._request('GET', endpoint, assistant=True)
        except Exception as e:
            return False, None, str(e)
    
    def list_thread_messages(self, thread_id: str):
        """å‘å¾Œç›¸å®¹æ–¹æ³•"""
        try:
            endpoint = f'/threads/{thread_id}/messages'
            return self._request('GET', endpoint, assistant=True)
        except Exception as e:
            return False, None, str(e)
    
    def audio_transcriptions(self, file_path: str, model: str):
        """å‘å¾Œç›¸å®¹æ–¹æ³•"""
        return self.transcribe_audio(file_path, model=model)
    
    # === å…§éƒ¨æ–¹æ³• ===
    @retry_on_rate_limit(max_retries=3, base_delay=1.0)
    def _request(self, method: str, endpoint: str, body=None, files=None, assistant=False):
        """ç™¼é€ HTTP è«‹æ±‚ï¼ˆå¸¶é‡è©¦æ©Ÿåˆ¶ï¼‰"""
        headers = {
            'Authorization': f'Bearer {self.api_key}'
        }
        
        try:
            if method == 'GET':
                if assistant:
                    headers['Content-Type'] = 'application/json'
                    headers['OpenAI-Beta'] = 'assistants=v2'
                r = requests.get(f'{self.base_url}{endpoint}', headers=headers, timeout=30)
            elif method == 'POST':
                if body:
                    headers['Content-Type'] = 'application/json'
                if assistant:
                    headers['OpenAI-Beta'] = 'assistants=v2'
                r = requests.post(f'{self.base_url}{endpoint}', headers=headers, json=body, files=files, timeout=30)
            elif method == 'DELETE':
                if assistant:
                    headers['OpenAI-Beta'] = 'assistants=v2'
                r = requests.delete(f'{self.base_url}{endpoint}', headers=headers, timeout=30)
            
            # æª¢æŸ¥ HTTP ç‹€æ…‹ç¢¼
            if r.status_code == 429:  # Rate limit
                raise requests.exceptions.RequestException(f"Rate limit exceeded: {r.status_code}")
            elif r.status_code >= 500:  # Server error
                raise requests.exceptions.RequestException(f"Server error: {r.status_code}")
            elif r.status_code >= 400:  # Client error
                try:
                    error_data = r.json()
                    error_msg = error_data.get('error', {}).get('message', f'HTTP {r.status_code}')
                    return False, None, error_msg
                except:
                    return False, None, f'HTTP {r.status_code}: {r.text[:200]}'
            
            response_data = r.json()
            if response_data.get('error'):
                return False, None, response_data.get('error', {}).get('message')
                
            return True, response_data, None
            
        except requests.exceptions.RequestException as e:
            # ç¶²è·¯ç›¸é—œéŒ¯èª¤æœƒè¢«é‡è©¦è£é£¾å™¨è™•ç†
            raise e
        except Exception as e:
            return False, None, f'OpenAI API ç³»çµ±ä¸ç©©å®šï¼Œè«‹ç¨å¾Œå†è©¦: {str(e)}'
    
    def _wait_for_run_completion(self, thread_id: str, run_id: str, max_wait_time: int = 120) -> Tuple[bool, Optional[Dict], Optional[str]]:
        """ç­‰å¾…åŸ·è¡Œå®Œæˆ"""
        start_time = time.time()
        
        while True:
            if time.time() - start_time > max_wait_time:
                return False, None, "Request timeout"
            
            is_successful, response, error_message = self.retrieve_thread_run(thread_id, run_id)
            if not is_successful:
                return False, None, error_message
            
            status = response['status']
            if status == 'completed':
                return True, response, None
            elif status in ['failed', 'expired', 'cancelled']:
                # å¾å›æ‡‰ä¸­æå–æ›´è©³ç´°çš„éŒ¯èª¤è¨Šæ¯
                error_details = response.get('last_error')
                if error_details:
                    error_message = f"Run failed with status: {status}. Code: {error_details.get('code')}. Message: {error_details.get('message')}"
                else:
                    error_message = f"Run finished with uncompleted status: {status}"
                return False, response, error_message
            
            # æ ¹æ“šç‹€æ…‹èª¿æ•´ç­‰å¾…æ™‚é–“
            if status == 'queued':
                time.sleep(10)
            else:
                time.sleep(3)
    
    def _get_thread_messages(self, thread_id: str) -> Tuple[bool, Optional[ChatResponse], Optional[str]]:
        """å–å¾—å°è©±ä¸²è¨Šæ¯"""
        try:
            is_successful, response, error_message = self.list_thread_messages(thread_id)
            if not is_successful:
                return False, None, error_message
            
            # è¨˜éŒ„å®Œæ•´çš„APIå›æ‡‰ç”¨æ–¼é™¤éŒ¯
            logger.debug(f"OpenAI Assistant API å®Œæ•´å›æ‡‰: {response}")
            # å–å¾—æœ€æ–°çš„åŠ©ç†å›æ‡‰
            for message in response['data']:
                if message['role'] == 'assistant' and message['content']:
                    content = message['content'][0]['text']['value']
                    chat_response = ChatResponse(
                        content=content,
                        metadata={'thread_messages': response}
                    )
                    return True, chat_response, None
            
            return False, None, "No assistant response found"
            
        except Exception as e:
            return False, None, str(e)
    
    # === ğŸ†• æ–°çš„ç”¨æˆ¶ç´šå°è©±ç®¡ç†æ¥å£ ===
    
    def chat_with_user(self, user_id: str, message: str, platform: str = 'line', **kwargs) -> Tuple[bool, Optional[RAGResponse], Optional[str]]:
        """
        ä¸»è¦å°è©±æ¥å£ï¼šä½¿ç”¨ OpenAI Assistant API çš„ thread ç³»çµ±
        
        OpenAI ä½¿ç”¨åŸç”Ÿ thread ç®¡ç†ï¼Œèˆ‡å…¶ä»–æ¨¡å‹çš„ç°¡åŒ–å°è©±æ­·å²ä¸åŒ
        
        Args:
            user_id: ç”¨æˆ¶ ID (å¦‚ Line user ID)
            message: ç”¨æˆ¶è¨Šæ¯
            platform: å¹³å°è­˜åˆ¥ (\'line\', \'discord\', \'telegram\')
            **kwargs: é¡å¤–åƒæ•¸
                
        Returns:
            (is_successful, rag_response, error_message)
        """
        try:
            # 1. å–å¾—æˆ–å‰µå»ºç”¨æˆ¶çš„ thread
            from ..database.connection import get_thread_id_by_user_id, save_thread_id
            
            thread_id = get_thread_id_by_user_id(user_id, platform)
            
            if not thread_id:
                # å‰µå»ºæ–° thread
                is_successful, thread_info, error = self.create_thread()
                if not is_successful:
                    return False, None, f"Failed to create thread: {error}"
                
                thread_id = thread_info.thread_id
                save_thread_id(user_id, thread_id, platform)
                logger.info(f"Created new thread {thread_id} for user {user_id} on platform {platform}")
            
            # 2. æ·»åŠ ç”¨æˆ¶è¨Šæ¯åˆ° thread
            user_message = ChatMessage(role='user', content=message)
            is_successful, error = self.add_message_to_thread(thread_id, user_message)
            if not is_successful:
                return False, None, f"Failed to add message to thread: {error}"
            
            # 3. åŸ·è¡Œ Assistant
            is_successful, chat_response, error = self.run_assistant(thread_id, **kwargs)
            if not is_successful:
                return False, None, error
            
            # 4. è™•ç† OpenAI å›æ‡‰æ ¼å¼ï¼ˆå¼•ç”¨ç­‰ï¼‰
            thread_messages = chat_response.metadata.get('thread_messages', {})
            formatted_content, sources = self._process_openai_response(thread_messages)
            
            # 5. å°‡è™•ç†å¾Œçš„å…§å®¹è½‰æ›ç‚º RAGResponse
            rag_response = RAGResponse(
                answer=formatted_content,
                sources=sources,  # å‚³é sources çµ¦ ResponseFormatter çµ±ä¸€è™•ç†
                metadata={
                    'user_id': user_id,
                    'thread_id': thread_id,
                    'model_provider': 'openai',
                    'uses_native_threads': True,
                    'finish_reason': chat_response.finish_reason,
                    'raw_metadata': chat_response.metadata,
                    'raw_content': chat_response.content
                }
            )
            
            logger.info(f"Completed OpenAI chat with user {user_id}, thread {thread_id}, response length: {len(rag_response.answer) if rag_response else 0}")
            return True, rag_response, None
            
        except Exception as e:
            logger.error(f"Error in chat_with_user for user {user_id}: {e}")
            return False, None, str(e)
    
    def clear_user_history(self, user_id: str, platform: str = 'line') -> Tuple[bool, Optional[str]]:
        """æ¸…é™¤ç”¨æˆ¶å°è©±æ­·å²ï¼ˆåˆªé™¤ OpenAI threadï¼‰"""
        try:
            from ..database.connection import get_thread_id_by_user_id, delete_thread_id
            
            # 1. å–å¾—ç”¨æˆ¶çš„ thread ID
            thread_id = get_thread_id_by_user_id(user_id, platform)
            if not thread_id:
                logger.info(f"No thread found for user {user_id} on platform {platform}")
                return True, None  # æ²’æœ‰ thread ä¹Ÿç®—æˆåŠŸ
            
            # 2. åˆªé™¤ OpenAI thread
            is_successful, error = self.delete_thread(thread_id)
            if not is_successful:
                logger.error(f"Failed to delete OpenAI thread {thread_id}: {error}")
                # ç¹¼çºŒåŸ·è¡Œï¼Œè‡³å°‘æ¸…é™¤æœ¬åœ°è¨˜éŒ„
            
            # 3. åˆªé™¤æœ¬åœ° thread è¨˜éŒ„
            delete_thread_id(user_id, platform)
            
            logger.info(f"Cleared conversation history for user {user_id} on platform {platform}, thread {thread_id}")
            return True, None
            
        except Exception as e:
            logger.error(f"Error clearing history for user {user_id}: {e}")
            return False, str(e)