import requests
import json
import hashlib
import time
import uuid
from ..core.logger import get_logger
from typing import List, Dict, Tuple, Optional
from .base import (
    FullLLMInterface, 
    ModelProvider, 
    ChatMessage, 
    ChatResponse, 
    ThreadInfo, 
    FileInfo,
    KnowledgeBase,
    RAGResponse
)
from ..utils.retry import retry_on_rate_limit
from ..services.conversation import get_conversation_manager

logger = get_logger(__name__)


class AnthropicModel(FullLLMInterface):
    """
    Anthropic Claude 2024 æ¨¡å‹å¯¦ä½œ
    
    ç‰¹è‰²åŠŸèƒ½ï¼š
    - Files API: æŒä¹…åŒ–æ–‡ä»¶ç®¡ç†ï¼Œæ”¯æ´è·¨å°è©±å¼•ç”¨
    - Extended Prompt Caching: æœ€é•·1å°æ™‚çš„æç¤ºå¿«å–ï¼Œé™ä½æˆæœ¬
    - å¢å¼·çš„ RAG å¯¦ä½œï¼šä½¿ç”¨ Files API ä½œç‚ºçŸ¥è­˜åº«
    - çµæ§‹åŒ–ç³»çµ±æç¤ºè©
    """
    
    def __init__(self, api_key: str, model_name: str = "claude-3-5-sonnet-20241022", base_url: str = None):
        self.api_key = api_key
        self.model_name = model_name
        self.base_url = base_url or "https://api.anthropic.com/v1"
        
        # Files API æ”¯æ´ - ä½¿ç”¨æœ‰ç•Œå¿«å–
        from ..core.bounded_cache import FileCache, ConversationCache
        self.files_store = FileCache(max_files=300, file_ttl=3600)  # 300å€‹æª”æ¡ˆï¼Œ1å°æ™‚TTL
        self.file_store = FileCache(max_files=300, file_ttl=3600)   # æª”æ¡ˆåç¨±å°æ‡‰
        
        # Extended Prompt Caching æ”¯æ´ - ä½¿ç”¨å°è©±å¿«å–
        self.cache_enabled = True
        self.cache_ttl = 3600  # 1å°æ™‚
        self.cached_conversations = ConversationCache(max_conversations=1000, conversation_ttl=3600)  # 1000å°è©±ï¼Œ1å°æ™‚TTL
        
        # ç³»çµ±æç¤ºè©é…ç½®
        self.system_prompt = self._build_system_prompt()
        
        # ç¬¬ä¸‰æ–¹èªéŸ³è½‰éŒ„æœå‹™é…ç½®ï¼ˆé è¨­ç‚ºç©ºï¼Œéœ€è¦é¡å¤–é…ç½®ï¼‰
        self.speech_service = None  # å¯æ•´åˆ Deepgram æˆ– AssemblyAI
        
        # å°è©±æ­·å²ç®¡ç†
        self.conversation_manager = get_conversation_manager()
    
    def get_provider(self) -> ModelProvider:
        return ModelProvider.ANTHROPIC
    
    def check_connection(self) -> Tuple[bool, Optional[str]]:
        """æª¢æŸ¥ Anthropic API é€£ç·š"""
        try:
            test_message = [ChatMessage(role="user", content="Hello")]
            is_successful, response, error = self.chat_completion(test_message, max_tokens=10)
            return is_successful, error
        except Exception as e:
            return False, str(e)
    
    @retry_on_rate_limit(max_retries=3, base_delay=1.0)
    def chat_completion(self, messages: List[ChatMessage], **kwargs) -> Tuple[bool, Optional[ChatResponse], Optional[str]]:
        """Anthropic Claude Chat Completion with Extended Prompt Caching"""
        try:
            # è½‰æ›è¨Šæ¯æ ¼å¼
            claude_messages = []
            system_message = kwargs.get('system', self.system_prompt)
            
            for msg in messages:
                if msg.role == "system":
                    system_message = msg.content
                else:
                    claude_messages.append({
                        "role": msg.role, 
                        "content": msg.content
                    })
            
            json_body = {
                "model": kwargs.get('model', self.model_name),
                "max_tokens": kwargs.get('max_tokens', 4000),
                "messages": claude_messages,
                "temperature": kwargs.get('temperature', 0.1)
            }
            
            # ç³»çµ±æç¤ºè©è™•ç†
            if system_message:
                if self.cache_enabled and len(system_message) > 1000:  # é•·æç¤ºè©å•Ÿç”¨å¿«å–
                    json_body["system"] = [
                        {
                            "type": "text",
                            "text": system_message,
                            "cache_control": {"type": "ephemeral"}
                        }
                    ]
                else:
                    json_body["system"] = system_message
            
            # é¡å¤–åƒæ•¸
            if kwargs.get('stop_sequences'):
                json_body["stop_sequences"] = kwargs['stop_sequences']
            
            is_successful, response, error_message = self._request('POST', '/messages', body=json_body)
            
            if not is_successful:
                return False, None, error_message
            
            content = response['content'][0]['text']
            finish_reason = response.get('stop_reason')
            
            chat_response = ChatResponse(
                content=content,
                finish_reason=finish_reason,
                metadata={
                    'usage': response.get('usage'),
                    'model': response.get('model'),
                    'cache_creation_input_tokens': response.get('usage', {}).get('cache_creation_input_tokens', 0),
                    'cache_read_input_tokens': response.get('usage', {}).get('cache_read_input_tokens', 0)
                }
            )
            
            return True, chat_response, None
            
        except Exception as e:
            return False, None, str(e)
    
    # === RAG ä»‹é¢å¯¦ä½œï¼ˆä½¿ç”¨è‡ªå»ºå‘é‡æœå°‹ï¼‰ ===
    
    def upload_knowledge_file(self, file_path: str, **kwargs) -> Tuple[bool, Optional[FileInfo], Optional[str]]:
        """ä½¿ç”¨ Claude Files API ä¸Šå‚³æª”æ¡ˆåˆ°çŸ¥è­˜åº«"""
        try:
            import os
            import mimetypes
            
            filename = os.path.basename(file_path)
            
            # æª¢æŸ¥æª”æ¡ˆå¤§å°ï¼ˆClaude Files API é™åˆ¶ï¼‰
            file_size = os.path.getsize(file_path)
            if file_size > 100 * 1024 * 1024:  # 100MB é™åˆ¶
                return False, None, f"æª”æ¡ˆéå¤§: {file_size / 1024 / 1024:.1f}MBï¼Œè¶…é 100MB é™åˆ¶"
            
            # æº–å‚™æª”æ¡ˆä¸Šå‚³
            with open(file_path, 'rb') as f:
                file_content = f.read()
            
            # åµæ¸¬æª”æ¡ˆé¡å‹
            content_type, _ = mimetypes.guess_type(file_path)
            if not content_type:
                content_type = 'text/plain'
            
            # ä½¿ç”¨ Files API ä¸Šå‚³
            files = {
                'file': (filename, file_content, content_type)
            }
            
            data = {
                'purpose': kwargs.get('purpose', 'knowledge_base')
            }
            
            is_successful, response, error_message = self._request(
                'POST', '/files', files=files, data=data
            )
            
            if not is_successful:
                return False, None, error_message
            
            file_id = response['id']
            
            # å¿«å–æª”æ¡ˆè³‡è¨Š
            file_info = FileInfo(
                file_id=file_id,
                filename=filename,
                size=file_size,
                status='processed',
                purpose=response.get('purpose', 'knowledge_base'),
                metadata={
                    'upload_time': time.time(),
                    'content_type': content_type,
                    'claude_file_id': file_id
                }
            )
            
            self.files_store[file_id] = file_info
            self.file_store[file_id] = filename
            
            return True, file_info, None
            
        except Exception as e:
            return False, None, str(e)
    
    def query_with_rag(self, query: str, thread_id: str = None, **kwargs) -> Tuple[bool, Optional[RAGResponse], Optional[str]]:
        """ä½¿ç”¨ Files API + Extended Caching å¯¦ä½œ RAG"""
        try:
            # æª¢æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„çŸ¥è­˜æª”æ¡ˆ
            if not self.files_store:
                # æ²’æœ‰çŸ¥è­˜æª”æ¡ˆï¼Œä½¿ç”¨ä¸€èˆ¬èŠå¤©
                messages = [ChatMessage(role="user", content=query)]
                is_successful, response, error = self.chat_completion(messages, **kwargs)
                
                if not is_successful:
                    return False, None, error
                
                rag_response = RAGResponse(
                    answer=response.content,
                    sources=[],
                    metadata={'model': 'anthropic-claude', 'no_sources': True}
                )
                return True, rag_response, None
            
            # å»ºç«‹åŒ…å«æª”æ¡ˆå¼•ç”¨çš„æç¤º
            files_context = self._build_files_context()
            
            enhanced_query = f"""ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„çŸ¥è­˜åŠ©ç†ã€‚è«‹æ ¹æ“šæä¾›çš„æ–‡æª”å…§å®¹å›ç­”å•é¡Œã€‚å¦‚æœä½ å¼•ç”¨æ–‡æª”å…§å®¹ï¼Œè«‹ä½¿ç”¨ [filename] æ ¼å¼æ¨™è¨»ä¾†æºã€‚

å¯ç”¨æ–‡æª”ï¼š
{files_context}

ç”¨æˆ¶å•é¡Œï¼š{query}

è«‹åŸºæ–¼ä¸Šè¿°æ–‡æª”å…§å®¹æä¾›ç²¾ç¢ºçš„å›ç­”ï¼Œå¦‚æœæ–‡æª”ä¸­æ²’æœ‰ç›¸é—œè³‡è¨Šï¼Œè«‹æ˜ç¢ºèªªæ˜ã€‚"""
            
            # ä½¿ç”¨ç³»çµ±æç¤ºè©å’Œå¿«å–
            messages = [ChatMessage(role="user", content=enhanced_query)]
            
            # å°‡æª”æ¡ˆä¸Šä¸‹æ–‡ä½œç‚ºç³»çµ±æç¤ºè©çš„ä¸€éƒ¨åˆ†ï¼Œä»¥å……åˆ†åˆ©ç”¨ Extended Caching
            system_with_files = f"{self.system_prompt}\n\nçŸ¥è­˜åº«ä¸Šä¸‹æ–‡ï¼š\n{files_context}"
            
            is_successful, response, error = self.chat_completion(
                messages, 
                system=system_with_files,
                **kwargs
            )
            
            if not is_successful:
                return False, None, error
            
            # æå–ä¾†æºè³‡è¨Š
            sources = self._extract_sources_from_response(response.content)
            
            rag_response = RAGResponse(
                answer=response.content,
                sources=sources,
                metadata={
                    'model': 'anthropic-claude',
                    'files_used': len(self.files_store),
                    'num_sources': len(sources),
                    'cache_enabled': self.cache_enabled,
                    'cache_tokens': response.metadata.get('cache_read_input_tokens', 0)
                }
            )
            
            return True, rag_response, None
            
        except Exception as e:
            return False, None, str(e)
    
    def get_knowledge_files(self) -> Tuple[bool, Optional[List[FileInfo]], Optional[str]]:
        """å–å¾— Files API æª”æ¡ˆåˆ—è¡¨"""
        try:
            # å…ˆå¾å¿«å–è¿”å›
            if self.files_store:
                return True, list(self.files_store.values()), None
            
            # å¦‚æœå¿«å–ç‚ºç©ºï¼Œå¾ API ç²å–
            is_successful, response, error_message = self._request('GET', '/files')
            
            if not is_successful:
                return False, None, error_message
            
            files = []
            for file_data in response.get('data', []):
                file_info = FileInfo(
                    file_id=file_data['id'],
                    filename=file_data['filename'],
                    size=file_data.get('bytes', 0),
                    status='processed',
                    purpose=file_data.get('purpose', 'knowledge_base'),
                    metadata={
                        'upload_time': file_data.get('created_at'),
                        'claude_file_id': file_data['id']
                    }
                )
                files.append(file_info)
                
                # æ›´æ–°å¿«å–
                self.files_store[file_data['id']] = file_info
                self.file_store[file_data['id']] = file_data['filename']
            
            return True, files, None
            
        except Exception as e:
            return False, None, str(e)
    
    def get_file_references(self) -> Dict[str, str]:
        """å–å¾—æª”æ¡ˆå¼•ç”¨å°æ‡‰è¡¨"""
        return {
            file_id: filename.replace('.txt', '').replace('.json', '')
            for file_id, filename in self.file_store.items()
        }
    
    
    @retry_on_rate_limit(max_retries=3, base_delay=1.0)
    def _request(self, method: str, endpoint: str, body=None, files=None, data=None):
        """ç™¼é€ HTTP è«‹æ±‚åˆ° Anthropic API"""
        headers = {
            'x-api-key': self.api_key,
            'anthropic-version': '2023-06-01'
        }
        
        # æª”æ¡ˆä¸Šå‚³è«‹æ±‚ä¸éœ€è¦ Content-Type
        if not files:
            headers['Content-Type'] = 'application/json'
        
        try:
            if method == 'POST':
                if files:
                    # æª”æ¡ˆä¸Šå‚³
                    r = requests.post(
                        f'{self.base_url}{endpoint}', 
                        headers=headers, 
                        files=files, 
                        data=data, 
                        timeout=60
                    )
                else:
                    # JSON è«‹æ±‚
                    r = requests.post(
                        f'{self.base_url}{endpoint}', 
                        headers=headers, 
                        json=body, 
                        timeout=30
                    )
            else:
                r = requests.get(f'{self.base_url}{endpoint}', headers=headers, timeout=30)
            
            # æª¢æŸ¥ HTTP ç‹€æ…‹ç¢¼
            if r.status_code == 429:
                raise requests.exceptions.RequestException(f"Rate limit exceeded: {r.status_code}")
            elif r.status_code >= 500:
                raise requests.exceptions.RequestException(f"Server error: {r.status_code}")
            elif r.status_code >= 400:
                try:
                    error_data = r.json()
                    error_msg = error_data.get('error', {}).get('message', f'HTTP {r.status_code}')
                    return False, None, error_msg
                except:
                    return False, None, f'HTTP {r.status_code}: {r.text[:200]}'
            
            response_data = r.json()
            return True, response_data, None
            
        except requests.exceptions.RequestException as e:
            raise e
        except Exception as e:
            return False, None, f'Anthropic API ç³»çµ±ä¸ç©©å®šï¼Œè«‹ç¨å¾Œå†è©¦: {str(e)}'
    
    # === å…¶ä»–ä»‹é¢ï¼ˆæš«æœªå¯¦ä½œï¼‰ ===
    
    def create_thread(self) -> Tuple[bool, Optional[ThreadInfo], Optional[str]]:
        """å»ºç«‹å°è©±ä¸² ID ä¸¦åˆå§‹åŒ–å¿«å–"""
        thread_id = str(uuid.uuid4())
        
        # åˆå§‹åŒ–å°è©±å¿«å–
        if self.cache_enabled:
            self.cached_conversations[thread_id] = {
                'created_at': time.time(),
                'messages': [],
                'system_context': self.system_prompt
            }
        
        thread_info = ThreadInfo(thread_id=thread_id, metadata={'cache_enabled': self.cache_enabled})
        return True, thread_info, None
    
    def delete_thread(self, thread_id: str) -> Tuple[bool, Optional[str]]:
        """åˆªé™¤å°è©±ä¸²åŠå…¶å¿«å–"""
        if thread_id in self.cached_conversations:
            del self.cached_conversations[thread_id]
        return True, None
    
    def add_message_to_thread(self, thread_id: str, message: ChatMessage) -> Tuple[bool, Optional[str]]:
        """æ–°å¢è¨Šæ¯åˆ°å°è©±ä¸²å¿«å–"""
        if self.cache_enabled and thread_id in self.cached_conversations:
            self.cached_conversations[thread_id]['messages'].append({
                'role': message.role,
                'content': message.content,
                'timestamp': time.time()
            })
        return True, None
    
    def run_assistant(self, thread_id: str, **kwargs) -> Tuple[bool, Optional[RAGResponse], Optional[str]]:
        """åŸ·è¡ŒåŠ©ç†åŠŸèƒ½ä½¿ç”¨å¿«å–çš„å°è©±ä¸Šä¸‹æ–‡"""
        try:
            if not self.cache_enabled or thread_id not in self.cached_conversations:
                return False, None, "ç„¡æ•ˆçš„å°è©±ä¸² ID æˆ–æœªå•Ÿç”¨å¿«å–"
            
            conversation = self.cached_conversations[thread_id]
            
            # æª¢æŸ¥æ˜¯å¦æœ‰æœ€æ–°è¨Šæ¯
            if not conversation['messages']:
                return False, None, "å°è©±ä¸²ä¸­ç„¡è¨Šæ¯"
            
            # å–å¾—æœ€æ–°çš„ç”¨æˆ¶è¨Šæ¯
            last_message = conversation['messages'][-1]
            if last_message['role'] != 'user':
                return False, None, "æœ€æ–°è¨Šæ¯ä¸æ˜¯ç”¨æˆ¶è¨Šæ¯"
            
            # ä½¿ç”¨ RAG è™•ç†æŸ¥è©¢
            return self.query_with_rag(last_message['content'], thread_id, **kwargs)
            
        except Exception as e:
            return False, None, str(e)
    
    def transcribe_audio(self, audio_file_path: str, **kwargs) -> Tuple[bool, Optional[str], Optional[str]]:
        """ä½¿ç”¨ç¬¬ä¸‰æ–¹æœå‹™é€²è¡ŒéŸ³è¨Šè½‰éŒ„"""
        try:
            # å¦‚æœæœ‰é…ç½®ç¬¬ä¸‰æ–¹èªéŸ³æœå‹™ï¼Œä½¿ç”¨å®ƒ
            if self.speech_service:
                return self.speech_service.transcribe(audio_file_path, **kwargs)
            
            # å¦å‰‡å˜—è©¦ä½¿ç”¨ Deepgramï¼ˆéœ€è¦é¡å¤–å®‰è£å’Œé…ç½®ï¼‰
            try:
                return self._transcribe_with_deepgram(audio_file_path, **kwargs)
            except ImportError:
                pass
            
            # æœ€å¾Œå˜—è©¦ AssemblyAI
            try:
                return self._transcribe_with_assemblyai(audio_file_path, **kwargs)
            except ImportError:
                pass
            
            return False, None, "Anthropic ä¸æ”¯æ´éŸ³è¨Šè½‰éŒ„ï¼Œè«‹é…ç½®ç¬¬ä¸‰æ–¹èªéŸ³æœå‹™ï¼ˆDeepgram æˆ– AssemblyAIï¼‰"
            
        except Exception as e:
            return False, None, str(e)
    
    def generate_image(self, prompt: str, **kwargs) -> Tuple[bool, Optional[str], Optional[str]]:
        """åœ–ç‰‡ç”Ÿæˆï¼ˆAnthropic ä¸æ”¯æ´ï¼‰"""
        return False, None, "Anthropic Claude ä¸æ”¯æ´åœ–ç‰‡ç”Ÿæˆï¼Œè«‹ä½¿ç”¨å…¶ä»–æ”¯æ´åœ–ç‰‡ç”Ÿæˆçš„æ¨¡å‹"
    
    def _build_system_prompt(self) -> str:
        """å»ºç«‹çµæ§‹åŒ–ç³»çµ±æç¤ºè©"""
        return """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„çŸ¥è­˜åŠ©ç†ï¼Œå…·æœ‰ä»¥ä¸‹èƒ½åŠ›å’Œç‰¹è³ªï¼š

## èƒ½åŠ›ç¯„åœ
- åŸºæ–¼æä¾›çš„æ–‡æª”å…§å®¹é€²è¡Œç²¾ç¢ºåˆ†æå’Œå›ç­”
- æä¾›çµæ§‹åŒ–ã€é©åˆç¹¼çºŒå°è©±çš„å›ç­”
- åœ¨ç„¡ç›¸é—œè³‡æ–™æ™‚æ˜ç¢ºèªªæ˜é™åˆ¶

## å›ç­”åŸå‰‡
1. ç¸½æ˜¯å„ªå…ˆä½¿ç”¨æä¾›çš„æ–‡æª”å…§å®¹
2. ç•¶å¼•ç”¨æ–‡æª”æ™‚ï¼Œä½¿ç”¨ [filename] æ ¼å¼æ¨™è¨»ä¾†æº
3. å¦‚æ–‡æª”ä¸­ç„¡ç›¸é—œè³‡è¨Šï¼Œæ˜ç¢ºèªªæ˜ä¸¦æä¾›ä¸€èˆ¬æ€§å»ºè­°
4. ä¿æŒå°ˆæ¥­ä½†å‹å–„çš„èªèª¿

## å›ç­”æ ¼å¼
- ä½¿ç”¨æ¸…æ™°çš„æ®µè½çµæ§‹
- é‡è¦è³‡è¨Šä½¿ç”¨æ¢åˆ—æˆ–ç·¨è™Ÿ
- åœ¨å›ç­”æœ«å°¾æ¨™è¨»ç›¸é—œæ–‡æª”ä¾†æº

è«‹å§‹çµ‚ä¿æŒé€™å€‹è§’è‰²è¨­å®šï¼Œæä¾›æœ€é«˜å“è³ªçš„çŸ¥è­˜æœå‹™ã€‚"""
    
    def _build_files_context(self) -> str:
        """å»ºç«‹æª”æ¡ˆä¸Šä¸‹æ–‡ä¿¡æ¯"""
        if not self.files_store:
            return "ç„¡å¯ç”¨æ–‡æª”"
        
        context_parts = []
        for file_info in self.files_store.values():
            context_parts.append(f"- {file_info.filename} (ID: {file_info.file_id})")
        
        return "\n".join(context_parts)
    
    def _extract_sources_from_response(self, response_text: str) -> List[Dict[str, str]]:
        """å¾å›ç­”ä¸­æå–ä¾†æºè³‡è¨Š"""
        import re
        sources = []
        
        # æœå°‹ [filename] æ ¼å¼çš„å¼•ç”¨
        pattern = r'\[([^\]]+)\]'
        matches = re.findall(pattern, response_text)
        
        for match in matches:
            # åœ¨æª”æ¡ˆåˆ—è¡¨ä¸­å°‹æ‰¾åŒ¹é…çš„æª”æ¡ˆ
            for file_id, filename in self.file_store.items():
                if match.lower() in filename.lower() or filename.lower() in match.lower():
                    sources.append({
                        'file_id': file_id,
                        'filename': filename,
                        'text': f"å¼•ç”¨æ–‡æª”: {filename}",
                        'citation': match
                    })
                    break
        
        return sources
    
    def _transcribe_with_deepgram(self, audio_file_path: str, **kwargs) -> Tuple[bool, Optional[str], Optional[str]]:
        """ä½¿ç”¨ Deepgram é€²è¡ŒèªéŸ³è½‰éŒ„"""
        try:
            from deepgram import Deepgram
            
            # éœ€è¦é…ç½®APIé‡‘é‘°
            if not hasattr(self, 'deepgram_api_key') or not self.deepgram_api_key:
                return False, None, "æœªé…ç½® Deepgram API é‡‘é‘°"
            
            dg_client = Deepgram(self.deepgram_api_key)
            
            with open(audio_file_path, 'rb') as audio:
                source = {'buffer': audio, 'mimetype': 'audio/wav'}
                options = {
                    'punctuate': True,
                    'language': kwargs.get('language', 'zh-CN'),
                    'model': 'nova-2'
                }
                
                response = dg_client.transcription.prerecorded(source, options)
                
                if response['results']['channels'][0]['alternatives']:
                    transcript = response['results']['channels'][0]['alternatives'][0]['transcript']
                    return True, transcript, None
                else:
                    return False, None, "ç„¡æ³•è½‰éŒ„éŸ³è¨Š"
                    
        except ImportError:
            raise ImportError("Deepgram SDK æœªå®‰è£")
        except Exception as e:
            return False, None, str(e)
    
    def _transcribe_with_assemblyai(self, audio_file_path: str, **kwargs) -> Tuple[bool, Optional[str], Optional[str]]:
        """ä½¿ç”¨ AssemblyAI é€²è¡ŒèªéŸ³è½‰éŒ„"""
        try:
            import assemblyai as aai
            
            # éœ€è¦é…ç½®APIé‡‘é‘°
            if not hasattr(self, 'assemblyai_api_key') or not self.assemblyai_api_key:
                return False, None, "æœªé…ç½® AssemblyAI API é‡‘é‘°"
            
            aai.settings.api_key = self.assemblyai_api_key
            
            transcriber = aai.Transcriber()
            transcript = transcriber.transcribe(audio_file_path)
            
            if transcript.status == aai.TranscriptStatus.completed:
                return True, transcript.text, None
            else:
                return False, None, f"è½‰éŒ„å¤±æ•—: {transcript.error}"
                
        except ImportError:
            raise ImportError("AssemblyAI SDK æœªå®‰è£")
        except Exception as e:
            return False, None, str(e)
    
    def set_speech_service(self, service_type: str, api_key: str):
        """é…ç½®ç¬¬ä¸‰æ–¹èªéŸ³æœå‹™"""
        if service_type.lower() == 'deepgram':
            self.deepgram_api_key = api_key
        elif service_type.lower() == 'assemblyai':
            self.assemblyai_api_key = api_key
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„èªéŸ³æœå‹™: {service_type}")
    
    # === ğŸ†• æ–°çš„ç”¨æˆ¶ç´šå°è©±ç®¡ç†æ¥å£ ===
    
    def chat_with_user(self, user_id: str, message: str, platform: str = 'line', **kwargs) -> Tuple[bool, Optional[RAGResponse], Optional[str]]:
        """
        ä¸»è¦å°è©±æ¥å£ï¼šç°¡å–®å°è©±æ­·å² + Files API RAG
        
        Args:
            user_id: ç”¨æˆ¶ ID (å¦‚ Line user ID)
            message: ç”¨æˆ¶è¨Šæ¯
            platform: å¹³å°è­˜åˆ¥ ('line', 'discord', 'telegram')
            **kwargs: é¡å¤–åƒæ•¸
            
        Returns:
            (is_successful, rag_response, error_message)
        """
        try:
            # 1. å–å¾—æœ€è¿‘çš„å°è©±æ­·å²
            recent_conversations = self._get_recent_conversations(user_id, platform, limit=kwargs.get('conversation_limit', 5))
            
            # 2. å„²å­˜ç”¨æˆ¶è¨Šæ¯
            self.conversation_manager.add_message(user_id, 'anthropic', 'user', message, platform)
            
            # 3. å»ºç«‹åŒ…å«å°è©±æ­·å²çš„ä¸Šä¸‹æ–‡
            messages = self._build_conversation_context(recent_conversations, message)
            
            # 4. ä½¿ç”¨ Files API é€²è¡Œ RAG æŸ¥è©¢
            is_successful, rag_response, error = self.query_with_rag(message, **kwargs)
            
            if not is_successful:
                return False, None, error
            
            # 5. å„²å­˜åŠ©ç†å›æ‡‰
            self.conversation_manager.add_message(user_id, 'anthropic', 'assistant', rag_response.answer, platform)
            
            # 6. æ›´æ–° metadata
            rag_response.metadata.update({
                'conversation_turns': len(recent_conversations),
                'user_id': user_id,
                'model_provider': 'anthropic'
            })
            
            logger.info(f"Completed chat with user {user_id}, response length: {len(rag_response.answer)}")
            return True, rag_response, None
            
        except Exception as e:
            logger.error(f"Error in chat_with_user for user {user_id}: {e}")
            return False, None, str(e)
    
    def clear_user_history(self, user_id: str, platform: str = 'line') -> Tuple[bool, Optional[str]]:
        """æ¸…é™¤ç”¨æˆ¶å°è©±æ­·å²"""
        try:
            success = self.conversation_manager.clear_user_history(user_id, 'anthropic', platform)
            if success:
                logger.info(f"Cleared conversation history for user {user_id}")
                return True, None
            else:
                return False, "Failed to clear conversation history"
        except Exception as e:
            logger.error(f"Error clearing history for user {user_id}: {e}")
            return False, str(e)
    
    def _get_recent_conversations(self, user_id: str, platform: str = 'line', limit: int = 5) -> List[Dict]:
        """å–å¾—ç”¨æˆ¶æœ€è¿‘çš„å°è©±æ­·å²"""
        try:
            return self.conversation_manager.get_recent_conversations(user_id, 'anthropic', limit, platform)
        except Exception as e:
            logger.warning(f"Failed to get recent conversations for user {user_id}: {e}")
            return []
    
    def _build_conversation_context(self, recent_conversations: List[Dict], current_message: str) -> List[ChatMessage]:
        """å»ºç«‹åŒ…å«å°è©±æ­·å²çš„ä¸Šä¸‹æ–‡"""
        messages = []
        
        # æ·»åŠ å°è©±æ­·å²
        for conv in recent_conversations[-8:]:  # æœ€å¤šå–æœ€è¿‘ 8 è¼ªå°è©±
            messages.append(ChatMessage(
                role=conv['role'],
                content=conv['content']
            ))
        
        # æ·»åŠ ç•¶å‰è¨Šæ¯
        messages.append(ChatMessage(role='user', content=current_message))
        
        return messages