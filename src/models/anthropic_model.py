"""
Anthropic Claude Model å¯¦ä½œ
ä½¿ç”¨ Anthropic Messages API æä¾›èŠå¤©åŠŸèƒ½

ðŸ“‹ æž¶æ§‹è·è²¬åˆ†å·¥ï¼š
âœ… RESPONSIBILITIES (æ¨¡åž‹å±¤è·è²¬):
  - å¯¦ä½œçµ±ä¸€çš„ FullLLMInterface æŽ¥å£
  - æä¾› chat_with_user() æ–‡å­—å°è©±åŠŸèƒ½
  - æä¾› transcribe_audio() éŸ³è¨Šè½‰éŒ„åŠŸèƒ½ (é€éŽå¤–éƒ¨æœå‹™)
  - ç®¡ç†å°è©±æ­·å²å’Œä¸Šä¸‹æ–‡
  - è™•ç† Anthropic API é™æµå’Œé‡è©¦é‚è¼¯

âŒ NEVER DO (çµ•å°ç¦æ­¢):
  - çŸ¥é“è¨Šæ¯ä¾†æºå¹³å° (LINEã€Telegram ç­‰)
  - è™•ç†å¹³å°ç‰¹å®šçš„è¨Šæ¯æ ¼å¼
  - ç›´æŽ¥è™•ç† webhook æˆ–ç¶²è·¯è«‹æ±‚
  - è·¯ç”±è¨Šæ¯æˆ–å”èª¿æœå‹™

ðŸ”„ çµ±ä¸€æŽ¥å£ï¼š
  - chat_with_user(user_id, message, platform) -> (bool, str, str)
  - transcribe_audio(file_path) -> (bool, str, str)
  - clear_user_history(user_id, platform) -> (bool, str)
  - check_connection() -> (bool, str)

ðŸŽ¯ æ¨¡åž‹ç‰¹è‰²ï¼š
  - ä½¿ç”¨ Claude çš„ Messages API
  - æ”¯æ´é•·å°è©±å’Œè¤‡é›œæŽ¨ç†
  - å„ªç§€çš„ç¨‹å¼ç¢¼å’Œæ–‡å­—ç”Ÿæˆèƒ½åŠ›
  - å°è©±æ­·å²å„²å­˜åœ¨è³‡æ–™åº«

âš ï¸ åŠŸèƒ½é™åˆ¶ï¼š
  - éŸ³è¨Šè½‰éŒ„: éœ€é…ç½®å¤–éƒ¨æœå‹™ (Deepgram/AssemblyAI)
  - åœ–ç‰‡ç”Ÿæˆ: ä¸æ”¯æ´ (è¿”å›ž "Anthropic does not support image generation")
"""

import requests
import json
import time
import uuid
import re
from ..core.logger import get_logger
from typing import List, Dict, Tuple, Optional, Any
from .base import (
    FullLLMInterface, 
    ModelProvider, 
    ChatMessage, 
    ChatResponse, 
    ThreadInfo, 
    FileInfo,
    RAGResponse
)
from ..utils.retry import retry_on_rate_limit
from ..services.conversation import get_conversation_manager
from ..core.bounded_cache import FileCache

logger = get_logger(__name__)

class AnthropicModel(FullLLMInterface):
    """
    Anthropic Claude 2024 æ¨¡åž‹å¯¦ä½œ
    """
    
    def __init__(self, api_key: str, model_name: str = "claude-3-5-sonnet-20240620", base_url: str = None, enable_mcp: bool = False):
        self.api_key = api_key
        self.model_name = model_name
        self.base_url = base_url or "https://api.anthropic.com/v1"
        self.file_cache = FileCache(max_files=300, file_ttl=3600)
        self.speech_service = None
        self.conversation_manager = get_conversation_manager()
        
        # MCP æ”¯æ´ - é è¨­é—œé–‰ï¼Œå¯é€éŽåƒæ•¸æˆ–è¨­å®šæª”å•Ÿç”¨
        if enable_mcp:
            self.enable_mcp = True
        else:
            # å¦‚æžœæ˜Žç¢ºå‚³éž Falseï¼Œå‰‡ç›´æŽ¥é—œé–‰ï¼Œå¦å‰‡æª¢æŸ¥è¨­å®šæª”
            try:
                from ..core.config import get_value
                feature_enabled = get_value('features.enable_mcp', False)
                mcp_enabled = get_value('mcp.enabled', False)
                self.enable_mcp = feature_enabled and mcp_enabled
            except Exception as e:
                logger.warning(f"Error reading MCP config: {e}")
                self.enable_mcp = False
            
        self.mcp_service = None
        if self.enable_mcp:
            self._init_mcp_service()
        
        # æ ¹æ“š MCP ç‹€æ…‹å»ºç«‹ system prompt
        self.system_prompt = self._build_system_prompt()

    def get_provider(self) -> ModelProvider:
        return ModelProvider.ANTHROPIC
    
    def _init_mcp_service(self) -> None:
        """åˆå§‹åŒ– MCP æœå‹™"""
        try:
            from ..services.mcp_service import get_mcp_service
            
            mcp_service = get_mcp_service()
            if mcp_service.is_enabled:
                self.mcp_service = mcp_service
                logger.info("Anthropic Model: MCP service initialized successfully")
            else:
                logger.warning("Anthropic Model: MCP service is not enabled")
                self.enable_mcp = False
        except Exception as e:
            logger.warning(f"Anthropic Model: Failed to initialize MCP service: {e}")
            self.enable_mcp = False
            self.mcp_service = None

    def check_connection(self) -> Tuple[bool, Optional[str]]:
        try:
            is_successful, _, error = self.chat_completion([ChatMessage(role="user", content="Hello")], max_tokens=10)
            return is_successful, error
        except Exception as e:
            logger.error(f"Anthropic connection check failed: {e}")
            return False, str(e)

    @retry_on_rate_limit(max_retries=3, base_delay=1.0)
    def chat_completion(self, messages: List[ChatMessage], **kwargs) -> Tuple[bool, Optional[ChatResponse], Optional[str]]:
        try:
            claude_messages = [{"role": msg.role, "content": msg.content} for msg in messages if msg.role != "system"]
            system_message = next((msg.content for msg in messages if msg.role == "system"), kwargs.get('system', self.system_prompt))

            json_body = {
                "model": kwargs.get('model', self.model_name),
                "max_tokens": kwargs.get('max_tokens', 4000),
                "messages": claude_messages,
                "temperature": kwargs.get('temperature', 0.01),
                "system": system_message
            }
            
            is_successful, response, error_message = self._request('POST', '/messages', body=json_body)
            
            if not is_successful:
                return False, None, error_message
            
            content = response['content'][0]['text']
            chat_response = ChatResponse(
                content=content,
                finish_reason=response.get('stop_reason'),
                metadata={'usage': response.get('usage'), 'model': response.get('model')}
            )
            return True, chat_response, None
        except Exception as e:
            logger.error(f"Anthropic chat completion failed: {e}")
            return False, None, str(e)

    def chat_with_user(self, user_id: str, message: str, platform: str = 'line', **kwargs: Any) -> Tuple[bool, Optional[RAGResponse], Optional[str]]:
        try:
            self.conversation_manager.add_message(user_id, 'anthropic', 'user', message, platform)
            conversations = self._get_recent_conversations(user_id, platform, kwargs.get('conversation_limit', 10))
            messages = self._build_conversation_context(conversations, message)
            
            # ä½¿ç”¨ MCP function calling å¦‚æžœå•Ÿç”¨
            if self.enable_mcp and self.mcp_service:
                import asyncio
                is_successful, response, error = asyncio.run(
                    self.query_with_rag_and_mcp(message, context_messages=messages, **kwargs)
                )
            else:
                is_successful, response, error = self.query_with_rag(message, context_messages=messages, **kwargs)
            
            if not is_successful:
                return False, None, error
            
            self.conversation_manager.add_message(user_id, 'anthropic', 'assistant', response.answer, platform)
            response.metadata.update({'user_id': user_id, 'model_provider': 'anthropic', 'mcp_enabled': self.enable_mcp})
            return True, response, None
        except Exception as e:
            logger.error(f"Error in chat_with_user for {user_id}: {e}")
            return False, None, str(e)
    
    async def query_with_rag_and_mcp(self, query: str, context_messages: List[ChatMessage] = None, **kwargs) -> Tuple[bool, Optional[RAGResponse], Optional[str]]:
        """æ”¯æ´ MCP function calling çš„ RAG æŸ¥è©¢"""
        messages = context_messages if context_messages else [ChatMessage(role="user", content=query)]
        return await self._perform_rag_query_with_mcp(messages, **kwargs)

    async def _perform_rag_query_with_mcp(self, messages: List[ChatMessage], **kwargs: Any) -> Tuple[bool, Optional[RAGResponse], Optional[str]]:
        """åŸ·è¡Œ RAG æŸ¥è©¢ï¼ˆæ”¯æ´ MCP function callingï¼‰"""
        try:
            system_prompt = self._build_files_context() if self.file_cache else self.system_prompt
            is_successful, response, error = await self.chat_completion_with_mcp(messages, system=system_prompt, **kwargs)
            
            if not is_successful:
                return False, None, error
            
            # è™•ç†ä¾†æºä¿¡æ¯
            sources = []
            
            # å¾žå›žæ‡‰ä¸­æå–çš„å‚³çµ±ä¾†æº
            traditional_sources = self._extract_sources_from_response(response.content)
            sources.extend(traditional_sources)
            
            # å¾ž MCP function calls ä¸­æå–çš„ä¾†æº
            if response.metadata and 'sources' in response.metadata:
                mcp_sources = response.metadata['sources']
                sources.extend(mcp_sources)
            
            # å‰µå»º RAGResponse
            rag_response = RAGResponse(
                answer=response.content, 
                sources=sources, 
                metadata={
                    **response.metadata,
                    'mcp_enabled': True,
                    'sources_count': len(sources)
                }
            )
            
            return True, rag_response, None
            
        except Exception as e:
            logger.error(f"Error in _perform_rag_query_with_mcp: {e}")
            return False, None, str(e)

    def query_with_rag(self, query: str, context_messages: List[ChatMessage] = None, **kwargs) -> Tuple[bool, Optional[RAGResponse], Optional[str]]:
        messages = context_messages if context_messages else [ChatMessage(role="user", content=query)]
        return self._perform_rag_query(messages, **kwargs)

    def _perform_rag_query(self, messages: List[ChatMessage], **kwargs: Any) -> Tuple[bool, Optional[RAGResponse], Optional[str]]:
        system_prompt = self._build_files_context() if self.file_cache else self.system_prompt
        is_successful, response, error = self.chat_completion(messages, system=system_prompt, **kwargs)
        if not is_successful:
            return False, None, error
        
        sources = self._extract_sources_from_response(response.content)
        return True, RAGResponse(answer=response.content, sources=sources, metadata=response.metadata), None

    def upload_knowledge_file(self, file_path: str, **kwargs) -> Tuple[bool, Optional[FileInfo], Optional[str]]:
        try:
            import os
            import mimetypes
            filename = os.path.basename(file_path)
            file_size = os.path.getsize(file_path)
            if file_size > 100 * 1024 * 1024:
                return False, None, f"æª”æ¡ˆéŽå¤§: {file_size / 1024 / 1024:.1f}MBï¼Œè¶…éŽ 100MB é™åˆ¶"
            
            with open(file_path, 'rb') as f:
                file_content = f.read()
            
            content_type, _ = mimetypes.guess_type(file_path)
            files = {'file': (filename, file_content, content_type or 'application/octet-stream')}
            data = {'purpose': kwargs.get('purpose', 'knowledge_base')}
            
            is_successful, response, error_message = self._request('POST', '/files', files=files, data=data)
            
            if not is_successful:
                return False, None, error_message
            
            file_info = FileInfo(
                file_id=response['id'], filename=filename, size=file_size, status='processed',
                purpose=response.get('purpose', 'knowledge_base'),
                metadata={'upload_time': time.time(), 'claude_file_id': response['id']}
            )
            self.file_cache[response['id']] = file_info
            return True, file_info, None
        except Exception as e:
            logger.error(f"Anthropic file upload failed for {file_path}: {e}")
            return False, None, str(e)

    def get_knowledge_files(self) -> Tuple[bool, Optional[List[FileInfo]], Optional[str]]:
        # This is a simplified implementation. A real-world scenario might involve pagination.
        try:
            if len(self.file_cache) > 0:
                return True, list(self.file_cache.values()), None
            is_successful, response, error = self._request('GET', '/files')
            if not is_successful:
                return False, None, error
            files = [FileInfo(
                file_id=f['id'], filename=f['filename'], size=f.get('bytes', 0),
                status='processed', purpose=f.get('purpose', 'knowledge_base'),
                metadata={'upload_time': f.get('created_at'), 'claude_file_id': f['id']}
            ) for f in response.get('data', [])]
            for file_info in files:
                self.file_cache[file_info.file_id] = file_info
            return True, files, None
        except Exception as e:
            logger.error(f"Failed to get knowledge files: {e}")
            return False, None, str(e)

    def get_file_references(self) -> Dict[str, str]:
        """Gets a map of file IDs to their clean names for citation."""
        return {info.file_id: info.filename.rsplit('.', 1)[0] for info in self.file_cache.values()}

    # Minimal implementation for interface compliance
    def create_thread(self) -> Tuple[bool, Optional[ThreadInfo], Optional[str]]:
        return True, ThreadInfo(thread_id=str(uuid.uuid4())), None

    def delete_thread(self, thread_id: str) -> Tuple[bool, Optional[str]]:
        return True, None

    def add_message_to_thread(self, thread_id: str, message: ChatMessage) -> Tuple[bool, Optional[str]]:
        return True, None

    def run_assistant(self, thread_id: str, **kwargs) -> Tuple[bool, Optional[RAGResponse], Optional[str]]:
        return False, None, "Not implemented. Use chat_with_user for conversation."

    def clear_user_history(self, user_id: str, platform: str = 'line') -> Tuple[bool, Optional[str]]:
        return self.conversation_manager.clear_user_history(user_id, 'anthropic', platform)

    def transcribe_audio(self, audio_file_path: str, **kwargs: Any) -> Tuple[bool, Optional[str], Optional[str]]:
        if not self.speech_service:
            return False, None, "Speech service not configured"
        try:
            return self.speech_service.transcribe(audio_file_path, **kwargs)
        except Exception as e:
            return False, None, str(e)

    def generate_image(self, prompt: str, **kwargs: Any) -> Tuple[bool, Optional[str], Optional[str]]:
        return False, None, "Anthropic does not support image generation."

    def set_speech_service(self, service: Any):
        self.speech_service = service

    def _get_recent_conversations(self, user_id: str, platform: str, limit: int) -> List[Dict]:
        return self.conversation_manager.get_recent_conversations(user_id, 'anthropic', limit, platform)

    def _build_conversation_context(self, recent_conversations: List[Dict], current_message: str) -> List[ChatMessage]:
        messages = [ChatMessage(role=conv['role'], content=conv['content']) for conv in recent_conversations]
        messages.append(ChatMessage(role='user', content=current_message))
        return messages

    def _build_system_prompt(self) -> str:
        """å»ºæ§‹ system promptï¼ˆåŒ…å« MCP function schemasï¼‰"""
        # å¾žè¨­å®šæª”è®€å–åŸºç¤Ž system prompt
        if self.enable_mcp:
            try:
                from ..core.config import get_value
                base_prompt = get_value('mcp.system_prompt', "You are a helpful assistant.")
            except Exception:
                base_prompt = "You are a helpful assistant."
        else:
            base_prompt = "You are a helpful assistant."
        
        if self.enable_mcp and self.mcp_service:
            try:
                # å–å¾— MCP function schemas ç‚º Anthropic æ ¼å¼
                function_schemas_prompt = self.mcp_service.get_function_schemas_for_anthropic()
                if function_schemas_prompt:
                    base_prompt += f"\n\n{function_schemas_prompt}"
                    
                    # åŠ å…¥ MCP æœ€ä½³å¯¦è¸çš„å·¥å…·ä½¿ç”¨æŒ‡å¼•
                    base_prompt += """

## å·¥å…·ä½¿ç”¨æŒ‡å¼• (Tool Usage Guidelines)

### å®‰å…¨åŽŸå‰‡ï¼š
- åƒ…åœ¨ç”¨æˆ¶æ˜Žç¢ºè«‹æ±‚æˆ–éœ€è¦æ™‚ä½¿ç”¨å·¥å…·
- åœ¨èª¿ç”¨å·¥å…·å‰èªªæ˜Žæ‚¨å°‡åŸ·è¡Œçš„æ“ä½œ
- å¼•ç”¨å·¥å…·çµæžœæ™‚è¨»æ˜Žè³‡æ–™ä¾†æº
- å°æ•æ„ŸæŸ¥è©¢æä¾›é©ç•¶çš„ä¸Šä¸‹æ–‡èªªæ˜Ž

### å·¥å…·èª¿ç”¨æ ¼å¼ï¼š
ç•¶æ‚¨éœ€è¦ä½¿ç”¨å·¥å…·æ™‚ï¼Œè«‹ä½¿ç”¨ä»¥ä¸‹ JSON æ ¼å¼ï¼š
```json
{"function_name": "å·¥å…·åç¨±", "arguments": {"åƒæ•¸å": "åƒæ•¸å€¼"}}
```

### å·¥å…·èª¿ç”¨æœ€ä½³å¯¦è¸ï¼š
1. **æ˜Žç¢ºæ„åœ–**ï¼šæ¸…æ¥šèªªæ˜Žç‚ºä»€éº¼éœ€è¦ä½¿ç”¨é€™å€‹å·¥å…·
2. **åƒæ•¸é©—è­‰**ï¼šç¢ºä¿æä¾›çš„åƒæ•¸å®Œæ•´ä¸”æ­£ç¢º
3. **çµæžœè™•ç†**ï¼šå°å·¥å…·è¿”å›žçš„çµæžœé€²è¡Œé©ç•¶çš„è§£é‡‹å’Œæ•´ç†
4. **éŒ¯èª¤è™•ç†**ï¼šå¦‚æžœå·¥å…·èª¿ç”¨å¤±æ•—ï¼Œå‘ç”¨æˆ¶èªªæ˜Žæƒ…æ³ä¸¦æä¾›æ›¿ä»£æ–¹æ¡ˆ
5. **ä¾†æºå¼•ç”¨**ï¼šæ˜Žç¢ºæ¨™ç¤ºè³‡è¨Šä¾†æºï¼Œæé«˜é€æ˜Žåº¦"""
                    
                    logger.info("Anthropic Model: Added MCP function schemas and security guidelines to system prompt")
            except Exception as e:
                logger.error(f"Failed to add MCP function schemas to system prompt: {e}")
        
        return base_prompt
    
    def _has_function_calls(self, response_text: str) -> bool:
        """æª¢æŸ¥å›žæ‡‰æ˜¯å¦åŒ…å« function calls"""
        import re
        # æª¢æŸ¥æ˜¯å¦æœ‰ JSON æ ¼å¼çš„ function call
        json_pattern = r'```json\s*\{[^}]*"function_name"[^}]*\}[^`]*```'
        return bool(re.search(json_pattern, response_text, re.DOTALL))
    
    def _extract_function_calls(self, response_text: str) -> List[Dict[str, Any]]:
        """å¾žå›žæ‡‰ä¸­æå– function calls"""
        function_calls = []
        json_pattern = r'```json\s*(\{[^}]*"function_name"[^}]*\})[^`]*```'
        
        logger.debug(f"ðŸ” Anthropic Model: Searching for function calls in response")
        logger.debug(f"ðŸ“ Response text length: {len(response_text)} chars")
        
        matches = re.findall(json_pattern, response_text, re.DOTALL)
        logger.info(f"ðŸŽ¯ Found {len(matches)} potential function call matches")
        
        for i, match in enumerate(matches, 1):
            try:
                logger.debug(f"ðŸ“‹ Parsing function call {i}: {match.strip()}")
                function_call = json.loads(match.strip())
                
                if 'function_name' in function_call and 'arguments' in function_call:
                    function_name = function_call['function_name']
                    logger.info(f"âœ… Valid function call {i}: {function_name}")
                    logger.debug(f"ðŸ“Š Arguments: {json.dumps(function_call['arguments'], ensure_ascii=False)}")
                    function_calls.append(function_call)
                else:
                    logger.warning(f"âš ï¸ Invalid function call structure {i}: missing function_name or arguments")
                    
            except json.JSONDecodeError as e:
                logger.warning(f"âŒ Failed to parse function call JSON {i}: {e}")
                logger.debug(f"ðŸ“„ Invalid JSON: {match.strip()}")
                continue
        
        logger.info(f"ðŸ“‹ Extracted {len(function_calls)} valid function calls")
        return function_calls
    
    async def chat_completion_with_mcp(self, messages: List[ChatMessage], **kwargs) -> Tuple[bool, Optional[ChatResponse], Optional[str]]:
        """æ”¯æ´ MCP function calling çš„å°è©±å®Œæˆ"""
        if not self.enable_mcp or not self.mcp_service:
            return self.chat_completion(messages, **kwargs)
        
        try:
            # åŸ·è¡Œåˆå§‹å°è©±
            is_successful, chat_response, error = self.chat_completion(messages, **kwargs)
            if not is_successful:
                return False, None, error
            
            response_text = chat_response.content
            
            # æª¢æŸ¥æ˜¯å¦æœ‰ function calls
            if not self._has_function_calls(response_text):
                logger.debug("No function calls detected in response")
                return True, chat_response, None
            
            # æå–ä¸¦åŸ·è¡Œ function calls
            function_calls = self._extract_function_calls(response_text)
            if not function_calls:
                logger.warning("Function call pattern detected but extraction failed")
                return True, chat_response, None
            
            logger.info(f"ðŸ”§ Anthropic Model: Processing {len(function_calls)} function calls")
            logger.debug(f"ðŸ“‹ Function calls detected: {[call['function_name'] for call in function_calls]}")
            
            # åŸ·è¡Œ function calls ä¸¦æ”¶é›†çµæžœ
            function_results = []
            mcp_interactions = []  # ðŸ”¥ æ”¶é›† MCP äº’å‹•è³‡è¨Šï¼Œç”¨æ–¼å‰ç«¯é¡¯ç¤º
            for i, function_call in enumerate(function_calls, 1):
                function_name = function_call['function_name']
                arguments = function_call['arguments']
                
                logger.info(f"ðŸŽ¯ Anthropic Model: Executing function {i}/{len(function_calls)}: {function_name}")
                logger.debug(f"ðŸ“Š Function arguments: {json.dumps(arguments, ensure_ascii=False, indent=2)}")
                
                result = await self.mcp_service.handle_function_call(function_name, arguments)
                
                if result.get('success', False):
                    logger.info(f"âœ… Function {function_name} executed successfully")
                else:
                    error_msg = result.get('error', 'Unknown error')
                    logger.error(f"âŒ Function {function_name} failed: {error_msg}")
                
                function_results.append({
                    'function_name': function_name,
                    'arguments': arguments,
                    'result': result
                })
                
                # ðŸ”¥ æå– MCP äº’å‹•è³‡è¨Š
                if 'mcp_interaction' in result:
                    mcp_interactions.append(result['mcp_interaction'])
            
            # å»ºæ§‹åŒ…å« function results çš„æ–°å°è©±
            logger.info("ðŸ”„ Anthropic Model: Formatting function results for final response")
            function_results_text = self._format_function_results(function_results)
            logger.debug(f"ðŸ“„ Formatted function results: {function_results_text[:500]}...")
            
            # æ·»åŠ  function results ä¸¦ç¹¼çºŒå°è©±
            extended_messages = messages.copy()
            extended_messages.append(ChatMessage(role='assistant', content=response_text))
            extended_messages.append(ChatMessage(role='user', content=f"Function call results:\n{function_results_text}\n\nPlease provide a final response based on these results."))
            
            logger.info(f"ðŸ“¤ Anthropic Model: Sending final request with {len(extended_messages)} messages")
            
            # åŸ·è¡Œæœ€çµ‚å°è©±
            final_success, final_response, final_error = self.chat_completion(extended_messages, **kwargs)
            
            if final_success:
                # çµ„åˆæœ€çµ‚å›žæ‡‰ï¼ŒåŒ…å«ä¾†æºä¿¡æ¯
                sources = self._extract_sources_from_function_results(function_results)
                final_response.metadata = final_response.metadata or {}
                final_response.metadata['function_calls'] = function_calls
                final_response.metadata['function_results'] = function_results
                final_response.metadata['sources'] = sources
                # ðŸ”¥ æ·»åŠ  MCP äº’å‹•è³‡è¨Šï¼Œä¾›å‰ç«¯é¡¯ç¤º
                final_response.metadata['mcp_interactions'] = mcp_interactions
                
                logger.info(f"âœ… Anthropic Model: MCP workflow completed successfully")
                logger.info(f"ðŸ“Š Final response: {len(final_response.content)} chars, {len(sources)} sources")
                logger.debug(f"ðŸ“š Sources extracted: {[source.get('filename', 'Unknown') for source in sources[:3]]}")
                logger.info(f"ðŸ”§ MCP interactions: {len(mcp_interactions)} tool calls recorded")
                
                return True, final_response, None
            else:
                logger.error(f"âŒ Anthropic Model: Final response failed: {final_error}")
                return False, None, final_error
                
        except Exception as e:
            logger.error(f"Error in chat_completion_with_mcp: {e}")
            return False, None, str(e)
    
    def _format_function_results(self, function_results: List[Dict[str, Any]]) -> str:
        """æ ¼å¼åŒ– function results ç‚ºæ–‡å­—"""
        formatted_results = []
        
        for i, result in enumerate(function_results, 1):
            function_name = result['function_name']
            success = result['result'].get('success', False)
            
            if success:
                data = result['result'].get('data', 'No data')
                formatted_results.append(f"{i}. {function_name}: {data}")
            else:
                error = result['result'].get('error', 'Unknown error')
                formatted_results.append(f"{i}. {function_name}: Error - {error}")
        
        return '\n'.join(formatted_results)
    
    def _extract_sources_from_function_results(self, function_results: List[Dict[str, Any]]) -> List[Dict[str, str]]:
        """å¾ž function results ä¸­æå–ä¾†æºä¿¡æ¯"""
        sources = []
        
        for result in function_results:
            if result['result'].get('success'):
                metadata = result['result'].get('metadata', {})
                if 'sources' in metadata:
                    sources.extend(metadata['sources'])
        
        return sources
    
    def get_mcp_status(self) -> Dict[str, Any]:
        """å–å¾— MCP æœå‹™ç‹€æ…‹"""
        return {
            "enabled": self.enable_mcp,
            "service_available": self.mcp_service is not None,
            "service_info": self.mcp_service.get_service_info() if self.mcp_service else None
        }
    
    def reload_mcp_config(self) -> bool:
        """é‡æ–°è¼‰å…¥ MCP è¨­å®š"""
        if self.mcp_service:
            success = self.mcp_service.reload_config()
            if success:
                # é‡æ–°å»ºç«‹ system prompt
                self.system_prompt = self._build_system_prompt()
                logger.info("Anthropic Model: MCP config reloaded and system prompt updated")
            return success
        return False

    def _build_files_context(self) -> str:
        files_context = "\n".join([f"- {info.filename}" for info in self.file_cache.values()])
        return f"{self.system_prompt}\n\nUse the following documents to answer the question:\n{files_context}"

    def _extract_sources_from_response(self, response_text: str) -> List[Dict[str, str]]:
        import re
        sources = []
        matches = set(re.findall(r'\[([^\]]+)\]', response_text))
        file_refs = {info.filename: info.file_id for info in self.file_cache.values()}
        for match in matches:
            if match in file_refs:
                sources.append({'file_id': file_refs[match], 'filename': match})
        return sources

    @retry_on_rate_limit(max_retries=3, base_delay=1.0)
    def _request(self, method: str, endpoint: str, body: Optional[Dict] = None, files: Optional[Dict] = None, data: Optional[Dict] = None) -> Tuple[bool, Optional[Dict], Optional[str]]:
        headers = {'x-api-key': self.api_key, 'anthropic-version': '2023-06-01'}
        if not files:
            headers['Content-Type'] = 'application/json'
        
        try:
            response = requests.request(
                method, f'{self.base_url}{endpoint}', headers=headers, json=body, 
                files=files, data=data, timeout=(30, 60)
            )
            
            # æª¢æŸ¥ HTTP ç‹€æ…‹ç¢¼ä½†ä¸æ‹‹å‡ºç•°å¸¸
            if response.status_code >= 400:
                error_msg = f"HTTP Error: {response.status_code}"
                try:
                    error_details = response.json()
                    error_msg = error_details.get('error', {}).get('message', error_msg)
                except json.JSONDecodeError:
                    error_msg = response.text
                return False, None, error_msg
            
            return True, response.json(), None
        except requests.exceptions.HTTPError as e:
            error_msg = f"HTTP Error: {e.response.status_code}"
            try:
                error_details = e.response.json()
                error_msg = error_details.get('error', {}).get('message', error_msg)
            except json.JSONDecodeError:
                error_msg = e.response.text
            return False, None, error_msg
        except requests.exceptions.RequestException as e:
            raise e
