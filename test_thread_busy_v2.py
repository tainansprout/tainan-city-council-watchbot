#!/usr/bin/env python3
"""
æ›´æº–ç¢ºçš„æ¸¬è©¦ï¼šæ¨¡æ“¬åŒä¸€å€‹ LINE ç”¨æˆ¶åœ¨åŒä¸€å€‹ thread ä¸­å¿«é€Ÿç™¼é€å¤šå€‹è¨Šæ¯
"""

import sys
import os
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed

# æ·»åŠ å°ˆæ¡ˆè·¯å¾‘
sys.path.append('.')

from src.core.config import load_config
from src.models.factory import ModelFactory
from src.platforms.base import PlatformMessage, PlatformUser, PlatformType
from src.services.chat import ChatService
from src.database.connection import Database

def test_same_user_concurrent_messages():
    """æ¸¬è©¦åŒä¸€å€‹ç”¨æˆ¶å¿«é€Ÿé€£çºŒç™¼é€è¨Šæ¯"""
    print("ğŸ§ª æ¸¬è©¦åŒä¸€å€‹ç”¨æˆ¶å¿«é€Ÿé€£çºŒç™¼é€è¨Šæ¯...")
    
    # è¼‰å…¥é…ç½®
    config = load_config('config/config.yml')
    
    # å‰µå»ºæ¨¡å‹å’Œæœå‹™
    llm_config = config.get('llm', {})
    provider = llm_config.get('provider', 'openai')
    model_config = config.get(provider, {})
    model_config['provider'] = provider
    model = ModelFactory.create_from_config(model_config)
    
    database = Database(config['db'])
    chat_service = ChatService(model=model, database=database, config=config)
    
    # ä½¿ç”¨å›ºå®šçš„ç”¨æˆ¶ IDï¼ˆæ¨¡æ“¬çœŸå¯¦ LINE ç”¨æˆ¶ï¼‰
    test_user = PlatformUser(
        user_id='line_user_fixed_123',  # å›ºå®šçš„ç”¨æˆ¶ ID
        display_name='Fixed LINE User',
        platform=PlatformType.LINE
    )
    
    # é¦–å…ˆç™¼é€ä¸€å€‹è¨Šæ¯ä¾†å»ºç«‹ thread
    print("ğŸ“ å»ºç«‹åˆå§‹ thread...")
    initial_message = PlatformMessage(
        message_id='init_msg',
        user=test_user,
        content='åˆå§‹è¨Šæ¯ï¼Œå»ºç«‹ thread',
        message_type='text',
        reply_token='init_reply'
    )
    
    try:
        initial_response = chat_service.handle_message(initial_message)
        print(f"âœ… åˆå§‹ thread å»ºç«‹æˆåŠŸ")
        time.sleep(1)  # ç­‰å¾…åˆå§‹éŸ¿æ‡‰å®Œæˆ
    except Exception as e:
        print(f"âŒ åˆå§‹ thread å»ºç«‹å¤±æ•—: {e}")
        return
    
    # ç¾åœ¨å¿«é€Ÿç™¼é€å¤šå€‹è¨Šæ¯åˆ°åŒä¸€å€‹ç”¨æˆ¶ï¼ˆæ‡‰è©²ä½¿ç”¨åŒä¸€å€‹ threadï¼‰
    results = []
    
    def send_concurrent_message(message_num):
        """ç™¼é€ä½µç™¼è¨Šæ¯åˆ°åŒä¸€å€‹ç”¨æˆ¶"""
        message = PlatformMessage(
            message_id=f'concurrent_msg_{message_num}',
            user=test_user,  # ç›¸åŒçš„ç”¨æˆ¶
            content=f'ä½µç™¼è¨Šæ¯ {message_num}ï¼šè«‹å¿«é€Ÿå›æ‡‰',
            message_type='text',
            reply_token=f'concurrent_reply_{message_num}'
        )
        
        try:
            print(f"ğŸ“¤ ç™¼é€ä½µç™¼è¨Šæ¯ {message_num} (ç”¨æˆ¶: {test_user.user_id})")
            start_time = time.time()
            response = chat_service.handle_message(message)
            end_time = time.time()
            
            result = {
                'message_num': message_num,
                'success': True,
                'time_taken': end_time - start_time,
                'response_preview': response.content[:50] + '...' if len(response.content) > 50 else response.content
            }
            results.append(result)
            print(f"âœ… ä½µç™¼è¨Šæ¯ {message_num} æˆåŠŸ")
            
        except Exception as e:
            result = {
                'message_num': message_num,
                'success': False,
                'error': str(e),
                'error_type': type(e).__name__
            }
            results.append(result)
            print(f"âŒ ä½µç™¼è¨Šæ¯ {message_num} å¤±æ•—: {e}")
    
    # åŒæ™‚ç™¼é€å¤šå€‹è¨Šæ¯ï¼ˆæ¨¡æ“¬ç”¨æˆ¶å¿«é€Ÿé»æ“Šï¼‰
    print("\nğŸš€ åŒæ™‚ç™¼é€å¤šå€‹è¨Šæ¯åˆ°åŒä¸€å€‹ç”¨æˆ¶...")
    
    with ThreadPoolExecutor(max_workers=5) as executor:
        # å¹¾ä¹åŒæ™‚æäº¤å¤šå€‹ä»»å‹™
        futures = []
        for i in range(1, 6):  # ç™¼é€ 5 å€‹ä½µç™¼è¨Šæ¯
            future = executor.submit(send_concurrent_message, i)
            futures.append(future)
        
        # ç­‰å¾…æ‰€æœ‰ä»»å‹™å®Œæˆ
        for future in as_completed(futures):
            try:
                future.result()
            except Exception as e:
                print(f"âŒ åŸ·è¡Œä»»å‹™æ™‚ç™¼ç”Ÿç•°å¸¸: {e}")
    
    # é¡¯ç¤ºçµæœ
    print("\nğŸ“Š ä½µç™¼æ¸¬è©¦çµæœ:")
    for result in sorted(results, key=lambda x: x['message_num']):
        if result['success']:
            print(f"âœ… è¨Šæ¯ {result['message_num']}: æˆåŠŸ ({result['time_taken']:.2f}s)")
        else:
            print(f"âŒ è¨Šæ¯ {result['message_num']}: å¤±æ•—")
            print(f"   éŒ¯èª¤é¡å‹: {result['error_type']}")
            print(f"   éŒ¯èª¤è¨Šæ¯: {result['error']}")
    
    return results

def test_direct_openai_thread_collision():
    """ç›´æ¥æ¸¬è©¦ OpenAI Thread çš„ä½µç™¼è¡çª"""
    print("\nğŸ§ª ç›´æ¥æ¸¬è©¦ OpenAI Thread çš„ä½µç™¼è¡çª...")
    
    # è¼‰å…¥é…ç½®
    config = load_config('config/config.yml')
    
    # å‰µå»º OpenAI æ¨¡å‹
    llm_config = config.get('llm', {})
    provider = llm_config.get('provider', 'openai')
    model_config = config.get(provider, {})
    model_config['provider'] = provider
    model = ModelFactory.create_from_config(model_config)
    
    # å‰µå»ºä¸€å€‹ thread
    print("ğŸ“ å‰µå»ºæ¸¬è©¦ thread...")
    is_successful, thread_info, error = model.create_thread()
    if not is_successful:
        print(f"âŒ ç„¡æ³•å‰µå»º thread: {error}")
        return
    
    thread_id = thread_info.thread_id
    print(f"âœ… Thread å‰µå»ºæˆåŠŸ: {thread_id}")
    
    # å®šç¾©ä½µç™¼åŸ·è¡Œå‡½æ•¸
    def run_assistant_concurrent(run_num):
        """ä½µç™¼åŸ·è¡Œ Assistant"""
        from src.models.base import ChatMessage
        
        try:
            # æ·»åŠ è¨Šæ¯
            message = ChatMessage(role='user', content=f'ä½µç™¼åŸ·è¡Œæ¸¬è©¦ {run_num}')
            is_successful, error = model.add_message_to_thread(thread_id, message)
            
            if not is_successful:
                print(f"âŒ æ·»åŠ è¨Šæ¯å¤±æ•— (run {run_num}): {error}")
                return {'run_num': run_num, 'success': False, 'stage': 'add_message', 'error': error}
            
            # ç«‹å³åŸ·è¡Œ Assistantï¼ˆé€™è£¡å¯èƒ½æœƒè¡çªï¼‰
            print(f"ğŸƒ åŸ·è¡Œ Assistant (run {run_num})")
            is_successful, response, error = model.run_assistant(thread_id)
            
            if not is_successful:
                print(f"âŒ åŸ·è¡Œ Assistant å¤±æ•— (run {run_num}): {error}")
                return {'run_num': run_num, 'success': False, 'stage': 'run_assistant', 'error': error}
            
            print(f"âœ… Assistant åŸ·è¡ŒæˆåŠŸ (run {run_num})")
            return {'run_num': run_num, 'success': True}
            
        except Exception as e:
            print(f"âŒ åŸ·è¡Œéç¨‹ä¸­ç™¼ç”Ÿç•°å¸¸ (run {run_num}): {e}")
            return {'run_num': run_num, 'success': False, 'stage': 'exception', 'error': str(e), 'error_type': type(e).__name__}
    
    # ä½µç™¼åŸ·è¡Œå¤šå€‹ Assistant runs
    print(f"\nğŸš€ åŒæ™‚åŸ·è¡Œå¤šå€‹ Assistant runs (thread: {thread_id})")
    
    run_results = []
    with ThreadPoolExecutor(max_workers=3) as executor:
        futures = []
        for i in range(1, 4):  # åŒæ™‚åŸ·è¡Œ 3 å€‹ Assistant runs
            future = executor.submit(run_assistant_concurrent, i)
            futures.append(future)
        
        for future in as_completed(futures):
            try:
                result = future.result()
                run_results.append(result)
            except Exception as e:
                print(f"âŒ åŸ·è¡Œä»»å‹™æ™‚ç™¼ç”Ÿç•°å¸¸: {e}")
    
    # é¡¯ç¤ºçµæœ
    print("\nğŸ“Š ç›´æ¥ OpenAI ä½µç™¼æ¸¬è©¦çµæœ:")
    for result in sorted(run_results, key=lambda x: x['run_num']):
        if result['success']:
            print(f"âœ… Run {result['run_num']}: æˆåŠŸ")
        else:
            print(f"âŒ Run {result['run_num']}: å¤±æ•— (éšæ®µ: {result['stage']})")
            print(f"   éŒ¯èª¤: {result['error']}")
            if 'error_type' in result:
                print(f"   éŒ¯èª¤é¡å‹: {result['error_type']}")
    
    # æ¸…ç†
    try:
        model.delete_thread(thread_id)
        print(f"ğŸ—‘ï¸ å·²åˆªé™¤æ¸¬è©¦ thread: {thread_id}")
    except:
        pass
    
    return run_results

if __name__ == "__main__":
    print("ğŸ§ª OpenAI Thread ä½µç™¼è¡çªæ¸¬è©¦ v2")
    print("=" * 50)
    
    # æ¸¬è©¦ 1: åŒä¸€å€‹ç”¨æˆ¶å¿«é€Ÿç™¼é€å¤šå€‹è¨Šæ¯
    print("\nã€æ¸¬è©¦ 1ã€‘åŒä¸€å€‹ç”¨æˆ¶å¿«é€Ÿç™¼é€å¤šå€‹è¨Šæ¯")
    results1 = test_same_user_concurrent_messages()
    
    # æ¸¬è©¦ 2: ç›´æ¥æ¸¬è©¦ OpenAI Thread ä½µç™¼è¡çª
    print("\nã€æ¸¬è©¦ 2ã€‘ç›´æ¥æ¸¬è©¦ OpenAI Thread ä½µç™¼è¡çª")
    results2 = test_direct_openai_thread_collision()
    
    print("\nğŸ” åˆ†æçµæœ:")
    print("æ ¹æ“šä¸Šè¿°éŒ¯èª¤è¨Šæ¯ï¼Œæˆ‘å€‘å°‡æ›´æ–° error_handler.py ä¾†è™•ç†ç‰¹å®šçš„ OpenAI éŒ¯èª¤")