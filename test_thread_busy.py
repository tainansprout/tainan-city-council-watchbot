#!/usr/bin/env python3
"""
æ¸¬è©¦ OpenAI Assistant API Thread é‡è¤‡ç™¼é€è¨Šæ¯çš„éŒ¯èª¤è™•ç†
æ¨¡æ“¬ LINE ç”¨æˆ¶é‡è¤‡ç™¼é€è¨Šæ¯çš„æƒ…æ³
"""

import sys
import os
import asyncio
import time
from threading import Thread
from concurrent.futures import ThreadPoolExecutor

# æ·»åŠ å°ˆæ¡ˆè·¯å¾‘
sys.path.append('.')

from src.core.config import load_config
from src.models.factory import ModelFactory
from src.platforms.base import PlatformMessage, PlatformUser, PlatformType
from src.services.chat import ChatService
from src.database.connection import Database

def test_concurrent_messages():
    """æ¸¬è©¦é‡è¤‡ç™¼é€è¨Šæ¯åˆ°åŒä¸€å€‹ thread"""
    print("ğŸ§ª é–‹å§‹æ¸¬è©¦é‡è¤‡ç™¼é€è¨Šæ¯åˆ° OpenAI Assistant Thread...")
    
    # è¼‰å…¥é…ç½®
    config = load_config('config/config.yml')
    
    # å‰µå»ºæ¨¡å‹å’Œæœå‹™
    llm_config = config.get('llm', {})
    provider = llm_config.get('provider', 'openai')
    model_config = config.get(provider, {})
    model_config['provider'] = provider
    model = ModelFactory.create_from_config(model_config)
    
    database = Database(config['db'])
    chat_service = ChatService(model=model, database=database, config=config)
    
    # å‰µå»ºæ¸¬è©¦ç”¨æˆ¶
    test_user = PlatformUser(
        user_id='test_concurrent_user_123',
        display_name='Test Concurrent User',
        platform=PlatformType.LINE
    )
    
    # å‰µå»ºæ¸¬è©¦è¨Šæ¯
    def create_test_message(message_num):
        return PlatformMessage(
            message_id=f'test_msg_{message_num}',
            user=test_user,
            content=f'æ¸¬è©¦è¨Šæ¯ {message_num}ï¼šè«‹å¹«æˆ‘æŸ¥è©¢ä¸€äº›è³‡è¨Š',
            message_type='text',
            reply_token=f'test_reply_{message_num}'
        )
    
    # å„²å­˜çµæœ
    results = []
    
    def send_message(message_num):
        """ç™¼é€å–®å€‹è¨Šæ¯"""
        print(f"ğŸ“¤ ç™¼é€è¨Šæ¯ {message_num}")
        message = create_test_message(message_num)
        
        try:
            start_time = time.time()
            response = chat_service.handle_message(message)
            end_time = time.time()
            
            result = {
                'message_num': message_num,
                'success': True,
                'response': response.content[:100] + '...' if len(response.content) > 100 else response.content,
                'time_taken': end_time - start_time
            }
            results.append(result)
            print(f"âœ… è¨Šæ¯ {message_num} æˆåŠŸï¼Œè€—æ™‚ {result['time_taken']:.2f}s")
            
        except Exception as e:
            result = {
                'message_num': message_num,
                'success': False,
                'error': str(e),
                'error_type': type(e).__name__
            }
            results.append(result)
            print(f"âŒ è¨Šæ¯ {message_num} å¤±æ•—: {e}")
    
    print("\nğŸš€ é–‹å§‹ä½µç™¼ç™¼é€è¨Šæ¯...")
    
    # ä½¿ç”¨ ThreadPoolExecutor ä¾†æ¨¡æ“¬ä½µç™¼è«‹æ±‚
    with ThreadPoolExecutor(max_workers=3) as executor:
        # å¹¾ä¹åŒæ™‚ç™¼é€ 3 å€‹è¨Šæ¯
        futures = []
        for i in range(1, 4):
            future = executor.submit(send_message, i)
            futures.append(future)
            time.sleep(0.1)  # çŸ­æš«å»¶é²ï¼Œæ¨¡æ“¬å¿«é€Ÿé€£çºŒé»æ“Š
        
        # ç­‰å¾…æ‰€æœ‰è«‹æ±‚å®Œæˆ
        for future in futures:
            future.result()
    
    print("\nğŸ“Š æ¸¬è©¦çµæœ:")
    for result in sorted(results, key=lambda x: x['message_num']):
        if result['success']:
            print(f"âœ… è¨Šæ¯ {result['message_num']}: æˆåŠŸ ({result['time_taken']:.2f}s)")
        else:
            print(f"âŒ è¨Šæ¯ {result['message_num']}: å¤±æ•—")
            print(f"   éŒ¯èª¤é¡å‹: {result['error_type']}")
            print(f"   éŒ¯èª¤è¨Šæ¯: {result['error']}")
    
    return results

def test_sequential_fast_messages():
    """æ¸¬è©¦å¿«é€Ÿé€£çºŒç™¼é€è¨Šæ¯"""
    print("\nğŸ§ª é–‹å§‹æ¸¬è©¦å¿«é€Ÿé€£çºŒç™¼é€è¨Šæ¯...")
    
    # è¼‰å…¥é…ç½®
    config = load_config('config/config.yml')
    
    # ç›´æ¥ä½¿ç”¨ OpenAI model é€²è¡Œæ›´åº•å±¤çš„æ¸¬è©¦
    llm_config = config.get('llm', {})
    provider = llm_config.get('provider', 'openai')
    model_config = config.get(provider, {})
    model_config['provider'] = provider
    model = ModelFactory.create_from_config(model_config)
    
    # å‰µå»ºä¸€å€‹ thread
    print("ğŸ“ å‰µå»ºæ–°çš„ OpenAI Assistant Thread...")
    is_successful, thread_info, error = model.create_thread()
    if not is_successful:
        print(f"âŒ ç„¡æ³•å‰µå»º thread: {error}")
        return
    
    thread_id = thread_info.thread_id
    print(f"âœ… Thread å‰µå»ºæˆåŠŸ: {thread_id}")
    
    # å¿«é€Ÿç™¼é€å¤šå€‹è¨Šæ¯
    from src.models.base import ChatMessage
    
    for i in range(1, 4):
        print(f"\nğŸ“¤ ç™¼é€è¨Šæ¯ {i} åˆ° thread {thread_id}")
        message = ChatMessage(role='user', content=f'å¿«é€Ÿæ¸¬è©¦è¨Šæ¯ {i}')
        
        try:
            # æ·»åŠ è¨Šæ¯åˆ° thread
            is_successful, error = model.add_message_to_thread(thread_id, message)
            if not is_successful:
                print(f"âŒ æ·»åŠ è¨Šæ¯ {i} å¤±æ•—: {error}")
                continue
            
            print(f"âœ… è¨Šæ¯ {i} å·²æ·»åŠ åˆ° thread")
            
            # ç«‹å³åŸ·è¡Œ Assistantï¼ˆé€™è£¡å¯èƒ½æœƒè¡çªï¼‰
            try:
                is_successful, response, error = model.run_assistant(thread_id)
                if not is_successful:
                    print(f"âŒ åŸ·è¡Œ Assistant å¤±æ•— (è¨Šæ¯ {i}): {error}")
                    print(f"   éŒ¯èª¤é¡å‹: {type(error).__name__}")
                else:
                    print(f"âœ… Assistant åŸ·è¡ŒæˆåŠŸ (è¨Šæ¯ {i})")
                    
            except Exception as e:
                print(f"âŒ Assistant åŸ·è¡Œç•°å¸¸ (è¨Šæ¯ {i}): {e}")
                print(f"   ç•°å¸¸é¡å‹: {type(e).__name__}")
                
        except Exception as e:
            print(f"âŒ è™•ç†è¨Šæ¯ {i} æ™‚ç™¼ç”Ÿç•°å¸¸: {e}")
            print(f"   ç•°å¸¸é¡å‹: {type(e).__name__}")
        
        # å¾ˆçŸ­çš„å»¶é²ï¼Œæ¨¡æ“¬å¿«é€Ÿé»æ“Š
        time.sleep(0.5)
    
    # æ¸…ç†ï¼šåˆªé™¤æ¸¬è©¦ thread
    try:
        model.delete_thread(thread_id)
        print(f"ğŸ—‘ï¸ å·²åˆªé™¤æ¸¬è©¦ thread: {thread_id}")
    except:
        pass

if __name__ == "__main__":
    print("ğŸ§ª OpenAI Assistant Thread é‡è¤‡è¨Šæ¯æ¸¬è©¦")
    print("=" * 50)
    
    # æ¸¬è©¦ 1: é€é ChatService ä½µç™¼ç™¼é€
    print("\nã€æ¸¬è©¦ 1ã€‘é€é ChatService ä½µç™¼ç™¼é€è¨Šæ¯")
    results1 = test_concurrent_messages()
    
    # æ¸¬è©¦ 2: ç›´æ¥å° OpenAI API å¿«é€Ÿé€£çºŒç™¼é€
    print("\nã€æ¸¬è©¦ 2ã€‘ç›´æ¥å° OpenAI API å¿«é€Ÿé€£çºŒç™¼é€è¨Šæ¯")
    test_sequential_fast_messages()
    
    print("\nğŸ” åˆ†æçµæœ:")
    print("è«‹æª¢æŸ¥ä¸Šè¿°éŒ¯èª¤è¨Šæ¯ï¼Œæˆ‘å€‘å°‡æ ¹æ“šå…·é«”çš„éŒ¯èª¤å…§å®¹æ›´æ–° error_handler.py")